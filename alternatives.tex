\section{Alternative Models Using Sets of Priors***}
\label{sec:alternatives}


In Section~\ref{sec:generalmodel},
we have discussed a specific wide of models based on parametrically constructed sets of conjugate priors,
described important properties of inferences based on this class in general terms,
and illustrated the potential of generalised Bayesian inference methods
for the situations of prior near-ignorance and substantial prior information.
%illustrated the main ideas and potential of generalised Bayesian inference for this model framework.
In this section, we will discuss two alternative frameworks for inference based on sets of priors,
and consider a number of inference models that could serve as an alternative to the models discussed in Section~\ref{sec:generalmodel}.
The two frameworks based on sets of priors
in alternative to the model framework discussed in this thesis are 
\emph{neighbourhood models} (Section~\ref{sec:alternatives:neighbourhood}) and
the \emph{density-ratio class} (Section~\ref{alternatives:drc}).%
\footnote{By some authors, the density ratio class is considered a neighbourhood model
where instead of one central distribution $\p_0$ two distributions are considered
\parencite[e.g.,][\S 4.3]{1991:pericchi}.
We think, however, that the density ratio class is better characterised as a separate model framework.}
Then, we will briefly describe a few concrete inference models,
some of which are based on these two model frameworks,
distinguishing between models that are based on conjugate priors (Section~\ref{sec:alternatives:conjugate})
and models that are not (Section \ref{sec:alternatives:other}).

%Now, we will briefly discuss some further approaches that could serve
%as an alternative to the sets of conjugate priors discussed in this thesis.
%Firstly, in Section~\ref{sec:alternatives:conjugate} we will describe some models
%that also rely on conjugate priors in exponential families,
%but that cannot be subsumed in the framework of Section~\ref{sec:generalmodel}.
%In Section~\ref{sec:alternatives:other}, we will touch on models based on sets of priors that do not rely on conjugates.

\subsection{Neighbourhood Models}
\label{sec:alternatives:neighbourhood}

%different sorts of neighbourhood models as considered in robust Bayesian approaches \parencite[e.g.,][]{1994:berger}.
An important class of models that make use of sets of priors are \emph{neighborhood models}.
These are typically considered in the robust Bayesian approach \parencite[see, e.g.,][]{1994:berger,2000:rios},
where a certain prior distribution $\p_0$ is singled out as a potential model for prior information,
but, due to lack of confidence in this choice, a neigbourhood around $\p_0$ is considered,
consisting of distributions `near' $\p_0$.
The rationale for this approach is to ensure robustness of the Bayesian analysis based on a single prior $\p_0$
by checking that small deviations from $\p_0$ do not lead to large deviations in posterior inferences.
%Approaches along these lines are also called systematic sensitivity analysis, or Bayesian sensitivity analysis.
As mentioned in Section~\ref{sec:bayesiansensitivity},
imprecise probability offers a formal, not casuistic framework for such \emph{Bayesian sensitivity analysis};
however, interpretation of the sets of priors, and the modelling intention is different,
especially with respect to the inference situations we perceive as important modelling opportunities
for generalised Bayesian inference.%
\footnote{These are (i) the possibility of modeling prior near-ignorance
(see Sections~\ref{sec:motivation:near-ignorance} and \ref{sec:idm-and-near-ignorance}),
and (ii) \pdc\ sensitivity in case of informative priors
(see Sections~\ref{sec:motivation:pdc} and \ref{sec:pdc-sensitivity}).}
We will thus touch only briefly on neighbourhood models,
picking out two typical examples,
although many different kinds of neighbourhood models are discussed in the literature
(see, e.g., the surveys by \textcite{2000:bergerinsuaruggeri} and \textcite{2005:ruggeri}).

A typical example is the $\varepsilon$-contamination class \parencite[see, e.g.,][\S 4.3.2]{1994:berger},
which can be informally described as follows:
In a (virtual) sample distribution,
not all data are distributed according to $\p_0$;
instead, %a fraction of $\varepsilon$ 
$100\cdot\varepsilon$\% of the data is distributed according to any distribution from a set $\mathcal{Q}$, %according to some other law,
and depending on the choice for $\mathcal{Q}$, %the class of distributions this contaminating data can follow,
a variety of $\varepsilon$-contamination classes can be defined.%
\footnote{For $\mathcal{Q}$ taken as `all distributions',
the $\varepsilon$-contamination class is also called `linear-vacuous mixture'
in the imprecise probability literature \parencite[e.g.,][\S 7.3]{itip-special},
constituting an important special case of coherent lower previsions.}
%$100\cdot(1-\varepsilon)$\% of the data are distributed according to $\p_0$,
%while $100\cdot\varepsilon$\% of the data are distributed according to any other distribut

Another example for a neighbourhood model is the \emph{odds-ratio model}.
It tries to model approximate adherence to a central probability law with distribution $\p_0$
by giving the following constraints for pairs of events $A$ and $B$:
\begin{align*}
\frac{\p(A)}{\p(B)} \le (1-\epsilon)\frac{\p_0(A)}{\p_0(B)}\,,\quad A, B \subseteq \Omega
\end{align*}
The set of distributions $\p$ compatible with these restrictions
forms then an odds-ratio model with parameter $\epsilon$,
and can be represented by a lower prevision $\El_\epsilon$.
When such a model is taken as an imprecise prior in Bayesian inference,
the set of posteriors can again be expressed as an odds-ratio model
\parencite[\S 7.2]{itip-special}.
Other neighbourhood models, like, e.g.,
variants of the $\varepsilon$-contamination class mentioned above,
may instead not be closed under Bayesian updating.


\subsection{Density Ratio Class}
\label{alternatives:drc}

%Below, we will briefly describe an interesting model class
%that can be defined using conjugate or non-conjugate distributions 
%--
%Another interesting model that could be considered
%as an alternative to the sets of conjugate priors discussed in this thesis
%is the \emph{density-ratio class} model,
%also known as \emph{interval of measures}
The \emph{density ratio class}, also known as \emph{interval of measures}
\parencites{1981:derobertis}{1990:berger},
provides also an interesting model framework for Bayesian inference using sets of priors.
Here, instead of generating the set of prior distributions by varying their parameters in a set
(as in Section~\ref{sec:generalmodel}),
the set of priors $\mathcal{M}$ is defined by bounding the probability density functions
%of elements
$p(\vartheta) \in \mathcal{M}$
via a lower bounding function $l(\vartheta)$ and an upper bounding function $u(\vartheta)$.%
\footnote{\label{foot:rinderknecht}A very accessible presentation of density-ratio classes with
parametric bounding shapes $l(\vartheta)$ and $u(\vartheta)$,
along with a method for elicitation from an expert
providing quantiles (or quantile ranges) for a number of probability values,
is given in \textcite{2011:rinderknecht}.
We discuss this model in Section~\ref{sec:alternatives:other}.}

A set of (prior) distributions on $\vartheta$ is defined by %$\mathcal{M}_{l,u}$ by 
%through restricting the densities $p(\vartheta)$ in $\mathcal{M}_{l,u}$ to
\begin{align}\label{eq:densityratioclass}
\mathcal{M}_{l,u} = \left\{ p(\vartheta) :
\exists c \in \posreals: l(\vartheta) \le c p(\vartheta) \le u(\vartheta)\right\}\,,
\end{align}
where $l(\vartheta)$ and $u(\vartheta)$ are bounded non-negative functions (i.e., non-normalised densities)	
for which $l(\vartheta) \le u(\vartheta)\ \forall\ \vartheta$.
$l(\vartheta)$ and $u(\vartheta)$ are often called \emph{lower and upper density functions},
and only need to be known up to a multiplicative constant.
If $l(\vartheta)>0\ \forall\ \vartheta$, then \eqref{eq:densityratioclass} can also be written as
\begin{align*}
\mathcal{M}_{l,u} = \left\{ p(\cdot) :
\frac{p(\vartheta)}{p(\vartheta')} \le \frac{u(\vartheta)}{l(\vartheta')}\ \forall\ \vartheta, \vartheta' \right\}\,,
\end{align*}
hence the name `density ration class'.

The density ratio class defines a certain type of credal sets;
thus, as discussed in Section~\ref{sec:ip-main},
it can also be expressed via an associated coherent lower prevision $\El_{l,u}$.
%(derived from generic lower previsions)
%\parencite[Figure~5.5]{itip-special},
The density ratio class has a number of advantageous properties,
especially as compared to many neighbourhood models
\parencite[see, e.g.,][\S 2.3]{2011:rinderknecht};
most importantly, it has the convenient property of invariance under Bayesian updating.
The set of posteriors derived from $\mathcal{M}_{l,u}$
through the Generalised Bayes' Rule (i.e., by updating element by element,
see Sections~\ref{sec:gbr} and \ref{sec:imprecisebayes})
can again be expressed as a density ratio class,
with $l(\vartheta)$ and $u(\vartheta)$ updated according to Bayes' Rule \parencite{1981:derobertis}.

Although this invariance property is advantageous,
a consequence of it is that also the ratio $u(\vartheta)/l(\vartheta)$
is constant under updating, such that posterior imprecision,
as measured by the magnitude of $\mathcal{M}_{l,u}$ is the same as prior imprecision,
for any sample size $n$ \parencite[see, e.g.,][\S 4.2.2]{2011:rinderknecht:diss}.
This is in strong contrast to the behaviour of the models discussed in Section~\ref{sec:generalmodel},
where $\MZ$ converges to a %precise distribution
one-element set for $n \to \infty$. 

It is important to note here that even if the bounding functions $l(\vartheta)$ and $u(\vartheta)$ are defined parametrically
(or, as in the approaches by Coolen \parencite*{1993:coolen, 1994:coolen} described below, even as conjugates),
$\mathcal{M}_{l,u}$ does not contain only these parametric densities (or conjugate densities).
Instead, $\mathcal{M}_{l,u}$ contains a variety of shapes, where
(if $l(\vartheta)$ and $u(\vartheta)$ are not proportional)
the tail behaviour can vary between that of $l(\vartheta)$ and $u(\vartheta)$.%
\footnote{See, e.g., \cite[\S 3.2]{2011:rinderknecht}, or \cite[\S 4.3]{1991:pericchi}.}

The density ratio class is thus similar to sets of priors
discussed in the model framework from Section~\ref{sec:basicsetting} where
$\MZ$ is taken as all convex mixtures of parametric priors with parameters in $\PZ$.
Then, also $\MZ$ contains a variety of shapes that, however,
do not allow for substantially different tail behaviour,
but it also has the same property of invariance under Bayesian updating,
because $\MN$ can be constructed as the set of all convex mixtures of distributions with parameters in $\PN$.



\subsection{Approaches Based on Conjugate Priors}
\label{sec:alternatives:conjugate}

%first sets based on conjugate priors: %1993:coolen, 1994:coolen interval of measures:
%2011:rinderknecht does not consider update step, just elicitation, but 2011:rinderknecht:phd does
%then 2009:bickis
%Other work on exponential family sampling models includes \textcite{1993:coolen, 1994:coolen},
%who uses a different type of imprecise conjugate prior densities (interval of measures),
%and \textcite{2009:bickis}, who suggests the logit-normal model as an alternative to the IDM.
We discuss here some approaches based on conjugate priors.
First, a density ratio class model using conjugate distributions
for the bounding functions $l(\vartheta)$ and $u(\vartheta)$
by Coolen \parencite*{1993:coolen,1994:coolen} is described.
Then, a brief summary of an approach by \textcite{2009:bickis} is given,
who constructs a conjugate to the multinomial sample distribution
where a specific correlation structure for the categories can be specified.
Afterwards, some results of the study by \textcite{1991:pericchi} are reported,
who compare models for inference on the mean of a normal distribution with known variance
that are based on a number of conjugate and non-conjugate sets of priors.
 

\medskip

\textcite{1993:coolen} proposed an interesting model for sample distributions
from the one-parameter exponential family using the density ration class.
In this model, the prior bounding functions $l(\vartheta)$ and $u(\vartheta)$ from \eqref{eq:densityratioclass}
are linked through the relation $u(\vartheta) = c_0 \cdot l(\vartheta)$,
where $c_0 \ge 1$ is independent of $\vartheta$.
Defining $l(\vartheta\mid\pz)$ as (proportional to) the conjugate prior, with hyperparmeter $\pz$,
calculation of the posterior lower bounding function $l(\vartheta\mid\x,\pz)$ is straight-forward, by%
\footnote{Remember that $l(\cdot)$ needs to be known up to a multiplicative constant only.}
\begin{align*}
l(\vartheta\mid\x,\pz) &= l(\vartheta\mid\pz) f(\x\mid\vartheta) = l(\vartheta\mid\pn)\,,
%\end{align*}
\intertext{and the posterior upper bounding function is defined as}
%\begin{align*}
u(\vartheta\mid\x,\pz) &:= \frac{c_n}{c_0} u(\vartheta\mid\pz) f(\x\mid\vartheta) = c_n l(\vartheta\mid\pn)\,,
\end{align*}
where $c_n$ is introduced to allow the magnitude of $\mathcal{M}$ to decrease in dependence of the samples size $n$.%
\footnote{Note that $c_n \neq c_0$ means that we actually do not update $\mathcal{M}$ according to the Generalised Bayes' Rule.}
$c_n$ must thus be chosen as a function decreasing in $n$,
and \textcite{1993:coolen} suggests a functional form with
$c_n \to 1$ for $n \to \infty$, containing a parameter $\xi$
that has a meaning similar to $\nz$, in the sense that if $n = \xi$,
an information measure \parencite[suggested by][\S 5.3.7]{1991:walley} is doubled.%
\footnote{Imprecision for models with fixed $\nz$ (Section~\ref{sec:basicsetting}, item~\ref{enum:modeltypes-a})
is halfed when $\nz = n$, see Section~\ref{sec:gbicp-properties-criteria}, item~\ref{enum:n0vsn}.}

To use the model, one has to elicit the parameter(s) of the conjugate prior $\pz$,
along with $c_0$ and $\xi$.
The model is quite easy to handle, as the density ratio class provides relatively simple formulas for, e.g.,
lower and upper cumulative density functions, and lower and upper predictive densities.
These formulas are easy to calculate if the involved integrals are easy to obtain,
as is the case for a conjugate choice of $l(\vartheta)$.
However, this model is insensitive to \pdc, owing to the requirement of $c_0$ and $c_n$
to be independent of $\theta$ and $\x$ (except for the sample size $n$);
furthermore, as conceded by \textcite[p.~341]{1993:coolen},
there could be many other functional forms for $c_n$ that were equally reasonable
as the one suggested in the paper.

\medskip

\textcite{1994:coolen} presents a further study of this model with a focus on predictive inferences,
considering the special case of Bernoulli observations.
There, $l(\theta)$ is proportional to a $\be(\alpha,\beta)$,
and $u(\theta)$ is defined as
\begin{align*}
u(\theta) = l(\theta) + c^*_0 \cdot a(\theta)\,,
\end{align*}
where $a(\theta)$ is proportional to a $\be(\mu,\lambda)$,
and $c^*_0$ is again a factor that determines the prior imprecision.%
\footnote{The relation between $c_0$ and $c^*_0$ is thus $c^*_0 = c_0 - 1$.}
For the case $\mu=\alpha$, $\beta=\lambda$, and $c^*_0 = c^*_n = c$ for all $n$,
formulas are derived that allow to study
imprecision in posterior predicitive probabilities analytically.
This model gives interesting insights into the dependence of imprecision on $s$,
but the fact that posterior imprecision is the very same as prior imprecision
if $s/n = \alpha/(\alpha+\beta)$ (i.e., data and prior assignments are perfectly in line)
suggests that this model is not very suitable for inferences \parencite[p.~160]{1994:coolen}.
Furthermore, our conjecture is that the influence of $s$ on posterior imprecision
is due to the restriction that the two bounding functions have to be
proportional to Beta densities. For the case of $s/n = \alpha/(\alpha+\beta)$,
imprecision decreases for any $s \neq n/2$ \parencite[Table~1]{1994:coolen}.
This is very similar to the phenomenon that, in the Multinomial-Dirichlet model,
the posterior variance of $\theta_j$ decreases for any $n_j$ if $\yz_j = 1/2$
(see Section~\ref{sec:ex-mult}).

Afterwards, \textcite[\S 4]{1994:coolen} illustrates the general case
with $\mu\neq\alpha$, $\beta\neq\lambda$, and $c^*_n$ as in \textcite{1993:coolen}
with some numeric examples, showing a reasonable behaviour of the model.
However, there are no theoretical results for model behaviour of the general case
(as we gave in Section~\ref{sec:gbicp-properties-criteria}),
and elicitation of the parameters ($\alpha,\beta,\mu,\lambda,c^*_0,\xi$)
for an informative prior
seems quite difficult and possibly involves elaborate pre-posterior procedures,
as the influence of different choices of $l(\theta)$ and $a(\theta)$ on $\mathcal{M}$
is rather oblique.%
\footnote{However, the elicitation method by \textcite{2011:rinderknecht} could be used
to elicit $\alpha,\ \beta,\ \mu,\ \lambda$, and $c^*_0$.}

\medskip

\textcite{2009:bickis} suggests a multivariate logit-normal model as an alternative to the IDM
for an application where neighbouring category probabilities $\theta_j$ are correlated.
Instead of the conjugate Dirichlet prior for $\btheta$ (see Section~\ref{sec:diri-multi}),
a multivariate normal prior is assumed for the (element-wise) logits of $\btheta$,
i.e., $\log(\btheta/(1-\btheta)) \sim \norm_k(\mu \mbf{1}, \sigma^2\mbf{R})$,
where $\mbf{1} = (1, \ldots, 1)$, and $\mbf{R}$ is a correlation structure.
The resulting posterior in terms of $\btheta$ is itself not tractable, but can be approximated
by an exponential family that can be seen as the convex hull of the logit-normal
and dirichlet families \parencite[p.~189]{2009:bickis},
and that contains the Dirichlet distribution for the limit $\sigma^2 \to \infty$.
Although a conjugate prior in the sense that there is a simple update step for the hyperparameters,
posterior inferences for this prior must usually be derived by simulation,
as there are, e.g., no results on the expectation of this distribution.

In the paper, a near-noninformative set of priors is constructed
by means of a set of hyperparameters,
used for an interesting application to estimate a discrete hazard function
for which it is useful to mirror an autocorrelation structure in $\mbf{R}$.
However, the potential of this prior model for applications where no correlation structure must be assumed is
relatively low, due to the much more complex computations as compared to the IDM.
%\footnote{For a recent Bayesian alternative to the IDM, see \textcite{2013:mangilibenavoli}, also mentioned in Section~***.}
It remains to be investigated how an informative (imprecise) prior could be elicited,
and the behaviour in case of \pdc\ is as yet unkown.

\medskip

By studying credibility intervals for an unknown mean $\mu$ for samples from a normal distribution with known variance $\sigma^2$,
\textcite{1991:pericchi} give a neat overview on a number of approaches based on sets of priors,
a part of which are based on conjugate priors.
As we do in Section~\ref{sec:generalmodel},
this overview makes the distinction of modelling near-noninformativeness
versus models for substantial prior information,
the latter of which are investigated with respect to \pdc\ sensitivity.%
\footnote{In fact, \textcite{1991:pericchi} was our inspiration to make this distinction in the first place.}

For modelling near-noninformativeness, \textcite[\S 3]{1991:pericchi}
present several `translation-invariant' models,
i.e., models whose posterior inferences do not depend on the location of $\bar{x}$.
This includes a model where $\mathcal{M}$ consists of conjugate normal distributions $\mu \sim \norm(\mu_0, \nu^2)$
for which $|\mu - \mu_0| \le c \nu^2 / 2\sigma$, and where $\nu \to \infty$.
It is considered inferior to a model where $\mathcal{M}$ consists of all double exponential distributions
with a fixed variance but the mean varying in $\reals$, %i.e., a non-conjugate class,
showing the potential for models going beyond conjugate distributions.

\textcite[\S 4]{1991:pericchi} denote all models that are considered
for substantial prior information as `neighbourhood models',
and two variants of the $\varepsilon$-contamination class (see Section~\ref{sec:alternatives:neighbourhood}) are studied
that, however, are not satisfactorily according to the desiderata the authors had established in \S 2.
The model they advocate in \S 5 for this situation %if substantial prior information is at hand
is instead a density ratio class that
can be seen as a special case of the approach by \textcite[see below]{2011:rinderknecht:diss},
where $l(\vartheta)$ is proportional to the conjugate normal distribution,
and $u(\vartheta)$ is the improper uniform density $u(\vartheta) \propto 1$ \parencite[\S 4.3]{1991:pericchi}.
It shows a favourable behaviour similar to the model we argued for in Section~\ref{sec:pdc-sensitivity}
(which is discussed in more detail in Section~\ref{sec:jstp} below).


\subsection{Other Approaches Using Sets of Priors}
\label{sec:alternatives:other}

%comparison with other approaches based on sets of probability distributions
%then sets of priors in general: discrete models (comparison whitcomb),
In this section, we will give a short overview on \textcite[\S 4]{2011:rinderknecht:diss},
discussing a density ratio class model based on not necessarily conjugate bounding functions,
and then comment on an approach based on the discretisation of the parameter space
\parencite{2005:whitcomb}.%
\footnote{A further, very recent, contribution on Bayesian inference with sets of priors
is \textcite{2013:mangilibenavoli}, modelling prior near-ignorance on the unit simplex
by several sets of non-conjugate priors which provide an alternative to the IDM.}

\medskip

\textcite[\S 4]{2011:rinderknecht:diss} presents a model for inferences 
based on sets of priors of the form of a density-ratio class,
where the bounding functions $l(\vartheta)$ and $u(\vartheta)$ are parametric, but not necessarily conjugate
(we already mentioned his work with respect to elicitation in footnote~\ref{foot:rinderknecht}, page~\pageref{foot:rinderknecht}).
It is demonstrated that also marginals derived from a density ratio class with a multivariate parameter
take again the form of density ratio classes, and can be calculated straight-forwardly.
Also, it is shown that deduced quantities (like probabilistic predictions) derived from the set of posteriors
can be framed as a density ratio class.
However, while for quantities that are a deterministic function of the model parameters
the resulting density ratio class is exact, the bounding functions %$l$ and $u$
for probabilistic predictions are too wide, thus providing a conservative approximation for
the exact set of posterior predictive distributions \parencite[\S 4.2.4]{2011:rinderknecht:diss}.

The theoretical results are implemented and demonstrated via an example,
a model for prediction of a certain type of river biomass that depends on six model parameters.
Prior sets for these parameters were elicited from an expert,
and combined with data from surveys of different streams.
As the priors seem to be non-conjugate (there is no reference to the respective likelihoods in the publication),
joint and marginal posteriors, along with biomass predicitions, were calculated using
Markov Chain Monte Carlo (MCMC) techniques \parencite[see, e.g.,][]{1998:gilks}.
Interestingly, \textcite[\S 4.3]{2011:rinderknecht:diss} shows that
MCMC results for a single precise distribution can be used to approximate the set of posterior distributions,
such that computational burden for the density ratio class is only marginally more extensive than in case of a precise prior.

Results are reasonably precise if prior information for only one of the six model parameters
is modelled by a density ratio class, and for the other by precise probability distributions.
If prior information for each of the six parameters is modelled by a density ratio class and combined independently,
results are higly imprecise and of no practical use;
the posterior marginals are even much more imprecise than their prior counterparts.%
\footnote{In the example, the marginal posterior lower bounding functions $l(\vartheta\mid\x)$ are indistinguishable
from zero if plotted in the same coordinate system as their respective the upper bounding functions $u(\vartheta\mid\x)$,
such that the posterior set of distributions contains also nearly uniform distributions.}
This undesirable phenomenon is called \emph{dilation} \parencite[see][]{1993:seidenfeld}.
Here, it seems to result primarily from a kind of `curse of dimension',
but may also be due to the fact that, as noted above, the magnitude of $\mathcal{M}$ does not decrease with $n$
for density ratio classes updated via the Generalised Bayes' Rule.

Although being quite attractive for problems with a one-dimensional parameter
(or where there is sufficient information to model further parameters by precise priors),
the model is currently inadequate for higher-dimensional problems.
This could be adressed through the development of multivariate elicitation procedures,
eliciting also the dependence stucture for model parameters,
or by replacing the independent combination of marginal prior sets
by another strategy \parencite[as mentioned in][\S 5.2]{2011:rinderknecht:diss}.
A solution could be to factor a multivariate prior $p(\vartheta_1,\ldots,\vartheta_p)$
recursively by
\begin{align*}
p(\vartheta_1,\ldots,\vartheta_p)
 &= p(\vartheta_1\mid \vartheta_2,\ldots,\vartheta_p) p(\vartheta_2,\ldots,\vartheta_p) \\
 &= p(\vartheta_1\mid \vartheta_2,\ldots,\vartheta_p) p(\vartheta_2\mid \vartheta_3,\ldots,\vartheta_p)
    \cdots p(\vartheta_{p-1}\mid\vartheta_p) p(\vartheta_p)\,,
\end{align*}
where usually the dependencies can be reduced by a large degree through assumptions of conditional independence.
This is the approach in probabilistic graphical models,
where (in)dependencies between model parameters are visualised by a graph.
Important guidance could be drawn from the vivid research conducted in the area
of imprecise graphical models, also known as \emph{imprecise Bayesian} or \emph{credal networks},
especially with respect to independence concepts and efficient calculations
\parencite[for a recent overview, see][]{itip-ipgms}.

More importantly, however, we find the model unsatisfactory because it offers no clear mechanism %opportunity
to model posterior imprecision in dependence of sample size. % or \pdc***.
%there seems to be no consistent mechanism by which posterior imprecision
The model is thus inable to model prior near-ignorance, and does not exhibit
a natural decrease in imprecision as information accumulates.%
%to reflect \pdc***\ in posterior inferences.%
\footnote{However, a possibly attractive approach could be to
combine ideas from \textcite{1993:coolen} and \textcite{2011:rinderknecht:diss}
in a density ratio class model with non-Bayesian updating
where $c_n$ is defined such that posterior inferences also reflect \pdc.}

Also, except for a very specific special case \parencite[\S 4.3, as discussed above]{1991:pericchi},
for this model there are so far no studies or general results regarding the behaviour in case of \pdc.
This could be a promising topic of research,
given the model described in \textcite[\S 4.3]{1991:pericchi} shows favourable behaviour,
while the model by \textcite{1993:coolen} does not.
Our conjecture is that the difference, or the ratio, of $u(\vartheta)$ to $l(\vartheta)$
must vary with $\vartheta$ to provide \pdc\ sensitivity,
where \textcite[\S 4.3]{1991:pericchi} represents an extreme case,
combining a $l(\vartheta)$ proportional to the light-tailed normal distribution %(having light tails)
with $u(\vartheta) \propto 1$ that gives the most heavy tails possible.

\medskip

\textcite{2005:whitcomb} studies imprecise Bayesian inference in discrete parameter spaces.
The inference procedure is illustrated with three examples
where substantial prior information is combined with relatively few observations,
and its results are discussed with a focus on the relation of prior to posterior imprecision.

Contrary to the other studies mentioned so far,
Whitcomb considers a discretised parameter space,
i.e., there is only a finite number of values $\vartheta_1, \ldots, \vartheta_m$ the parameter $\vartheta$ can assume,
and uses a linear programming formulation of the Generalised Bayes' Rule
to derive posterior inferences from discrete prior distributions \parencite[\S 3]{2005:whitcomb}. %(\S 3)
These discrete priors are derived from expert elicitations,
given either as lower and upper bounds for $p(\vartheta_j), j=1,\ldots,m$,
or as lower and upper bounds for probability ratios $p(\vartheta_j)/p(\vartheta_{j'}), j\in\{1,\ldots,m\} \backslash j'$,
where one of the parameter values $\theta_1, \ldots, \theta_m$ serves as pivot. %$\vartheta_{j'}$.
From the latter, Whitcomb then derives lower and upper bounds for $p(\vartheta_j), j=1,\ldots,m$,
%through a linear programming 
such that in both cases the set of prior probability functions is given by
\begin{align*}
\mathcal{M} &= \{ p(\vartheta) \mid \ul{p}_j \le p(\vartheta_j) \le \ol{p}_j\ \forall j=1,\ldots,m \}\,,
\end{align*}
where $\ul{p}_j$ and $\ol{p}_j$ are the lower and upper (indirectly) elicited bounds for $p(\vartheta_j)$, respectively.
As a summary measure for imprecision, Whitcomb chooses $\Delta = \sum_{j=1}^m \ol{p}_j-\ul{p}_j$.

We focus here on the example of a reliability analysis problem
regarding the mean time to failure $\theta$ of a technical component \parencite[\S 4.1]{2005:whitcomb}, %(\S 4.1),
studying the influence of several hypothetical data sets on posterior imprecision.
The hypothetical data is assumed to come from a life testing experiment
with observations following an exponential distribution,
such that each data set can be represented by its exponential likelihood.
Likelihoods based on a mean in agreement
and in conflict with the elicited prior are considered,%
\footnote{However, as \textcite[\S 4.3]{2011:krautenbacher} shows, the mean meant to be in agreement with
the prior specifications is actually outside the interval $\big[\El[\theta], \Eu[\theta]\big]$
derived from the prior probability specifications. We discuss this work below.}
with three sample sizes $n=2,6,10$ in each group.
For the resulting six posteriors, imprecision in the probability intervals for the discrete parameter values
is compared to likewise imprecision in the prior.

In accordance to intuition, imprecision $\Delta$ decreases with the sample size
for those likelihoods based on a mean in agreement with the elicited prior,
the posterior probability intervals being more precise than the respective prior intervals.
For the likelihoods based on a mean in contrast with the prior, the picture is different.
Here, for $n=2$ and $n=6$, posterior imprecision is instead larger than prior imprecision,
mirroring the conflict of information from prior and data.
$\Delta$ is largest for $n=6$,
indicating that in this case prior and data seem to have a similar weight.%
\footnote{In contrast to the models discribed in Section~\ref{sec:generalmodel}
where the parameter $\nz$ clearly communicates the weight of the prior as compared to the sample,
the prior is here defined non-parametric and does not entail any parameters
by which the weight of an elicited prior can be gauged.}
Only for the largest sample size $n=10$, posterior imprecision is less than prior imprecision,
indicating that the prior is now overwhelmed by the data.
Strangely, posterior imprecision for this likelihood is even less than
for the likelihood based on the same sample size and a mean in agreement with the prior.
This seems somehow unintuitive, and could be due to specifics of the elicited prior probability intervals,
or could be an artefact of the relatively low number of distinct parameter values $m = 7$.

\textcite[\S 4]{2011:krautenbacher} compared the results from this example %\parencite{2005:whitcomb}
with a model of the framework from Section~\ref{sec:generalmodel}
(model type \ref{enum:rectangular}, see also Section~\ref{sec:4-gw-071216} below).
To derive a prior parameter set $\PZ$ from the expert assessments given by \textcite[Table~I]{2005:whitcomb}, %in terms of $\theta$,
he suggested and implemented an algorithm that determined $\PZ$
by searching over a parameter grid, starting from a precise distribution
in accordance with the prior probability intervals.
His results are generally similar to those of \textcite[\S 4.1]{2005:whitcomb},
but with some interesting differences in the details.

As the conjugate prior model is formulated in terms of $\lambda = 1/\theta$,
%In order to compare same with same,
Krautenbacher first calculates 
%lower and upper cumulative density functions,
expectation intervals and (approximate) unions of highest density intervals for $\lambda$
based on the discrete prior and posteriors of Whitcomb.
These are then compared with
expectation intervals and unions of highest density intervals for $\lambda$
derived from the determined $\PZ$.% and the different $\PN$.%
\footnote{See also Example~\ref{ex:ymodel-nv}, page~\pageref{ex:ymodel-nv},
for calculation of unions of highest density intervals in these models.}

In Whitcomb's model, imprecision as regarded through highest density and expectation intervals for $\lambda$ is
naturally somewhat different from the imprecision $\Delta$ based on $\theta$.
All posterior expectation intervals are shorter than the prior expectation interval;
for the prior-data agreement case, expectation interval results are similar to those based on $\Delta$,
with $\big[\Eu[\lambda]-\El[\lambda]\big]$ decreasing according to sample size.
For the prior-data conflict situations, however, similarities vanish:
while $\Delta$ increased for $n=2$ and $n=6$, and dropped sharply for $n=10$,
$\big[\Eu[\lambda]-\El[\lambda]\big]$ is decreasing with $n$ just like in the prior-data agreement case,
but with now even shorter intervals.
However, with regards to the skewness of Whitcombs's posteriors in terms of $\lambda$
\parencite[depicted in][Abb.~24, p.~62]{2011:krautenbacher},
the expectation intervals are probably misleading here.
Unions of highest density intervals for $\lambda$, which are considered as an alternative,
can be determined only very coarsely, as the number of distinct parameter values in Whitcomb's model is very low.
%For these, the picture is more as expected:
Nevertheless, results for these are more intuitive. %Here, the picture is righted:
Starting from length $2.00$ of the prior HD interval,
posterior HD intervals get shorter with growing sample size in both groups,
and the intervals in case of prior-data conflict are always larger
than their counterparts in the prior-data agreement case (see Table~\ref{tab:hdintervals}).

In the conjugate-based model, expectation intervals instead behave as expected.
Krautenbacher derived the prior parameter set as $\PZ = [2.39, 2.85] \times [2.91, 4.08]$;
however, the set of priors $\MZ$ based on $\PZ$ fit the constraints posed by Whitcomb's prior probability intervals
not very well \parencite[see][Abb.~20]{2011:krautenbacher};
the conjugate Gamma distributions do not seem to capture all aspects of the prior information in this case.
%Nevertheless, 
%Keeping this in mind,
As is clear from the general properties described in Section~\ref{sec:pdc-sensitivity},
imprecision as measured by $\big[\Eu[\lambda]-\El[\lambda]\big]$ decreases with sample size,
with slightly higher imprecision in the prior-data conflict case. % as expected.
Owing to the caveat above, we will not compare these with the expectation intervals
from Whitcomb's model, and instead look more closely on the HD intervals from both models,
as given in Table~\ref{tab:hdintervals}.

\begin{table}
\centering
\begin{tabular}{lrcccccc}
\toprule
&     & \multicolumn{3}{c}{\textcite[\S 4.1]{2005:whitcomb}} & \multicolumn{3}{c}{\textcite[\S 4]{2011:krautenbacher}} \\%[1.2ex]
\midrule
& $n$ & \multicolumn{2}{c}{HD interval} & $\Delta_\text{HD}$ & \multicolumn{2}{c}{HD interval} & $\Delta_\text{HD}$ \\%[1.2ex]
\cmidrule{3-4} \cmidrule{6-7}
prior &  0 & 0.00 & 2.00 & 2.00 & 0.05 & 1.01 & 0.96 \\[0.5ex]
pda   &  2 & 0.10 & 1.00 & 0.90 & 0.12 & 1.07 & 0.95 \\
      &  6 & 0.20 & 1.00 & 0.80 & 0.22 & 1.09 & 0.87 \\
      & 10 & 0.25 & 1.00 & 0.75 & 0.30 & 1.09 & 0.79 \\[0.5ex]
pdc   &  2 & 0.20 & 2.00 & 1.80 & 0.14 & 1.39 & 1.25 \\
      &  6 & 0.33 & 2.00 & 1.67 & 0.34 & 2.01 & 1.67 \\
      & 10 & 0.50 & 2.00 & 1.50 & 0.54 & 2.52 & 1.98 \\
\bottomrule
\end{tabular}
\caption[Highest density intervals for $\lambda$ based on the discrete model
(Whitcomb 2005, \S 4.1) and on the conjugate model (Krautenbacher 2011, \S 4).]%
%\parencite[\S 4.1]{2005:whitcomb} and on the conjugate model \parencite[\S 4]{2011:krautenbacher}]%
{Highest density intervals for $\lambda$ based on the discrete model
\parencite[\S 4.1, left]{2005:whitcomb} and on the conjugate model \parencite[\S 4, right]{2011:krautenbacher},
as given in \textcite[Tab.~2, Tab.~3]{2011:krautenbacher}.
$\Delta_\text{HD}$ gives the length of the HD interval;
`pda' indicates posterior intervals for data in agreement with the prior,
`pdc' indicates posterior intervals for the situation of prior-data conflict.}
\label{tab:hdintervals}
\end{table}

Although the prior HD interval of the discrete model has double the length of the conjugate model,
posterior HD intervals are surprisingly similar in length and position
for the prior-data agreement case.
The conjugate-based posterior HD interval lengths in case of prior-data conflict are,
in contrast to the discrete model, growing with $n$;
from comparison with the expectation intervals that behave as expected,
we think that this unintuitive result can be appropriated
to peculiarities of the Gamma distribution.%
\footnote{While for the prior variance holds $\V(\lambda) \in [0.028, 0.070]$,
the variance for the posterior with $n=10$ in case of prior-data conflict ranges in $[0.120, 0.263]$.
The upper variance thus almost quadruples, leading to a very wide posterior HD interval.}

In summary, both models show more or less adequate results,
with the discrete model showing some unintuitive behaviour with respect to the expectation intervals,
and the conjugate model with respect to HD intervals.
Although the conjugate model behaves, as ensured by the general results, `right' in terms of expectation intervals,
the picture can be different for other inferences, depending on the functional form of the conjugate distributions.
The example here makes it clear that, in considering expectations only
(as is often done in the literature on imprecise probability methods,
being based on the notion of previsions), important effects on inference can be obscured.
On the other hand,
a prerequisite for a conjugate model to give meaningful posterior inferenes is %that
%one should keep in mind that the use conjugate model requires
that the parametric priors are indeed a good model for the prior information at hand.
For this example, this seems not to be the case.

A model based on a discretised parameter space offers more flexibility,
at the cost of higher computational complexity.
For the approach by Whitcomb, it also difficult to discern the effects of a chosen prior
in interplay with a parametric likelihood.%
\footnote{As mentioned above, there is no natural summary measure
giving the weight of the information encoded in the prior in comparison in the data,
as is given by $\nz$ in the conjugate models.}
This could be tackled by considering more refined elicitation strategies involving `pre-posterior' elements,
in which the analyst is asked to indicate what she is willing to learn from hypothetical data
(see Section~\ref{sec:othershapes}, where such a strategy is used to develop a shape for $\PZ$).

The question whether a discrete, nonparameteric model or instead a parametric, often continuous, model should be used
is widespread in statistical inference in general.
In traditional statistics, absolutely continuous distributions are usually employed
when inference using discrete distributions becomes too complex,
typically approximating a non-parametric model with a parametric one.%
\footnote{As an example, consider the test for independence in contingency tables.
Fisher's exact test, a non-parametric test using a permutation argument (thus resulting in a discrete distribution),
can become difficult to calculate for large samples.
An alternative is then the chi-squared test that is based on the continuous, one-parametric $\chi^2(df)$ distribution,
which, for large samples, is a good approximation of the distribution
of the $\chi^2$ test statistic then used to determine the test decision.}
Similarly, the algorithms to compute posterior credal sets and inferences for discrete models,
often framed via the alternative model formulation as conditional lower previsions
\parencite[see, e.g.,][]{itip-computation},
may easily become unfeasible for large $m$;
the alternative is then to consider sets of continuous prior distributions
like in the model framework from Section~\ref{sec:generalmodel}.


\medskip

After having reviewed some models in alternative to the model framework from Section~\ref{sec:generalmodel},
we will now study some examples for models from this framework in more detail.
Section~\ref{sec:jstp} discusses models of type~(\ref{enum:rectangular}) (p.~\pageref{enum:rectangular}),
and a software implementation is briefly described in Section~\ref{sec:luck}.
Section~\ref{sec:isipta11} then presents a model of type~(\ref{enum:generalset}) (p.~\pageref{enum:generalset}),
along with a fundamentally different approach that combines (posterior) inferences from two different models.

