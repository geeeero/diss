\chapter{Appendix}
\label{cha:appendix}


\section{Bayesian Linear Regression: Different Conjugate Models and Their (In)Sensi\-ti\-vi\-ty to Prior-Data Conflict}
\label{sec:festschrift}

This section reproduces the work
``Bayesian Linear Regression --- Different Conjugate Models and Their (In)Sensitivity to Prior-Data Conflict'',
published as technical report no.~69 at the Department of Statistics of Ludwig-Maximilians-University Munich (LMU)
\parencite{Walter2009b}. This technical report is a substantially extended version of a contribution to
``Statistical Modelling and Regression Structures: Festschrift in Honour of Ludwig Fahrmeir'' \parencite{Walter2010a}.
As such, it is reproduced here almost verbatim, except for some minor rewording,
and the addition of some comments and footnotes linking this work to other parts of this thesis.
Furthermore, the notation was changed slightly towards the one introduced in Section~\ref{sec:regularconjugates}
(most importantly, denoting posterior parameters with upper index ${}\un$ instead of ${}^{(1)}$,
e.g., writing $\yn$ instead of $y^{(1)}$),
and citations were updated and changed towards the style employed throughout this thesis.

Here, two approaches for
Bayesian linear regression modeling based on conjugate priors are
considered in detail, namely the standard approach, described, e.g., in
\textcite{2013:fahrmeier-kneib-lang-marx}, and an alternative adoption of the
general construction procedure for exponential family sampling
models. We recognize that --- in contrast to some standard i.i.d.\
models like the scaled normal model and the Beta-Binomial or
Dirichlet-Multinomial model, where prior-data conflict is completely
ignored --- the models may show some reaction to prior-data conflict,
however in a rather unspecific way. Finally we briefly sketch the
extension to a corresponding imprecise probability model, where,
%by enlarging the underlying set of distributions and the imprecision of the resulting interval-valued probabilities,
by considering sets of prior distributions instead of a single prior,
prior-data conflict can be handled in a very appealing and intuitive way.


\subsection{Introduction}
\label{sec:festschrift-intro}

Regression analysis is a central tool in applied statistics that
aims to answer the omnipresent question how certain variables
(called covariates, confounders, regressors, stimulus or
independent variables, here denoted by $\x$) influence a certain
outcome (called response or dependent variable, here denoted by $z$).
Due to the complexity of real-life data situations, basic linear
regression models, where the expectation of the outcome $z_i$ simply equals the linear predictor $\x_i^\tra\bbeta$,
have been generalized in numerous ways, ranging from generalised linear
models (\cite{2001:fahrmeier-tutz}, see also \cite{1985:fahrmeir-kaufmann} for classical work on asymptotics)
for non-normal distributions of $z_i\mid \x_i$, or linear mixed models %(LMM, zitate)
allowing the inclusion of clustered observations, over semi- and nonparametric models
\parencite{2009:kauermann,2007:fahrmeir,2007:scheipl}, up to
generalised additive (mixed) models and structured
additive regression \parencite{2009:fahrmeir,2006:fahrmeir,2007:kneib}.

Estimation in such highly complex models may be based on
different estimation techniques such as (quasi-) likelihood, general
estimation equations (GEE) or Bayesian methods. Especially the
latter offer in some cases the only way to attain a reasonable
estimate of the model parameters, due to the possibility to include
some sort of prior knowledge about these parameters, for instance by
``borrowing strength'' \parencite[e.g.,][]{1996:higgins}.

The tractability of large scale models with their ever increasing complexity
of the underlying models and data sets should not obscure that
still many methodological issues are a matter of debate.
Since the early days of modern Bayesian inference one central issue has,
of course, been the potentially strong dependence of the inferences on the prior.
In particular in situations where data is scarce or unreliable, the
actual estimate obtained by Bayesian techniques may rely heavily on
the shape of prior knowledge, expressed as prior probability
distributions on the model parameters. %parameters of interest.
Recently, new arguments came into this debate by new methods
for detecting and investigating \emph{prior-data conflict} \parencite{2006:evans,2008:bousquet},
i.e. situations where  ``[\ldots] the observed data is surprising
in the light of the sampling model and the prior,
[so that] we must be at least suspicious about the validity
of inferences drawn.'' \parencite[p.~893]{2006:evans}%
\footnote{See also the discussion on prior-data conflict in
Sections~\ref{sec:motivation:pdc}, \ref{sec:pdc-sensitivity},
and in the paper reproduced in Section~\ref{sec:jstp}.}

The present contribution investigates the sensitivity of inferences on potential prior-data conflict:
What happens in detail to the posterior distribution, and the estimates derived from it, if prior knowledge and
what the data indicates are severely conflicting?
If the sample size $n$ is not sufficiently large to discard the
possibly erroneous prior knowledge and thus to rely on data only,
prior-data conflict should affect the inference and
should  --- intuitively and informally ---  result in an increased degree
of uncertainty in posterior inference. Probably most statisticians would thus expect
a higher variance of the posterior distribution in situations of prior-data conflict.

However, this is by no means automatically the case,
in particular when adopting conjugate prior models,
which are often used when data are
scarce, where only strong prior beliefs allow for a reasonably
precise answer in inference. Two simple and prominent examples
of complete insensitivity to prior-data conflict are recalled in
Section~\ref{sec:iid}: i.i.d.\ inferences on the mean of a scaled normal distribution
and on the probability distribution of a categorical variable by the Dirichlet-Multinomial model.%
\footnote{See the description of these two models in Sections~\ref{sec:norm-norm} and \ref{sec:diri-multi}, respectively.}

Sections~\ref{sec:scp} and \ref{sec:cccp} extend
the question of (in)sensitivity to prior-data to regression models.
We confine attention to linear regression analysis with conjugate priors,
because --- contrary to the more advanced regression model classes ---
the linear model still allows a fully analytical access, making it possible to understand
potential restrictions imposed by the model in detail.
We discuss and compare two different conjugate models:
\begin{enumerate}[(i)]
\item the standard conjugate prior (SCP, Section~\ref{sec:scp}) as described in \textcite{2013:fahrmeier-kneib-lang-marx} or,
in more detail, in \textcite{1994:ohagan}; and
\item a conjugate prior, called ``canonically constructed conjugate prior'' (CCCP, Section \ref{sec:cccp}) in the following,
which is derived by a general method used to construct conjugate priors to sample distributions that
belong to a certain class of exponential families, described, e.g., in \textcite{2000:bernardosmith}.%
\footnote{This is the regular conjugate framework of Section~\ref{sec:regularconjugates}.}
\end{enumerate}
Whereas the former is the more general prior model, allowing for
a very flexible modeling of prior information (which might be welcome or not),
the latter allows only a strongly restricted covariance structure for $\bbeta$,
however offering a clearer insight in some aspects of the update process.

In a nutshell, the result is that both conjugate models do react
to prior-data conflict by an enlarged factor to the variance-covariance matrix
of the distribution on the regression coefficients $\bbeta$; however, this reaction is unspecific, as it affects the
variance and covariances of all components of $\bbeta$ in a uniform way
--- even if the conflict occurs only in one single component.

Probably such an unspecific reaction of the variance is the most a (classical)
Bayesian statistician can hope for, and traditional probability theory based on precise probabilities can offer.
Indeed, \textcite{1987:kyburg:ess} notes that
\begin{quotation}
\begin{small}
%``
[\ldots] there appears to be no way, within the theory, of
distinguishing between the cases in which there are good statistical grounds
for accepting a prior distribution, and cases in which the prior distribution
reflects merely ungrounded personal opinion.%''\\
\end{small}
\end{quotation}
and the same applies, in essence, to the posterior distribution.

A more sophisticated modeling would need a more elaborated concept
of imprecision than is actually provided by looking at the variance
(or other characteristics) of a (precise) probability distribution.
Indeed, recently the theory of imprecise probabilities \parencite{1991:walley,2001:weichselberger}
is gaining strong momentum.%
\footnote{See Section~\ref{sec:ip-intro} for a short exposition of the theoretical foundations
and motivations for the use of imprecise probability in statistical inference.}
It emerged as a general methodology to cope
with the multidimensional character of uncertainty, also reacting
to recent insights and developments in decision theory\footnote{See
\textcite{2005:hsu-bhatt} for a neuro science corroboration of the constitutive
difference of stochastic and non-stochastic aspects of uncertainty
in human decision making, in the tradition of Ellsberg's \parencite*{1961:ellsberg} seminal experiments.}
and artificial intelligence,\footnote{See, e.g., \textcite{1996:walley::expert}
for the use of imprecise probability methods in expert systems.}
where the exclusive role of probability
as a methodology for handling uncertainty has eloquently been rejected \parencite{1999:klir}:
\begin{quotation}
\begin{small}
%``
For three hundred years [\ldots] uncertainty was conceived solely in
terms of probability theory. This seemingly unique connection
between uncertainty and probability is now challenged [\ldots by several
other] theories, which are demonstrably capable of characterizing
situations under uncertainty. [\ldots]

[\ldots] it has become clear that there are several distinct types of
uncertainty. That is, it was realized that uncertainty is a
multidimensional concept. [\ldots\ That] multidimensional nature of
uncertainty was obscured when uncertainty was conceived solely in
terms of probability theory, in which it is manifested by only one
of its dimensions.
\end{small}
\end{quotation}

Current applications include, among many other, risk analysis,
reliability modeling and decision theory, see \textcite{2009:ISIPTA},
\textcite{2011:ISIPTA} and \textcite{2009:CoolenSchrijner} for recent collections on the subject.%
\footnote{See also, e.g., the list of applications of the IDM given in Section~\ref{sec:idm-and-near-ignorance}.}
As a welcome byproduct, imprecise probability models also provide
a formal superstructure on models considered in robust Bayesian analysis \parencite{2000:rios}, and
frequentist robust statistic in the tradition of \textcite{1973:huberstrassen},
see also \textcite{2009:augustin-hable} for a review.

By considering \emph{sets} of distributions, and corresponding
interval-valued probabilities for events, imprecise probability models allow
to express the quality of the underlying knowledge in an elegant way.
The higher the ambiguity, the larger c.p.\ the sets.
The traditional concept of probability is contained as a special case,
appropriate if and only if there is perfect stochastic information.
This methodology allows also for a natural handling of prior-data conflict.
If prior and data are in conflict, the set of posterior distributions are enlarged,
and inferences become more cautious.%
\footnote{For more details on the topic of imprecision and \pdc, see Section~\ref{sec:jstp}.}

In Section~\ref{sec:discussion-festschrift}, we briefly report that the CCCP model has a structure
that allows a direct extension to an imprecise probability model
along the lines of Quaeghebeur and de Cooman's \parencite*{2005:quaeghebeurcooman} imprecise probability models
for i.i.d.\ exponential family models. Extending the models further
by applying arguments from \textcite[see Section~\ref{sec:jstp}]{Walter2009a}
%\footnote{Reproduced in Section~\ref{sec:jstp}.}
yields a powerful generalisation
of the linear regression model that is also capable of a component-specific reaction to prior-data conflict.

\subsection{Prior-data Conflict in the i.i.d.\ Case}
\label{sec:iid}

As a simple demonstration that conjugate models might not react to
prior-data conflict reasonably, inference on the mean of data from a
scaled normal distribution and inference on the category
probabilities in multinomial sampling will be described in the
%following examples~\ref{ex:scn} and \ref{ex:mult}. %\\
following two subsections.

\subsubsection{Samples from a scaled Normal distribution} %$\norm(\mu,1)$}
\label{sec:ex-scn}
The conjugate distribution to an i.i.d.-sample $\x$ of
size $n$ from a scaled normal distribution with mean $\mu$, denoted
by $\norm(\mu,1)$ %as constructed by this method
is a normal distribution with mean $\mu\uz$ and variance ${\sigma\uz}^2$\footnote{Here, and in the
following, parameters of a prior distribution will be denoted by an upper index ${}\uz$, whereas
parameters of the respective posterior distribution by an upper index ${}\un$.}.
The posterior is then again a normal distribution with the following updated parameters:%\vspace*{-4ex}
\footnote{This is the Normal-Normal model from Section~\ref{sec:norm-norm}, where
$\sigma_0^2 = 1$, $\yz = \mu\uz$, and $\nz = 1/{\sigma\uz}^2$.}
\begin{align}
\mu\un &=  \frac{ \frac{1}{n}   }{ \frac{1}{n} + {\sigma\uz}^2 } \mu\uz
          +\frac{ {\sigma\uz}^2 }{ \frac{1}{n} + {\sigma\uz}^2 } \bar{x}
        =  \frac{ \frac{1}{{\sigma\uz}^2} }{ \frac{1}{{\sigma\uz}^2} + n } \mu\uz
          +\frac{ n }                      { \frac{1}{{\sigma\uz}^2} + n } \bar{x} \label{equ:scn-mu1}\\
{\sigma\un}^2 &= \frac{ {\sigma\uz}^2 \cdot \frac{1}{n} }{ {\sigma\uz}^2 + \frac{1}{n} }
               = \frac{1}{ \frac{1}{{\sigma\uz}^2} + n }\,. \label{equ:scn-sig1}
\end{align}
The posterior expectation (and mode) is thus a simple weighted average of the prior mean
$\mu\uz$ and the estimation from data $\bar{x}$, with weights
%$\frac{1}{{\sigma\uz}^2}$
$1/{\sigma\uz}^2$ and $n$, respectively.%
\footnote{The reason for using these seemingly strange weights will become clear later.}
The variance of the posterior distribution is getting smaller automatically.

Now, in a situation where data is scarce, but with prior information one is very confident about,
one would choose a low value for ${\sigma\uz}^2$, thus resulting in a high weight
for the prior mean $\mu\uz$ in the calculation of $\mu\un$.
The posterior distribution will be centered around a mean between $\mu\uz$
and $\bar{x}$, and it will be even more pointed as the prior, because
${\sigma\un}^2$ is considerably smaller than ${\sigma\uz}^2$, the factor to
${\sigma\uz}^2$ in \eqref{equ:scn-sig1} being quite smaller than one.

The posterior basically would thus say that one can be quite sure that the mean $\mu$
is around $\mu\un$, regardless if $\mu\uz$ and $\bar{x}$ were near to each other
or not, where the latter would be a strong hint for prior-data conflict.
The posterior variance does not depend on this; the posterior distribution is thus
insensitive to prior-data conflict.

Even if one is not so confident about one's prior knowledge and thus assigning a
relatively large variance to the prior, the posterior mean is less strongly influenced
by the prior mean, but the posterior variance still is getting smaller, no matter
if the data support the prior information or not.
%and this is the point: it gets smaller by the same amount / factor no matter
%if the prior knowledge
%\end{example}

The same insensitivity appears also in the widely used Dirichlet-Multinomial model
as presented in the following subsection:

\subsubsection{Samples from a Multinomial distribution} %$\mult(\btheta)$}
\label{sec:ex-mult}
Given a sample of size $n$ from a multinomial distribution, with
probabilities $\theta_j$ for categories or classes $j= 1,\ldots,k$, subsumed in
the vectorial parameter $\btheta$ (with $\sum_{j=1}^k \theta_j = 1$),
the conjugate prior on $\btheta$ is a Dirichlet distribution $\dir(\balpha)$.
%Written in terms of a reparameterisation used in , e.g. in \textcite{1996:walley::idm},
Written in terms of the canonical parameters $\nz$ and $\yz$ as in Section~\ref{sec:diri-multi},
%$\alpha\uz_j = s\uz \cdot t\uz_j$, such that $\sum_{j=1}^k t\uz_j = 1$, $(t\uz_1,\ldots,t\uz_k)^\tra =: t\uz$,
$\alpha_j = \nz \cdot \yz_j$, such that $\sum_{j=1}^k \yz_j = 1$, $(\yz_1,\ldots,\yz_k)^\tra =: \byz$.
Recall that the components of $\byz$ have a direct
interpretation as prior class probabilities, whereas $\nz$ is a parameter
indicating the confidence in the values of $\byz$, similar to the inverse
variance as in Section~\ref{sec:ex-scn}
(the quantity $\nz$ will appear also in Section \ref{sec:cccp}).%
\footnote{If $\btheta \sim \dir(\nz, \byz)$, then
$\V(\theta_j) = \frac{\yz_j (1-\yz_j)}{\nz+1}$. If $\nz$ is high,
then the variances of $\btheta$ will become low, thus indicating high
confidence in the chosen values of $\byz$.}

As seen in Section~\ref{sec:diri-multi},
the posterior distribution, obtained after updating via Bayes' Rule with a sample vector $\vec{n} = (n_1,\ldots,n_k)$,
$\sum_{j=1}^k n_j = n$ collecting the observed counts in each category,
is a Dirichlet distribution with parameters
\begin{align*}
\yn_j &= \frac{\nz}{\nz + n} \yz_j + \frac{n}{\nz + n} \cdot \frac{n_j}{n}\,, &
\nn   &= \nz + n \,.
\end{align*}
The posterior class probabilities $\byn$ are calculated as a weighted mean
of the prior class probabilities $\byz$ and $\frac{n_j}{n}$, the proportion in the sample,
with weights $\nz$ and $n$, respectively; the confidence parameter $\nz$ is
incremented by the sample size $n$.

Also here, there is no systematic reaction to prior-data conflict. The posterior
variance for each class probability $\theta_j$ is %calculates as
\begin{align*}
\V(\theta_j\mid \vec{n}) &= \frac{\yn_j (1 - \yn_j)}{\nn+1}
                          = \frac{\yn_j (1 - \yn_j)}{\nz+n+1}\,.
\end{align*}
The posterior variance depends heavily on $\yn_j (1 - \yn_j)$,
having values between $0$ and $\frac{1}{4}$, which do not change
specifically to prior data conflict. The denominator increases from
$\nz+1$ to $\nz+n+1$.

Imagine a situation with strong prior information suggesting a value of $\yz_j = 0.25$,
so one could choose $\nz = 5$, resulting in a prior class variance of $\frac{1}{32}$.
Consider a sample of size $n=10$ with all observations belonging to class $j$ (thus $n_j=10$),
being in clear contrast to the prior information. The posterior class probability
is then $\yn_j = 0.75$, resulting the enumerator value of the class variance to remain constant.
Therefore, due to the increasing denominator, the variance decreases to $\frac{3}{256}$,
in spite of the clear conflict between prior and sample information.
Of course, one can also construct situations where the variance increases, but this
happens only in case of an update of $\yz_j$ towards $\frac{1}{2}$.
If $\yz_j = \frac{1}{2}$, the variance will decrease for any degree of prior-data conflict.


\subsection{The Standard Approach for Bayesian Linear Regression (SCP)}
\label{sec:scp}

The regression model is noted as follows:
\begin{equation*}
z_i = \x_i^\tra\bbeta + \varepsilon_i \,,\quad \x_i \in \reals^{p} \,,\;
\bbeta \in \reals^p \,,\; \varepsilon_i \sim \norm(0,\sigma^2)\,,
\end{equation*}
where $z_i$ is the response, $\x_i$ the vector of the $p$ covariates for observation $i$,
and $\bbeta$ is the $p$-dimensional vector of adjacent regression coefficients.

The vector of regressors $\x_i$ for each observation $i$ is generally
considered to be non-stochastic, thus it holds that $z_i \sim \norm(\x_i^\tra\bbeta,\sigma^2)$,
or, for $n$ i.i.d.\ samples, $\z \sim \norm_n(\X\bbeta,\sigma^2 \mbf{I})$,
i.e., $\z \in \reals^n$, the column vector of the responses $z_i$,
has a multivariate normal distribution with vector of expectations $\X\bbeta$,
where $\X \in \reals^{n \times p}$ is the \emph{design matrix}
(of which row $i$ is the vector of covariates $\x_i^\tra$ for observation $i$),
and matrix of variances and covariances $\sigma^2 \mbf{I}$,
where $\mbf{I}$ is the identity or unit matrix of size $n$.%\\

Without loss of generality, one can either assume $x_{i1} = 1 \; \forall i$ such that
the first component of $\bbeta$ is the intercept parameter,%
\footnote{usually denoted by $\beta_0$; however,
we stay with the numbering $1,\ldots, p$ for the components of $\bbeta$.}
or consider only centered responses $\z$ and standardized covariates
to make the estimation of an intercept unnecessary.

%vorher allgemein: $p(\beta,\,\sigma^2) = p(\beta\mid\sigma^2) p(\sigma^2)$, klassische conjugierte Priori ist NIG, etc...
In Bayesian linear regression analysis, the distribution of the response $\z$ is interpreted as
a distribution of $\z$ given the parameters $\bbeta$ and $\sigma^2$, and %a (class of)
prior distributions on $\bbeta$ and $\sigma^2$ must be considered.
For this, it is convenient to split the joint prior on $\beta$ and $\sigma^2$ as %follows
%\begin{align*}
%p(\beta,\,\sigma^2) &= p(\beta\mid\sigma^2) p(\sigma^2)
%\end{align*}
$p(\bbeta,\sigma^2) = p(\bbeta\mid\sigma^2) p(\sigma^2)$
and to consider conjugate distributions for both parts, respectively.

%\section{**Standard-Modell wie in O'Hagan / FKL}

In the literature, the proposed conjugate prior for $\bbeta\mid\sigma^2$
is a normal distribution with expectation vector $\mz \in \reals^p$ and variance-covariance
matrix $\sigma^2 \Mz$, where $\Mz$ is a symmetric positive definite matrix of
size $p \times p$. The prior on $\sigma^2$ is an inverse gamma distribution
(i.e., $1/\sigma^2$ is gamma distributed) with parameters $\az$ and $\bz$, in the sense that
\begin{align*}
p(\sigma^2) \propto \frac{1}{(\sigma^2)^{\az + 1}} \exp\Big\{ -\frac{\bz}{\sigma^2} \Big\}\,.
\end{align*}
%$p(\sigma^{-2}) \propto (\sigma^{-2})^{a\uz + 1} \exp\{-b\uz \sigma^{-2} \}$.
The joint prior on $\btheta = (\bbeta,\, \sigma^2)^\tra$ is then denoted as a normal-inverse gamma (NIG) distribution.
The derivation of this prior and the proof of its
conjugacy can be found, e.g., in \textcite{2013:fahrmeier-kneib-lang-marx}, or in \textcite{1994:ohagan},
the latter using a different parameterisation of the inverse gamma part,
where $\az = \frac{d}{2}$ and $\bz = \frac{a}{2}$.

For the prior model, it holds thus that (if $\az > 1$ resp.\ $\az > 2$)
\begin{align}
\begin{aligned}
\E[\bbeta\mid\sigma^2] &= \mz \,,                  & \V(\bbeta\mid\sigma^2) &= \sigma^2  \Mz \,, \\
\E[\sigma^2]           &= \frac{\bz}{\az - 1} \,,  & \V(\sigma^2)           &= \frac{(\bz)^2}{(\az - 1)^2 (\az - 2)}\,.
\end{aligned}
\label{equ:scp-priori_betasigma}
\end{align}
As $\sigma^2$ is considered as nuisance parameter, the unconditional distribution
on $\bbeta$ is of central interest, because it subsumes the shape of prior knowledge on $\bbeta$
as expressed by the choice of parameters $\mz$, $\Mz$, $\az$ and $\bz$.
It can be shown that $p(\bbeta)$ is a multivariate noncentral $t$ distribution
with $2\az$ degrees of freedom, location parameter $\mz$ and dispersion parameter $\frac{\bz}{\az} \Mz$,
such that
\begin{align}
\E[\bbeta] &= \mz \,, &  \V(\bbeta) &= \frac{\bz}{\az - 1} \Mz = \E[\sigma^2] \Mz\,.
\label{equ:scp-priori_beta}
\end{align}
The joint posterior distribution $p(\btheta\mid z)$, due to conjugacy, is then
again a normal-inverse gamma distribution with the updated parameters
\begin{align*}
\mn &= \left( {\Mz}^{-1} + \XtX \right)^{-1} \left( {\Mz}^{-1} \mz + \Xtz \right) \,, \\
\Mn &= \left( {\Mz}^{-1} + \XtX \right)^{-1} \,, \\
\an &= \az + \frac{n}{2} \,, \\ %\qquad
\bn &= \bz + \frac{1}{2} \left( \ztz + {\mz}^\tra {\Mz}^{-1} \mz  - {\mn}^\tra {\Mn}^{-1} \mn \right) \,.
\end{align*}
The properties of the posterior distributions can thus be analyzed
by inserting the updated parameters into \eqref{equ:scp-priori_betasigma}
and \eqref{equ:scp-priori_beta}.


\subsubsection{Update of \texorpdfstring{$\bbeta\mid\sigma^2$}{beta|sigma2}}
\label{sec:scp-update1}

The normal distribution part of the joint prior is updated as follows:%\vspace*{-0.5ex}
\begin{align*}
\E[\bbeta\mid\sigma^2, \z] &= \mn \\
                           &= \big( {\Mz}^{-1} + \XtX \big)^{-1} \!\big( {\Mz}^{-1} \mz + \Xtz \big) \\
                           &= (\mbf{I} - \mbf{A})\, \mz + \mbf{A}\, \bls\,,
\end{align*}
where $\mbf{A} = \big({\Mz}^{-1} + \XtX\big)^{-1} \XtX$. The posterior estimate of $\bbeta\mid\sigma^2$
thus can be seen as a matrix-weighted mean of the prior guess and the least-squares estimate $\bls$.
The larger the diagonal elements of $\Mz$ (i.e., the weaker the prior information),
the smaller the elements of ${\Mz}^{-1}$ and thus the `nearer' is $\mbf{A}$ to the identity matrix,
so that the posterior estimate is nearer to the least-squares estimate.

The posterior variance of $\bbeta\mid\sigma^2$ is %calculates as
\begin{align*}
\V(\bbeta\mid\sigma^2, \z) &= \sigma^2 \Mn = \sigma^2 \left( {\Mz}^{-1} + \XtX \right)^{-1}\,.
\end{align*}
As the elements of ${\Mn}^{-1}$ get larger with as compared to ${\Mz}^{-1}$,
the elements of $\Mn$ will, roughly speaking, become smaller than those of $\Mz$,
so that the variance of $\bbeta\mid\sigma^2$ decreases.

Therefore, the updating of $\bbeta\mid\sigma^2$ is obviously insensitive to prior-data conflict,
because the posterior distribution will not become flatter in case of a large
distance between $\E[\bbeta]$ and $\bls$. %due to this automatic reduction of variances.
Actually, as \textcite{1994:ohagan} derives,
for any $\phi = \vec{a}^\tra \bbeta$, $\vec{a} \in \reals^p$, i.e., any linear combination of elements of $\bbeta$, it holds that
$\V(\phi\mid\sigma^2,\z) \leq \V(\phi\mid\sigma^2)$, becoming a strict inequality if
$\X$ has full rank. In particular, the variance of each $\beta_i$ decreases
automatically with the update step.


\subsubsection{Update of \texorpdfstring{$\sigma^2$}{sigma2}}
\label{sec:scp-update2}

It can be shown \parencite{1994:ohagan} that
\begin{align}
\E[\sigma^2\mid \z] &= \frac{2\az - 2}{2\az + n - 2} \E[\sigma^2]
                     + \frac{n - p}   {2\az + n - 2} \sls
                     + \frac{p}       {2\az + n - 2} \spdc\,,
\label{equ:scp-decomposition}
\end{align}
where $\sls = \frac{1}{n-p} (\z - \X\bls)^\tra (\z - \X\bls)$ is
the least-squares based estimate for $\sigma^2$, and
\begin{align*}
\spdc &= \frac{1}{p} (\mz - \bls)^\tra \big(\Mz + (\XtX)^{-1} \big)^{-1} (\mz - \bls)\,.
\end{align*}
For the latter it holds that $\E[\spdc\mid\sigma^2] = \sigma^2$;
the posterior expectation of $\sigma^2$ can thus be seen
as a weighted mean of three estimates:
\begin{enumerate}[(i)]
\item the prior expectation for $\sigma^2$,
\item the least-squares estimate, and
\item an estimate based on a weighted squared difference of the prior mean $\mz$ and $\bls$,
the least-squares estimate for $\bbeta$.
\end{enumerate}
%
The weights depend on $\az$ (one prior parameter for the inverse gamma part),
the sample size $n$, and the dimension of $\bbeta$, respectively.
The role of the first weight gets more plausible when remembering the formula for the prior variance of $\sigma^2$
in \eqref{equ:scp-priori_betasigma}, where $\az$ appears in the denominator. A larger value of $\az$ means
thus smaller prior variance, in turn giving a higher weight for $\E[\sigma^2]$
in the calculation of $\E[\sigma^2\mid \z]$.
The weight to $\sls$ corresponds to the classical degrees of freedom, $n-p$. With the
the sample size approaching infinity, this weight will dominate the others, such that
$\E[\sigma^2\mid \z]$ approaches $\sls$.%, indeed .

Similar results hold for the posterior mode instead of the posterior expectation.

Here, the estimate $\spdc$ allows some reaction to prior-data conflict:
it measures the distance between $\mz$ (prior) and $\bls$ (data)
estimates for $\bbeta$, with a large distance resulting basically in a large value of $\spdc$
and thus an enlarged posterior estimate for $\sigma^2$.

The weighting matrix for the distances is playing an important role as well.
%
The influence of $\Mz$ is as follows: for components of $\bbeta$
one is quite certain about the assignment of $\mz$, the respective diagonal elements
of $\Mz$ will be low, so that these diagonal elements of the weighting matrix
will be high. Therefore, large distances in these dimensions will increase $\spdc$ strongly.
An erroneously high confidence in the prior assumptions on $\bbeta$ is thus
penalised by an increasing posterior estimate for $\sigma^2$.
%
The influence of $\XtX$ interprets as follows: covariates with a low spread
in $x$-values, giving an unstable base for the estimate $\bls$,
will result in low diagonal elements of $\XtX$.
%(for centered $\X$, the diagonals are the covariate's variances).
Via the double inverting, those diagonal elements of the weighting matrix
will remain low and thus give the difference a low weight.
Therefore, $\spdc$ will not excessively increase due to a large difference
in dimensions where the location of $\bls$ is to be taken cum grano salis.

As to be seen in the following subsection, the behavior of $\E[\sigma\mid \z]$ is of high
importance for posterior inferences on $\bbeta$.


\subsubsection{Update of \texorpdfstring{$\bbeta$}{beta}}
\label{sec:scp-update3}

The posterior distribution of $\bbeta$ is again a multivariate $t$, with expectation
\begin{align*}
\E[\bbeta\mid \z] &= \E\big[ \E[\bbeta\mid\sigma^2, \z] \mid \z \big]= \mn
\end{align*}
as described in Section~\ref{sec:scp-update1},
and variance %\vspace*{-2ex}
\begin{align}
\V[\beta\mid \z]
                  &= \frac{\bn}{\an - 1} \Mn \nonumber\\
                  &= \E[\sigma^2\mid \z]\, \Mn  \label{equ:scp-varbeta}\\
                  &= \bigg( \frac{2\az - 2}{2\az + n - 2} \E[\sigma^2]
                           +\frac{n - p}   {2\az + n - 2} \sls
                           +\frac{p}       {2\az + n - 2} \spdc \bigg) \nonumber\\
                  & \hspace*{5ex}\cdot
                     \left( {\Mz}^{-1} + \XtX \right)^{-1} \nonumber\\
                  &= \bigg( \frac{2\az - 2}{2\az + n - 2} \E[\sigma^2]
                           +\frac{n - p}   {2\az + n - 2} \sls
                           +\frac{p}       {2\az + n - 2} \spdc \bigg) \nonumber\\
                  & \hspace*{5ex}\cdot
                     \Big( \Mz - \Mz \X^\tra (\mbf{I} + \X \Mz \X^\tra)^{-1} \X \Mz \Big) \,, \nonumber%
%\vspace*{-2ex}%
\end{align}
not being directly expressible as a function of $\E[\sigma^2] \Mz$,
the prior variance of $\beta$. %\\ %, due to the double inversion of $\bs{M}\uz$.
%\begin{align*}
%\left( {\bs{M}\uz}^{-1} + \XtX \right)^{-1}
% &= \bs{M}\uz - \bs{M}\uz \X^\tra (\mbf{I} + \X \bs{M}\uz \X^\tra)^{-1} \X \bs{M}\uz
%\end{align*}

Due to the effect of $\E[\sigma^2\mid \z]$, the posterior variance-covariance matrix
of $\bbeta$ can increase in case of prior data conflict, if the rise of $\E[\bbeta\mid \z]$
(due to an even stronger rise of $\spdc$) can overcompensate the decrease in the elements of $\Mn$.
However, we see that the effect of prior-data conflict on the posterior variance of $\bbeta$ %distribution
is \emph{globally} and not component-specific; it influences the variances for \emph{all} components of $\bbeta$
with the same amount, even if the conflict was confined only to some or even just one single component.
Taking it to the extremes, if the prior assignment $\mz$ was (more or less) correct in all but one component,
with that one being far out, the posterior variances will increase for all components,
also for the ones with prior assignments that have turned out to be basically correct.

\subsection{An Alternative Approach for Conjugate Priors in Bayesian Linear Regression (CCCP)}
\label{sec:cccp}

%erst allgemein: wie kann priori zu lik konstruiert werden.\\
%dann verkuerzte konstruktion\\
%alles noch be-tilde-n und ent-fett-en\\

In this section, a prior model for $\btheta = (\bbeta,\,\sigma^2)$ will be constructed
along the general construction method for sample distributions that form a
linear, canonical exponential family
\parencite[see the canical conjugates framework in Section~\ref{sec:regularconjugates}, and, e.g.,][]{2000:bernardosmith}.
%
As shown for the examples in Sections~\ref{sec:beta-binom}, \ref{sec:norm-norm} and \ref{sec:diri-multi},
the method is typically used for the i.i.d.\ case, but the likelihood arising from
$\z \sim \norm(\X\bbeta, \sigma^2\mbf{I})$ will be shown to follow the specific
exponential family form as well.

The canonically constructed conjugate prior (CCCP) model will
%The prior constructed along the general method \cite{BernardoSmith} in the following will
%result in the same parametric class of prior distributions, but with a different parametrization and other differences.
%Comments on these differences will be made after the derivation.
also result in a normal-inverse gamma distribution, but with a fixed variance-covariance structure.
The CCCP model is thus a special case of the SCP model, which -- as will be detailed in this subsection --
offers some interesting further insights into the structure of the update step.

%nur andere parametrisierung, und struktur der var-covar-matrix nicht frei w\"{a}hlbar.
%w\"{u}rde aber auch sehr schwierig, Menge von var-covar-matrizen zu definieren, bei denen
%die pos.def. garantiert ist...

%\subsubsection{Derivation of the Conjugate Prior}

The likelihood arising from the distribution of $\z$,
\begin{align*}
\lefteqn{f(\z \mid \bbeta, \sigma^2)}\hspace{5ex}\\
               &= \prod_{i=1}^n f(z_i \mid \bbeta, \sigma^2)\\
               &= \frac{1}{(2\pi)^{\frac{n}{2}} (\sigma^2)^{\frac{n}{2}}}
                  \exp\left\{ -\frac{1}{2 \sigma^2} \sum_{i=1}^n (z_i - \x_i^\tra\bbeta)^2  \right\}\\
               &= \frac{1}{(2\pi)^{\frac{n}{2}} (\sigma^2)^{\frac{n}{2}}}
                  \exp \Big\{ -\frac{1}{2 \sigma^2} (\z -\X\bbeta)^\tra (\z -\X\bbeta) \Big\}\\
               &= \frac{1}{(2\pi)^{\frac{n}{2}}}
                  \exp \Big\{ -\frac{n}{2}\log(\sigma^2) \Big\} \\ 
               & \hspace*{9.4ex}%
                  \exp \Big\{ -\frac{1}{2 \sigma^2} \ztz
                              +\frac{1}{2 \sigma^2} \z^\tra \X\bbeta
                              +\frac{1}{2 \sigma^2} (\X\bbeta)^\tra \z
                              -\frac{1}{2 \sigma^2} (\X\bbeta)^\tra (\X\bbeta) \Big\}\\
               &= \underbrace{ \frac{1}{(2\pi)^{\frac{n}{2}}} }_{ \mbf{a}(\bs{z}) = \prod_{i=1}^n \mbf{a}(z_i)}
                  \exp \Big\{ \underbrace{ \left(\frac{\bbeta}{\sigma^2}\right)^\tra}_{\bpsi_1}
                              \underbrace{ \X^\tra \z }_{ \tau_1(\z) } \,
                              \underbrace{-\frac{1}{\sigma^2}}_{\psi_2}
                              \underbrace{ \frac{1}{2} \ztz }_{\tau_2(\z)} \,
                              \underbrace{-\Big(\frac{1}{2 \sigma^2} \bbeta^\tra \XtX \bbeta
                                          +\frac{n}{2} \log(\sigma^2) \Big)}_{n \mbf{b}(\psi)} \Big\}\,,
\end{align*}
indeed corresponds to the canonical exponential family form%
\footnote{In Equation~\ref{eq:expofam-sampledens}, the integration constant $\mbf{a}(\cdot)$
was omitted.}
\begin{align*}
f(\z\mid\bpsi) &= \mbf{a}(z) \cdot \exp \{\langle \psi, \tau(z) \rangle - n \cdot \mbf{b}(\bpsi)\}\,,
%\label{equ:likelihood}
\end{align*}
where $\bpsi=\bpsi(\bbeta, \sigma^2)$ is a certain function of $\bbeta$ and $\sigma^2$,
the parameters of interest. $\tau(\z)$ is a $p+1$-dimensional sufficient statistic of $\z$ used in the update step. 
Here, we have
\begin{align}
\bpsi          &= \begin{pmatrix} \frac{\bbeta}{\sigma^2} \\ -\frac{1}{\sigma^2} \end{pmatrix}\,, &
\tau(\z)       &= \begin{pmatrix} \Xtz \\ \frac{1}{2} \ztz \end{pmatrix}\,, &
\mbf{b}(\bpsi) &= \frac{1}{2n\sigma^2} \bbeta^\tra \XtX \bbeta + \frac{1}{2}\log(\sigma^2)\,.
\label{equ:cccp-psi}
\end{align}
According to the general construction method, a conjugate prior %(and posterior)
for $\bpsi$ can be obtained from these ingredients by the following equation:%
\footnote{See Equation~\ref{eq:canonicalprior}, where the integration constant $\mbf{c}(\cdot)$ was omitted as well.}
\begin{align}
p(\bpsi) &= \mbf{c}(\nz,\byz)\cdot \exp\left\{ \nz \cdot [\langle \bpsi, \byz \rangle - \mbf{b}(\bpsi)] \right\}\,,
\label{eq:canonicalconjugate-festschrift}
\end{align}
where $\nz$ and $\byz$ are the parameters that define the concrete prior distribution of its distribution family;
%(The parameters for the posterior distribution on $\psi$ will be denoted by an upper index ${}\uo$.)
whereas $\bpsi$ and $\mbf{b}(\bpsi)$ were identified in \eqref{equ:cccp-psi}.
$\mbf{c}(\cdot)$ corresponds to a normalisation factor for the prior.%
\footnote{When applying the general construction method to the two examples from Section~\ref{sec:iid},
the priors as presented there will result,
where $\yz = \mu\uz$ and $\nz = 1/{\sigma\uz}^2$ for the prior to the scaled normal model.
For details of the derivation, see Sections~\ref{sec:norm-norm} and \ref{sec:diri-multi}, respectively.}

Here, the conjugate prior writes as
\begin{align*}
%p(\beta) &= \mbf{c}(n\uz,y\uz)\cdot \exp\left\{ n\uz \cdot [\langle \beta, y\uz \rangle - \mbf{b}(\beta)] \right\}\,.
p(\bpsi) d\bpsi &= \mbf{c}(\nz, \byz) \exp\Big\{ \nz \big[ {\byz}^\tra \begin{pmatrix} \frac{\bbeta}{\sigma^2} \\
                                                                                      -\frac{1}{\sigma^2} \end{pmatrix}
                                                -\frac{1}{2n\sigma^2} \bbeta^\tra \XtX \bbeta
                                                -\frac{1}{2} \log(\sigma^2) \big]\Big\} d\bpsi\,.
\end{align*}
As this is a prior on $\bpsi$, but we want to arrive at a prior on $\btheta = (\bbeta^\tra,\, \sigma^2)^\tra$,
we must transform the density $p(\bpsi)$:
\begin{align*}
p(\btheta) d\btheta &= p(\bpsi) d\bpsi \cdot \left| \mbox{det}\left( \frac{d\bpsi}{d\btheta} \right) \right|
\end{align*}
For the transformation, we need the determinant of the Jacobian matrix $\frac{d\bpsi}{d\btheta}$. 
As it holds that
\begin{align*}
\frac{d\psi_i}{d\theta_j}         &= \frac{1}{d\beta_j}\, \frac{\beta_i}{\sigma^2}
                                   = \begin{cases} 0                  & i \neq j \\
                                                   \frac{1}{\sigma^2} & i = j \end{cases}      & &\forall i,\, j \in \{1,\ldots,p\}\,,\\
\frac{d\psi_{p+1}}{d\theta_j}     &= \frac{1}{d\beta_j}\, \left(-\frac{1}{\sigma^2}\right) = 0 & &\forall j \in \{1,\ldots,p\}\,,\\
\frac{d\psi_i}{d\theta_{p+1}}     &= \frac{1}{d\sigma^2}\, \frac{\beta_i}{\sigma^2} = -\frac{\beta_i}{(\sigma^2)^2} & &\forall i \in \{1,\ldots,p\}\,,\\
\frac{d\psi_{p+1}}{d\theta_{p+1}} &= \frac{1}{d\sigma^2} \, \left(-\frac{1}{\sigma^2}\right)
                                   = \frac{1}{(\sigma^2)^{2}}\,,
\end{align*}
we get
\begin{align*}
\left| \mbox{det}\left( \frac{d\bpsi}{d\btheta} \right) \right| &=
\left| \mbox{det}\left(\begin{array}{cc} \frac{1}{\sigma^2} \mbf{I} &\; -\frac{\bbeta}{(\sigma^2)^{2}} \\[1.5ex] %\hline
                                         \mbf{0}                    &\;  \frac{1}{(\sigma^2)^{2}} \end{array}\right) \right|
 = \frac{1}{(\sigma^2)^{p+2}}\,,
\end{align*}
where $\mbf{I}$ is the $p \times p$ identity matrix, and $\mbf{0}$ a $p$-dimensional row vector of zeroes.
Therefore, the prior on $\btheta = (\bbeta,\, \sigma^2)^\tra$ is
\begin{align}
\begin{split}
\lefteqn{p(\btheta) d\btheta} \hspace{0ex}\\
                   &= p(\bpsi) d\bpsi \cdot \left| \mbox{det}\left( \frac{d\bpsi}{d\btheta} \right) \right| \\
                   &= \mbf{c}(\nz, \byz) \\
                   & \hspace*{3ex}        \exp\Big\{ \nz {\byz_1}^\tra \frac{\bbeta}{\sigma^2}
                                                    -\nz  \yz_2 \frac{1}{\sigma^2}
                                                    -\frac{\nz}{2n\sigma^2} \bbeta^\tra \XtX \bbeta
                                                    -\frac{\nz}{2} \log(\sigma^2)
                                                    - (p+2)        \log(\sigma^2) \Big\}\,, %\nonumber
\end{split}
\label{equ:cccp-ptheta}
\end{align}
where $\byz = \Big({\byz_1}^\tra, \yz_2\Big)^\tra$, and $\byz_1 \in \reals^p$, $\yz_2 \in \posreals$.

$\btheta$ can now be shown to follow a normal-inverse gamma distribution by comparing coefficients.
In doing that, some attention must be paid to the terms proportional to $-1/\sigma^2$ (appearing as $-\log(\sigma^2)$ in the exponent),
because these can appear in both the normal distribution $p(\bbeta\mid\sigma^2)$ and in the inverse gamma $p(\sigma^2)$ distribution.
Furthermore, it is necessary to complete the square for the normal part, resulting in an additional term for the inverse gamma part.

The density of a normal distribution on $\bbeta\mid\sigma^2$ with a mean vector%
\footnote{We will denote the parameters of the canonically constructed prior (CCCP)
by an overlined version of the parameters of the standard conjugate prior (SCP)
in order to emphasise the different meanings.}
$\tmz = \tmz(\nz,\byz)$ and a variance-covariance matrix $\sigma^2 \tMz = \sigma^2 \tMz(\nz,\byz)$,
both to be seen as functions of the canonical parameters $\nz$ and $\byz$,
has the following form:
\begin{align*}
p(\bbeta\mid\sigma^2) &= \frac{1}{(2\pi)^{\frac{p}{2}} (\sigma^2)^{\frac{p}{2}}}
                         \exp \Big\{ -\frac{1}{2 \sigma^2} \big(\bbeta - \tmz\big)^\tra {\tMz}^{-1}
                                                           \big(\bbeta - \tmz\big) \Big\}\\
                      &= \frac{1}{(2\pi)^{\frac{p}{2}}}
                         \exp\Big\{ {\tmz}^\tra {\tMz}^{-1} \,\frac{\bbeta}{\sigma^2}
                                   -\frac{1}{2\sigma^2} \bbeta^\tra {\tMz}^{-1}\, \bbeta \\ & \hspace*{15ex}
                                   -\frac{1}{2\sigma^2} {\tmz}^\tra {\tMz}^{-1} \tmz
                                   -\frac{p}{2} \log(\sigma^2)\Big\}\,.
\end{align*}
Comparing coefficients with the terms from \eqref{equ:cccp-ptheta} depending on $\bbeta$, we get
\begin{align*}
{\tMz}^{-1} &= %\ol{\bs{M}}(n\uz)^{-1} =
               \frac{\nz}{n}\, \XtX\,, &
       \tmz &= %\ol{m}(y\uz) =
               n\, (\XtX)^{-1}\, \byz_1\,,
\end{align*}
where the latter derives from
\begin{alignat*}{2}
& &
\nz {\byz_1}^\tra \frac{\bbeta}{\sigma^2} &\stackrel{!}{=} {\tmz}^\tra\, \frac{\nz}{n\sigma^2}\, \XtX \bbeta \\
&\Longleftrightarrow\quad &
\nz {\byz_1}^\tra                         &\stackrel{!}{=} {\tmz}^\tra\, \frac{\nz}{n}\, \XtX \\
&\Longleftrightarrow\quad &
{\byz_1}^\tra (\XtX)^{-1}\, n             &\stackrel{!}{=} {\tmz}^\tra\,.
\end{alignat*}
We must thus complete the square in the exponent with
%\begin{align*}
\begin{multline*}
    -\frac{1}{2\sigma^2} {\tmz}^\tra {\tMz}^{-1} \tmz
    +\frac{1}{2\sigma^2} {\tmz}^\tra {\tMz}^{-1} \tmz \\
  = -\frac{1}{2\sigma^2}\left( n \cdot \nz {\byz_1}^\tra (\XtX)^{-1} \byz_1 \right)
    -\frac{1}{\sigma^2} \left(-\frac{\nz n}{2} {\byz_1}^\tra (\XtX)^{-1} \byz_1 \right)\,,
\end{multline*}
%\end{align*}
such that the joint density of $\bbeta$ and $\sigma^2$ reads as
\begin{align}
%\lefteqn{
p(\bbeta,\sigma^2) &= \mbf{c}(\nz, \byz) \nonumber\\ & \hspace*{3ex} 
                      \exp\Big\{\underbrace{\!\! \nz {\byz_1}^{\!\tra}\! \frac{\bbeta}{\sigma^2}
                                            -\frac{\nz}{2n\sigma^2} \bbeta^\tra\! \XtX \bbeta
                                            -\frac{1}{2\sigma^2} \Big(\! n\cdot \nz {\byz_1}^{\!\tra }\!\! (\XtX)^{-1} \byz_1\! \Big)
                                            -\frac{p}{2} \log(\sigma^2)}_{\text{to }p(\bbeta\mid\sigma^2) \text{ (normal distribution)}}
                                                  \nonumber\\ & \hspace*{9ex}
                                \underbrace{-\frac{1}{\sigma^2} \Big(\!\!-\frac{\nz n}{2} {\byz_1}^{\!\tra}\!\! (\XtX)^{-1} \byz_1\! \Big)\!
                                            -\nz  \yz_2 \frac{1}{\sigma^2}
                                            -\Big(\!\frac{\nz + p}{2} + 2\!\Big) \log(\sigma^2)\!\!}_{\text{to } p(\sigma^2) \text{ (inverse gamma distribution)}} \Big\} \,.
\label{equ:priori_betasigma}
\end{align}
Therefore, one part of the conjugate prior \eqref{equ:priori_betasigma} reveals as a
multivariate normal distribution with mean vector
$\tmz = n\,(\XtX)^{-1}\, \byz_1$ and
variance-covariance matrix $\sigma^2\tMz = \frac{n\sigma^2}{\nz}\, (\XtX)^{-1}$, i.e.\
\begin{align}\label{equ:priori_betaggsigma}
\bbeta\mid\sigma^2 &\sim \norm_p \left( n\,(\XtX)^{-1}\, \byz_1\,,\ \frac{n\sigma^2}{\nz}\, (\XtX)^{-1} \right)\,.
\end{align}
The other terms in \eqref{equ:priori_betasigma} can be directly identified with the core of
an inverse gamma distribution with parameters
\begin{align*}
\taz &= \frac{\nz + p}{2} + 1 \qquad \text{and}\\ %\label{equ:a0} \\
\tbz &= \nz \yz_2 -\frac{\nz}{2} {\byz_1}^\tra n(\XtX)^{-1} \byz_1
      = \nz \yz_2 -\frac{1}{2} {\tmz}^\tra {\tMz}^{-1} \tmz\,,%\label{equ:b0}
\end{align*}
\begin{align}\label{equ:priori_sigma}
\text{i.e., }\quad
\sigma^2 &\sim \ig \bigg( \frac{\nz + p + 2}{2}\,,\ \nz \yz_2 - \frac{\nz}{2} {\byz_1}^\tra n(\XtX)^{-1} \byz_1  \bigg)\,.
\end{align}
We have thus derived the CCCP distribution on $(\bbeta,\sigma^2)$,
which can be expressed either in terms of the canonical prior parameters $\nz$ and $\byz$,
or in terms of the prior parameters from Section~\ref{sec:scp}, $\tmz$, $\tMz$, $\taz$ and $\tbz$.

As already noted, $\tMz = \frac{n}{\nz}(\XtX)^{-1}$ can be seen as a restricted version of $\Mz$.
$(\XtX)^{-1}$ is known as the variance-covariance structure from the least squares estimate $\V(\bbeta) = \sls(\XtX)^{-1}$,
and is here the fixed prior variance-covariance structure for $\bbeta\mid\sigma^2$.
Confidence in the prior assignment is expressed by the choice of $\nz$:
With $\nz$ chosen large relative to $n$, strong confidence in the prior assignment
of $\tmz$ can be expressed, whereas a low value of $\nz$ will result in a less pointed
prior distribution on $\bbeta\mid\sigma^2$.

$\tmz$ can be chosen freely from $\reals^p$, just like $\mz$ for the SCP,
because $\byz_1 \in \reals^p$;
values for $\taz$ are instead restricted by $\taz > p/2 + 1$, as $\nz$ must be postive.

The condition $\yz_2 > 0$ does not actually restrict the choice of $\tbz$,
as the second term $-\frac{1}{2} {\tmz}^\tra {\tMz}^{-1} \tmz$,
containing a quadratic form with a postive definite matrix,
is always negative, such that the first term $\nz\yz_2$ must be positive anyway
in order to allow a positive value for $\tbz$ (which is needed to make the prior proper).

As seen in Section~\ref{sec:regularconjugates},
the update step for a canonically constructed prior, expressed in terms of $\nz$ and $\byz$,
possesses a convenient form: In the prior \eqref{eq:canonicalconjugate-festschrift},
the parameters $\nz$ and $\byz$ must simply be replaced
by their updated versions $\nn$ and $\byn$, which calculate as
\begin{align*}
\yn_j &= \frac{\nz \yz_j + \tau(\z)_j}{\nz + n}\,, \quad \forall j \in \{1, \ldots, p+1\}\,, \\
\nn   &= \nz + n \,.
\end{align*}
In the following, we will describe what this means for the update steps of
$\bbeta\mid\sigma^2$, $\sigma^2$, and $\beta$, and compare these results
with those for the SCP. %in Section~\ref{sec:discussion-festschrift}.


\subsubsection{Update of \texorpdfstring{$\bbeta\mid\sigma^2$}{beta|sigma2}}
\label{sec:cccp-update1}

As $\byz$ and $\byn$ are not directly interpretable, it is certainly easier 
to express prior beliefs on $\bbeta$ via the mean vector $\tmz$ of the prior distribution of
$\beta\mid\sigma^2$ just as in the SCP model.
As the transformation $\tmz \mapsto \byz$ is linear, this poses no problem:
\begin{align}
\E[\bbeta\mid\sigma^2,\z]
 &= \tmn %\nonumber\\
  = n (\XtX)^{-1}\, \byn_1 \nonumber\\
 &= n (\XtX)^{-1} \bigg(\frac{\nz}{\nz + n}\,\byz_1 + \frac{n}{\nz + n}\cdot \frac{1}{n}(\Xtz) \bigg) \nonumber\\
 &= n (\XtX)^{-1} \frac{\nz}{\nz + n} \cdot \frac{1}{n}(\XtX) \tmz
  + n (\XtX)^{-1} \frac{n}  {\nz + n} \cdot \frac{1}{n}(\Xtz) \nonumber\\
% &= \frac{n\uz}{n\uz + n} \cdot \underbrace{\tmz}_{\ewt[\beta\mid\sigma^2]}
%  + \frac{n}   {n\uz + n} \cdot \underbrace{(\XtX)^{-1}\Xtz}_{\bls}\,.
 &= \frac{\nz}{\nz + n} \E[\bbeta\mid\sigma^2]
  + \frac{n}  {\nz + n} \bls\,.
  \label{equ:update_beta}
\end{align}
The posterior expectation for $\bbeta\mid\sigma^2$ is here a
scalar-weighted mean of the prior expectation and the least squares
estimate, with weights $\nz$ and $n$, respectively. The role of
$\nz$ in the prior variance of $\bbeta\mid\sigma^2$ is directly
mirrored here. As described for the generalised setting in
Section~\ref{sec:regularconjugates} (see also Section~\ref{070517-sec2-1}), $\nz$ can
be seen as a parameter describing the ``prior strength'' or
expressing ``pseudocounts''. In line with this interpretation, high
values of $\nz$ as compared to $n$ result here in a strong
influence of $\tmz$ for the calculation of $\tmn$, whereas for small
values of $\nz$, $\E[\bbeta\mid\sigma^2,\z]$ %the posterior expectation of $\beta\mid\sigma^2$
will be dominated by the value of $\bls$.

The variance of $\bbeta\mid\sigma^2$ is updated as follows:
\begin{align}
\V(\bbeta\mid\sigma^2,\z) &= \frac{n\sigma^2}{\nn}(\XtX)^{-1} %\nonumber\\
                           = \frac{n\sigma^2}{\nz + n}(\XtX)^{-1}\,. \nonumber%\vspace*{-5ex}
\end{align}
Here, $\nz$ is updated to $\nn$, and thus the posterior variances are
automatically smaller than the prior variances, just as in the SCP model.
%
%However, modelling of the covariance structure seems a bit simplistic, as only $n\uz$ is
%updated to $n\uo$, while no other prior specifications can be made. Due to the fixed
%choice of $n\uz$ in the \ymodel\ calculus, (co-) variances are not interval-valued
%but real numbers. By the use of the \nymodel\ calculus, the latter point could be relaxed,
%while the fixed covariance structure will remain.
%

\subsubsection{Update of \texorpdfstring{$\sigma^2$}{sigma2}}
\label{sec:cccp-update2}

For the assignment of the parameters $\taz$ and $\tbz$ to define the inverse gamma part
of the joint prior, only $\yz_2$ is left to choose,
as $\nz$ and $\byz_1$ are already assigned via the choice of $\tmz$ and $\tMz$.
To choose $\yz_2$, it is convenient to consider the prior expectation of $\sigma^2$
(alternatively, the prior mode of $\sigma^2$ could be considered as well):

%The update step on the inverse gamma part of the joint prior (i.e., the distribution of $\sigma^2$)
%is done by updating the parameters $a\uz$ and $b\uz$ from (\ref{equ:priori_sigma}) to $a\uo$ and $b\uo$,
%which are derived by replacing $n\uz$ and $y\uz$ by $n\uo$ and $y\uo$ in the formulae
%(\ref{equ:a0}) and (\ref{equ:b0}).
\begin{align*}
\E[\sigma^2] &= \frac{\tbz}{\taz - 1}
              = \frac{\nz \yz_2 -\frac{1}{2} {\tmz}^\tra {\tMz}^{-1}\tmz}{\frac{\nz + p}{2}+1-1} \\
             &= \frac{2}{\nz + p}\Big(\nz \yz_2 -\frac{\nz}{2} {\byz_1}^\tra n(\XtX)^{-1} \byz_1\Big) \\
             &= \frac{2 \nz}{\nz + p} \yz_2 - \frac{1}{\nz + p} {\tmz}^\tra {\tMz}^{-1}\tmz\,.
\end{align*}
A value of $\yz_2$ dependent on the value of $\E[\sigma^2]$ can thus be chosen by the linear mapping

%Eliciting the remaining prior parameter $y\uz_2$ directly via $b\uz$ ($a\uz$ contains
%only $n\uz$ which should be elicited at equation~\ref{equ:update_beta}) might be difficult and
%it is maybe more easy to consider the prior expected value of $\sigma^2$:
%It follows that a value given for $\mathds{E}[\sigma^2]$ leads to a value of $y\uz_2$
%through the linear mapping
\begin{align*}
\yz_2 &= \frac{\nz + p}{2 \nz} \E[\sigma^2] + \frac{1}{2\nz} {\tmz}^\tra {\tMz}^{-1}\tmz\,.
\end{align*}
%bounds can be given for $\mathds{E}[\sigma^2]$ which are leading to bounds for $y\uz_2$, provided that
%the value of $n\uz$ was already chosen when eliciting the prior beliefs on $\mathds{V}(\beta\mid\sigma^2)$.
%
%The prior variance of $\sigma^2$ then calculates as
%\begin{align*}
%\var(\sigma^2) &= \frac{{\tbz}^2}{(\taz-1)^2(\taz-2)}
%                = \frac{1}{\frac{n\uz + p}{2} +1 - 2}\big( \ewt[\sigma^2] \big)^2 %\\
%                = \frac{2}{n\uz + p -2} \big(\ewt[\sigma^2]\big)^2 \,,
%\end{align*}
%and might be of interest for assessing one's choice regarding $n\uz$ and $y\uz_2$.\\
%
For the posterior expected value of $\sigma^2$, there is a similar decomposition
as for the SCP model, and furthermore two other possible decompositions offering
interesting interpretations of the update step of $\sigma^2$.
The three decompositions are presented in the following.


\paragraph{Decomposition Including an Estimate of \texorpdfstring{$\sigma^2$}{sigma2} Through the Null Model.}

In a first decomposition, the posterior variance of $\sigma^2$ can be written as:
\begin{align}
\E[\sigma^2\mid \z] &= \frac{\tbn}{\tan - 1} %\nonumber\\
                     = \frac{2 \nn}{\nn + p} \yn_2 - \frac{1}{\nn + p} {\tmn}^\tra {\tMn}^{-1} \tmn \nonumber\\
                    &= \frac{2 \nz}{\nz + n + p} \yz_2
                     + \frac{1}{\nz + n + p} \ztz
                     - \frac{1}{\nz + n + p} {\tmn}^\tra {\tMn}^{-1} \tmn \nonumber\\
                    &= \frac{\nz + p}{\nz + n + p} \E[\sigma^2]
                         + \frac{n-1}{\nz + n + p} \frac{1}{n-1} \ztz \nonumber\\ & \hspace*{20.6ex}
                         +   \frac{1}{\nz + n + p} \Big( {\tmz}^\tra {\tMz}^{-1} \tmz
                                                       - {\tmn}^\tra {\tMn}^{-1} \tmn \Big)\,,
\label{equ:cccp-sigma1}
%
%                                = \frac{2n\uz y\uz_2 + \ztz}{n\uz + n + p}
%                                = \frac{2 n\uz}{n\uz + n + p} y\uz_2 + \frac{1}{n\uz +n + p} \ztz\\
%                               &= \frac{n\uz + p}{n\uz + n + p} \mathds{E}[\sigma^2] + \frac{n}{n\uz +n + p} \frac{1}{n}\ztz\,,
\end{align}
and so can be seen as a weighted average of the prior expected value,
$\frac{1}{n-1}\ztz$, and a term depending on %standardized %inversely weighted
prior and posterior estimates for $\bbeta$, with weights $\nz + p$, $n-1$ and $1$, respectively.
When adopting the centered $\z$, standardized $\X$ approach,
$\frac{1}{n-1}\ztz$ is the estimate for $\sigma^2$ under the null model, that is, if $\bbeta = \vec{0}$.

Contrary to what a cursory inspection might suggest,
the third term's influence, having the constant weight of $1$,
will not vanish for $n \to \infty$, %growing sample sizes,
as the third term does not approach a constant.%
\footnote{Although $\tmn$ approaches $\bls$, and $\tmz$ is a constant,
${\tMz}^{-1}$ and ${\tMn}^{-1}$ are increasing for growing $n$,
with ${\tMn}^{-1}$ increasing faster than ${\tMz}^{-1}$.
The third term will thus eventually turn negative, reducing the null model variance that has weight $n-1$.}

The third term reflects the change in information about $\bbeta$:
\begin{enumerate}[(i)]
\item If we are very uncertain about the prior beliefs on $\bbeta$ as expressed in $\tmz$
and thus assign a small value for $\nz$ as compared to $n$,
we will get relatively large variances and covariances in $\tMz$ by the factor $n/\nz > 1$ %\raisebox{0ex}[0ex][0ex]{$\frac{n}{n\uz} > 1$}
to $(\XtX)^{-1}$, resulting in a small term ${\tmz}^\tra {\tMz}^{-1} \tmz$. After updating, the %variances and covariances
elements in $\tMn$ become smaller automatically
due to the updated factor $n/(\nz+n)$ %\raisebox{0ex}[0ex][0ex]{$\frac{n}{n\uz + n}$}
to $(\XtX)^{-1}$.

If the values of $\tmn$ do not differ much from the values in $\tmz$,
then the term ${\tmn}^\tra {\tMn}^{-1} \tmn$ would be larger than
its prior counterpart ${\tmz}^\tra {\tMz}^{-1} \tmz$, ultimately reducing the posterior expectation for $\sigma^2$
through the third term being negative.

If $\tmn$ does significantly differ from $\tmz$,
then the term ${\tmn}^\tra {\tMn}^{-1} \tmn$ can actually be smaller than
${\tmz}^\tra {\tMz}^{-1} \tmz$, and thus give a larger value of $\E[\sigma^2\mid \z]$ 
as compared with the situation $\tmn \approx \tmz$.

\item On the contrary, large values for $\nz$ as compared to $n$, indicating
high trust in prior beliefs on $\bbeta$, %(expressed in $\tmz$)
lead to small variances and covariances in $\tMz$ by the factor $n/\nz$ %\raisebox{0ex}[0ex][0ex]{$\frac{n}{\nz} < 1$}
to $(\XtX)^{-1}$,
resulting in a larger term ${\tmz}^\tra {\tMz}^{-1} \tmz$ as compared to the case with low $\nz$.
After updating, variances and covariances in $\tMn$ will become even smaller,
amplifying the term ${\tmn}^\tra {\tMn}^{-1} \tmn$ even more if
%the values of $\tmo$ do not differ much from the values of $\tmz$,
$\tmn \approx \tmz$,
ultimately reducing the posterior expectation for $\sigma^2$ more than in the situation with low $\nz$.

If, however, the values of $\tmn$ do differ significantly from the values in $\tmz$,
the term ${\tmn}^\tra {\tMn}^{-1} \tmn$ can be smaller than
${\tmz}^\tra {\tMz}^{-1} \tmz$ also here, and even more so as compared to the situation with low $\nz$,
giving eventually an even larger posterior expectation for $\sigma^2$.
\end{enumerate}


\paragraph{Decomposition Similar to the SCP Model.}

%Interestingly, this is not the only useful interpretation of $\mathds{E}[\sigma^2\mid\bs{z}]$.
%The formula (\ref{equ:posteriori_sigma}) can be shown to convert in other two different forms
%that offer an intuitive interpretation for the update step of $\sigma^2$,
%where one gives an interpretation similar to the one for the SCP model in gleichung.\\

A decomposition similar to the one in Section~\ref{sec:scp-update2} can be derived
by considering the third term from \eqref{equ:cccp-sigma1} in more detail:

\begin{align*}
\lefteqn{
    {\tmz}^\tra {\tMz}^{-1} \tmz
  - {\tmn}^\tra {\tMn}^{-1} \tmn } \hspace*{3ex} \\
 &= \nz \cdot n \cdot {\byz_1}^\tra (\XtX)^{-1} \byz_1
  - \nn \cdot n \cdot {\byn_1}^\tra (\XtX)^{-1} \byn_1 \\
 &= \nz \cdot n \cdot {\byz_1}^\tra (\XtX)^{-1} \byz_1
  - (\nz + n) \cdot n \frac{\nz {\byz_1}^\tra + \ztX}{\nz + n} (\XtX)^{-1} \frac{\nz \byz_1 + \Xtz}{\nz + n} \\
 &= \left(\nz \cdot n - \frac{n \cdot {\nz}^2}{\nz + n}\right) {\byz_1}^\tra (\XtX)^{-1} \byz_1
  - \frac{2\nz \cdot n}{\nz + n}                               {\byz_1}^\tra (\XtX)^{-1} \Xtz \\ & \hspace*{42ex}
  - \frac{n}{\nz + n}                                                   \ztX (\XtX)^{-1} \Xtz \\
 &= \frac{n}{\nz + n} \Big[               {\tmz}^\tra {\tMz}^{-1} \tmz
                           - 2            {\tmz}^\tra {\tMz}^{-1} \bls
                           -\frac{n}{\nz} {\bls}^\tra {\tMz}^{-1} \bls \Big] \\
 &= \frac{n}{\nz + n} \Big[ \big(\tmz - \bls\big)^\tra {\tMz}^{-1} \big(\tmz - \bls\big)
                           -\big(\frac{n}{\nz}+1\big)  {\bls}^\tra {\tMz}^{-1} \bls \Big] \\
 &= \frac{n}{\nz + n}       \big(\tmz - \bls\big)^\tra {\tMz}^{-1} \big(\tmz - \bls\big)
                           - \ztX (\XtX)^{-1} \Xtz\,.
\end{align*}
Thus, we get
\begin{align}
\E[\sigma^2\mid\z] &= \frac{\nz + p}{\nz + n + p} \E[\sigma^2]
                     +\frac{1}      {\nz + n + p} \Big( \ztz - \ztX (\XtX)^{-1} \Xtz \Big) \nonumber\\ & \hspace*{5ex}
                     +\frac{1}      {\nz + n + p} \cdot \frac{n}{\nz + n} (\tmz - \bls)^\tra {\tMz}^{-1} (\tmz - \bls) \nonumber\\
                   &= \frac{\nz + p}{\nz + n + p} \E[\sigma^2]
                     +\frac{n - p}  {\nz + n + p} \cdot \underbrace{\frac{1}{n-p} %
                                                          (z - \X\bls)^\tra (z - \X\bls)}_{\sls} \nonumber\\ & \hspace*{5ex}
                     +\frac{p}      {\nz + n + p} \cdot \underbrace{\frac{n}{\nz + n} %
                                                          \frac{1}{p} (\tmz - \bls)^\tra {\tMz}^{-1} (\tmz - \bls)}_{=:\tspdc}\,.
\label{equ:cccp-sigma2}
\end{align}
The posterior expectation for $\sigma^2$ can therefore be seen also here as
a weighted average of the prior expected value,
the estimation $\sls$ resulting from least squares methods,
and $\tspdc$, an estimate for $\sigma^2$ similar to $\spdc$,%
\footnote{$\E[\tspdc\mid\sigma^2]=\sigma^2$ computes very similar to the
calculations given in \textcite[p.~249]{1994:ohagan} and is given below.}
with weights $\nz + p$, $n-p$ and $p$, respectively.

As in the update step for $\beta\mid\sigma^2$, $\nz$ is guarding the
influence of the prior expectation on the posterior expectation.
Just as in the decomposition for the SCP model,
the weight for $\sls$ will dominate the others when the sample size approaches infinity.
%
%Just as in the decomposition in Section\ref{sec:scp-update2},
Also for the CCCP model, $\tspdc$ %the estimate based on weighted squares of $\tmz - \bls$
is getting large if prior beliefs on $\beta$ are skewed with respect to ``what the data says'',
eventually inflating the posterior expectation of $\sigma^2$.
The weighting of the differences is similar as well: High prior confidence
in the chosen value of $\tmz$ as expressed by a high value of $\nz$ will
give a large ${\tMz}^{-1}$, and thus penalising erroneous assignments stronger
as compared to a lower value of $\nz$.
Again, $\XtX$, the matrix structure in ${\tMz}^{-1}$, weighs the differences
for components with covariates having a low spread weaker
due to the instability of the respective component of $\bls$ under such conditions.

Now we give the proof that $\E[\tspdc\mid\sigma^2] = \sigma^2$.
As a preparation, it holds that
\begin{align*}
\E[\bls\mid\sigma^2] &= \E\big[ \E[\bls\mid\bbeta, \sigma^2] \big| \sigma^2 \big]
                      = \E[\beta\mid\sigma^2] = \tmz\,, \quad\text{and} \\
\V(\bls\mid\sigma^2) &= \E\big[ \V(\bls\mid\bbeta, \sigma^2) \big| \sigma^2 \big]
                       +\V\big( \E[\bls\mid\bbeta, \sigma^2] \big| \sigma^2 \big) \\
                     &= \E[\sigma^2 (\XtX)^{-1} \mid\sigma^2]
                       +\V(\bbeta\mid\sigma^2) \\
                     &= \sigma^2 (\XtX)^{-1} + \frac{n\sigma^2}{\nz} (\XtX)^{-1}
                      = \frac{\nz + n}{\nz} \sigma^2 (\XtX)^{-1} \,.
\end{align*}
With this in mind, we can now derive
\begin{align*}
\lefteqn{
\E\big[ \big(\tmz - \bls\big)^\tra {\tMz}^{-1} \big(\tmz - \bls\big) \big| \sigma^2 \big] } \hspace*{3ex} \\
    &= \E\Big[ \tr\Big({\tMz}^{-1} \big(\tmz - \bls\big) \big(\tmz - \bls\big)^\tra \Big) \big| \sigma^2 \Big] \\
    &= \tr\Big( {\tMz}^{-1} \E\big[ \big(\tmz - \bls\big) \big(\tmz - \bls\big)^\tra \big| \sigma^2 \big] \Big) \\
    &= \tr\Big( \frac{\nz}{n}(\XtX) \cdot \frac{\nz + n}{\nz} \sigma^2 (\XtX)^{-1} \Big) \\
    &= \tr\Big( \frac{\nz + n}{n} \sigma^2 \mbf{I}_p \Big) = \frac{\nz + n}{n} \cdot p \cdot \sigma^2 \,,
\end{align*}
such that the factor $\frac{n}{\nz + n} \frac{1}{p}$ in $\tspdc$ cancels out, and indeed $\E[\tspdc\mid\sigma^2] = \sigma^2$.

\paragraph{Decomposition with Estimates of \texorpdfstring{$\sigma^2$}{sigma2} Through Prior and Posterior Residuals.}

A third interpretation of $\E[\sigma^2\mid \z]$
can be derived by another reformulation of the third term in \eqref{equ:cccp-sigma1}:
\begin{align*}
\lefteqn{
    {\tmz}^\tra {\tMz}^{-1} \tmz
  - {\tmn}^\tra {\tMn}^{-1} \tmn }\hspace*{3ex} \\
&=  \frac{\nz}{n} {\tmz}^\tra \XtX \tmz
  - \frac{\nn}{n} {\tmn}^\tra \XtX \tmn \\
&=  \frac{\nz}{n} (\z - \X\tmz)^\tra (\z - \X\tmz)
  - \frac{\nn}{n} (\z - \X\tmn)^\tra (\z - \X\tmn) \\ & \hspace*{3ex}
  + \frac{\nn}{n} \ztz      - \frac{\nz}{n} \ztz
  + \frac{\nz}{n} 2\ztX\tmz - \frac{\nn}{n} 2\ztX\tmn \\
&=  \frac{\nz}{n} (z - \X\tmz)^\tra (z - \X\tmz)
  - \frac{\nn}{n} (z - \X\tmn)^\tra (z - \X\tmn) %\\ & \hspace*{3ex}
  + \ztz
  - 2\ztX\bls \,. % (\XtX)^{-1} \Xtz \,.
\end{align*}
With this, we get
\begin{align}
\E[\sigma^2\mid \z] &= \frac{\nz + p} {\nz + n + p} \E[\sigma^2]
                      +\frac{\nz + p} {\nz + n + p}
                        \underbrace{\frac{\nz}{n} \cdot \frac{1}{\nz + p}
                                    (\z - \X\tmz)^\tra (\z - \X\tmz)}_{=: {\sigma\uz}^2, \text{ as }
                                                                       \E[{\sigma\uz}^2\mid\sigma^2] = \sigma^2} \nonumber\\ & \hspace*{2ex}
                      +\frac{2(n - p)}{\nz + n + p} \sls
                      -\frac{\nn + p} {\nz + n + p}
                        \underbrace{\frac{\nn}{n} \cdot \frac{1}{\nn + p}
                                    (\z - \X\tmn)^\tra (\z - \X\tmn)}_{=: {\sigma\un}^2, \text{ as }
                                                                       \E[{\sigma\un}^2\mid\sigma^2,\z] = \E[{\sigma\un}^2\mid\sigma^2] = \sigma^2} %\nonumber\\ & \hspace*{19ex}
                                                                         .
\label{equ:cccp-sigma3}%\vspace*{-1.5ex}
\end{align}
Here, the calculation of $\E[\sigma^2\mid \z]$ is based
again on $\E[\sigma^2]$ and $\sls$, but now complemented with
two special estimates:
${\sigma\uz}^2$, an estimate based on the ``prior residuals'' $z - \X\tmz$,
and a respective posterior version ${\sigma\un}^2$, based on $z - \X\tmn$.

However, $\E[\sigma^2\mid \z]$ is only ``almost'' a weighted average of these ingredients,
as the weights sum up to $\nz - p + n$ instead of $\nz + p + n$.
Especially strange is the negative weight for ${\sigma\un}^2$,
actually making the factor to ${\sigma\un}^2$ in \eqref{equ:cccp-sigma3} result to $-1$.

A possible interpretation would be to group $\E[\sigma^2]$ and ${\sigma\uz}^2$
as prior-based estimations with joint weight $2(\nz + p)$,
and $\sls$ as data-based estimation with weight $2(n-p)$.
Together, these estimations have a weight of $2(\nz + n)$,
being almost (neglecting the missing $2p$) a ``double estimate''
that is corrected back to a ``single'' estimate
with the posterior-based estimate ${\sigma\un}^2$.


\subsubsection{Update of \texorpdfstring{$\bbeta$}{beta}}
\label{sec:cccp-update3}

As for the SCP model, the posterior on $\bbeta$,
being the most relevant distribution for inferences, is a multivariate $t$
with expectation $\tmn$ as described in Section~\ref{sec:cccp-update1}.
For $\V(\beta\mid \z)$, one gets different formulations, depending
on the formula for $\E[\sigma^2\mid \z]$:
\begin{align}
\V(\bbeta\mid \z)
                   &\stackrel{\phantom{\eqref{equ:cccp-sigma1}}}{=}
  \frac{\tbn}{\tan - 1} \tMn %
     = \E[\sigma^2\mid \z]\, \frac{n}{\nn}(\XtX)^{-1} \label{equ:cccp-varbeta} \\
\nonumber\\
% ------------- 4.2.1 -------------
                   &\stackrel{\eqref{equ:cccp-sigma1}}{=}
  \frac{\nz + p}{\nz + n + p} \frac{\nz}{\nn} \underbrace{\E[\sigma^2] \frac{n}{\nz} (\XtX)^{-1}}_{\V(\bbeta)}
    + \frac{n-1}{\nz + n + p}   \frac{n}{\nn} \frac{1}{n-1} \ztz (\XtX)^{-1} \nonumber\\ & \hspace*{10ex}
      + \frac{1}{\nz + n + p}   \frac{n}{\nn} \Big( {\tmz}^\tra {\tMz}^{-1} \tmz
                                                      - {\tmn}^\tra {\tMn}^{-1} \tmn \Big) (\XtX)^{-1} \nonumber\\
\nonumber\\
% ------------- 4.2.2 -------------
                   &\stackrel{\eqref{equ:cccp-sigma2}}{=}
  \frac{\nz + p}{\nz + n + p} \frac{\nz}{\nn} \underbrace{\E[\sigma^2] \frac{n}{\nz} (\XtX)^{-1}}_{\V(\bbeta)}
 +\frac{n - p}  {\nz + n + p}   \frac{n}{\nn} \underbrace{\sls (\XtX)^{-1}}_{\V(\bls)} \nonumber\\ & \hspace*{10ex}
 +\frac{p}      {\nz + n + p}   \frac{n}{\nn} \tspdc (\XtX)^{-1} \nonumber\\
\nonumber\\
% ------------- 4.2.3 -------------
                   &\stackrel{\eqref{equ:cccp-sigma3}}{=}
  \frac{\nz + p} {\nz + n + p} \frac{\nz}{\nn} \underbrace{\E[\sigma^2] \frac{n}{\nz} (\XtX)^{-1}}_{\V(\bbeta)}
 +\frac{\nz + p} {\nz + n + p} \frac{\nz}{\nn} \underbrace{{\sigma\uz}^2 \frac{n}{\nz} (\XtX)^{-1}}_{=:\V\uz(\bbeta)} \nonumber\\ & \hspace*{10ex}
 +\frac{2(n - p)}{\nz + n + p}   \frac{n}{\nn} \underbrace{\sls (\XtX)^{-1}}_{\V(\bls)}
 -\frac{\nn + p} {\nz + n + p}                 \underbrace{{\sigma\un}^2 \frac{n}{\nn} (\XtX)^{-1}}_{=:\V\un(\bbeta)} \,. \nonumber
\end{align}
In these equations, it is possible to isolate $\V(\bbeta)$, $\V(\bls)$ and,
in the formulation with $\eqref{equ:cccp-sigma3}$, the newly defined
$\V\uz(\bbeta)$ and $\V\un(\bbeta)$.
However, all three versions do not constitute a weighted average,
even when the corresponding formula for $\E[\sigma^2\mid \z]$ has this property.

Just as in the SCP model, $\V(\beta\mid \z)$ can increase if the automatic decrease of the elements
in $\tMn$ is overcompensated by a strong increase of $\E[\sigma^2\mid\z]$.
Again, this reaction to prior-data conflict is unspecific
because it depends on $\E[\sigma^2\mid \z]$ alone,
and affects all elements of variance-covariance matrix in the same way.


\subsection{Discussion and Outlook}
\label{sec:discussion-festschrift}


For both the SCP and CCCP model, $\E[\bbeta\mid \z]$ can be seen as a weighted average
of $\E[\bbeta]$ and $\bls$, such that the posterior distribution on $\bbeta$ will be
centered around a mean somewhere between $\E[\bbeta]$ and $\bls$, with the location
depending on the respective weights. The weights for the CCCP model appear especially intuitive:
$\bls$ is weighted with the sample size $n$, whereas $\E[\bbeta]$ has the weight $\nz$,
reflecting the ``prior strength'' or ``pseudocounts''.

Due to this, prior-data conflict may at most affect the variances only. Indeed, for both prior models,
$\E[\sigma^2\mid \z]$ can increase in the presence of prior-data conflict, as shown by the decompositions
in Sections~\ref{sec:scp-update2} and \ref{sec:cccp-update2}.

Through the formulations \eqref{equ:scp-varbeta} and \eqref{equ:cccp-varbeta} for $\V(\bbeta\mid \z)$,
respectively, it can be seen that the posterior distribution on $\bbeta$ can in fact
become less pointed than the prior when prior-data conflict is present.
Nevertheless, the effect might not be as strong as desired: In the formulations
\eqref{equ:scp-decomposition} and \eqref{equ:cccp-sigma2}, respectively, the effect is based only on one term of the decomposition,
and furthermore may be foiled through the automatic decrease of $\Mn$ and $\tMn$.
Probably the most problematic finding is that this (possibly weak) reaction affects the whole
variance-covariance matrix uniformally, and thus, in both models, the reaction
to prior-data conflict is by no means component-specific.

Therefore, the prior models lack the capability to %have not any capability to
mirror the appropriateness of the prior assignments for each covariate individually.
As the SCP model is already the most general approach in the class of conjugate priors,
this non-specificity feature seems inevitable in Bayesian linear regression based on precise conjugate priors.


In fact, as argued in Section~\ref{sec:festschrift-intro}, a more sophisticated and specific reaction to prior-data conflict
is only possible by extending considerations beyond the traditional concept of probability.
Imprecise probabilities, as a general methodology to cope with the multidimensional nature of uncertainty, appears promising here.
For generalised Bayesian approaches, the possibility to mirror the quality of prior knowledge
is one of the main reasons for the paradigmatic skip from classical probability to interval or imprecise probability.%
\footnote{See the discussion of this motive in Section~\ref{sec:motivation:pdc}.}
In this framework, ambiguity in the prior specification
can be modeled by considering sets ${\cal M}$ of prior distributions.
In the most common approach based on Walley's \parencite*{1991:walley} Generalised Bayes' Rule (see Section~\ref{sec:gbr}),
posterior inference is then based on a set of posterior distributions ${\cal M}_{\vert \z}$,
resulting from updating the distributions in the prior set element by element.%
\footnote{See Section~\ref{sec:imprecisebayes}.}

Of particular computational convenience are again models based on conjugate priors, as developed
for the Dirichlet-Multinomial model by \textcite{1996:walley::idm}, see also \textcite{2009:bernard},%
\footnote{The IDM is discussed in more detail in Section~\ref{sec:idm-and-near-ignorance}.}
and for i.i.d.\ exponential family sampling models by \textcite{2005:quaeghebeurcooman},
which were extended by \textcite{Walter2009a}%
\footnote{See Section~\ref{sec:jstp} for a reproduction of this work.}
to allow an elegant handling of prior-data conflict:
With the magnitude of the set ${\cal M}_{\vert \z}$ mapping the posterior ambiguity,
high prior-data conflict leads, ceteris paribus, to a large
${\cal M}_{\vert \z},$ resulting in high imprecision in the
posterior probabilities, and cautious inferences based on it, while in the case of no prior-data conflict
${\cal M}_{\vert \z}$, and thus the imprecision, is much smaller.%
\footnote{See the overview on imprecise probability models based on this class of conjugate priors in Section~\ref{sec:generalmodel}.}

The essential technical ingredient to derive this class of models is the general construction principle,
described in Section~\ref{sec:regularconjugates},
underlying the CCCP model from Section~\ref{sec:cccp}, and thus that model can be extended directly to
a powerful corresponding imprecise probability model.\footnote{For $\sigma^2$ fixed, the model from
Section~\ref{sec:scp} can also be comprised under the more general structure described in Section~\ref{070517-sec2-1},
that also can be extended to imprecise probabilities, see \textcite{Walter2007a}, and \textcite{Walter2006a} for details.}
A detailed development is beyond the scope of this contribution. %, but the basic steps may finally be sketched:

\iffalse
*aus sec~\ref{sec:cccp}, noch be-tilde-n und ent-fett-en*\\
To make inference on $\beta$ using the \nymodel\ calculus, thus one can choose (elementwise) lower
and upper bounds for $\bs{m}(y\uz)$ based on one's prior knowledge on $\beta$; $n\uz$ must be chosen fix and
determines the prior covariance matrix for $\beta\mid\sigma^2$, as $\mathds{V}(\beta\mid\sigma^2) = \frac{n\sigma^2}{n\uz}(\XtX)^{-1}$.
The bounds for $\bs{m}(y\uz_1)$ can then be translated into bounds for $y\uz_1$ by the linear
mapping $y\uz = \frac{1}{n}(\XtX) \bs{m}(y\uz)$.
Then, through the linear update step on $n\uz$ and on the bounds for
$y\uz_1$, the updated parameter $n\uo$ and the (updated) bounds for $y\uo_1$ can be obtained.
Finally, these updated bounds can be retranslated into interpretable bounds for $\bs{m}(y\uo_1)$, which is
the mean vector of the posterior distribution for $\beta\mid\sigma^2$. The updated $n\uo$ is used to calculate the
posterior covariance matrix for $\beta\mid\sigma^2$.

In short terms:
\begin{enumerate}
\item fix lower and upper bounds for $\bs{m}(y\uz_1)$ based on prior
knowledge on $\beta$; $n\uz$ must be chosen fix %($\rightarrow$ Friday)
and determines the prior covariance matrix for $\beta\mid\sigma^2$:
$\mathds{V}(\beta\mid\sigma^2) = \frac{n\sigma^2}{n\uz}(\XtX)^{-1}$;
\item `translate' bounds for $\bs{m}(y\uz_1)$ into bounds for $y\uz_1$ by
$y\uz_1 = \frac{1}{n}(\XtX) \bs{m}(y\uz_1)$;
\item perform the linear update step on $n\uz$ and the bounds for
$y\uz_1$ to obtain $n\uo$ and bounds for $y\uo_1$;
\item `retranslate' the bounds for $y\uo_1$ into interpretable
bounds for $\bs{m}(y\uo_1)$ by $\bs{m}(y\uo_1) = n (\XtX)^{-1} y\uo_1$.
\end{enumerate}

As all transformations are linear, the passing from $\bs{m}({\cal Y}\uz_1)$ to $\bs{m}({\cal Y}\uo_1)$) is easy.
Finally, $n\uz$ has to be varied along the lines of Walter \& Augustin (2009, Section~4), to allow proper handling of prior data conflict.
\fi

%\subsubsection*{Acknowledgements}
%The authors are very grateful to Erik Quaeghebeur and Frank Coolen for intensive discussions on foundations of generalised Bayesian
%inference, and to Thomas Kneib for help at several stages of writing this paper.



