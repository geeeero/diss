\chapter{Appendix}
%\label{cha:festschrift}


\section{Bayesian Linear Regression --- Different Conjugate Models and Their (In)Sensitivity to Prior-Data Conflict}
\label{sec:festschrift}

This section reproduces the work
``Bayesian Linear Regression --- Different Conjugate Models and Their (In)Sensitivity to Prior-Data Conflict'',
published as technical report no.~69 at the Department of Statistics of Ludwig-Maximilians-University Munich (LMU)
\parencite{Walter2009b}. This technical report is a substantially extended version of a contribution to
``Statistical Modelling and Regression Structures: Festschrift in Honour of Ludwig Fahrmeir'' \parencite{Walter2010a}.
As such, it is reproduced here verbatim, except for some minor rewording,
and the addition of some comments and footnotes linking this work to other parts of this thesis.
***Furthermore, the notation was changed slightly towards the one introduced in Section~\ref{sec:regularconjugates}
(most importantly, writing $\nn$ and $\yn$ for the canonical posterior parameters
instead of $n^{(1)}$ and $y^{(1)}$, respectively),
and citations were updated and changed to the style employed throughout this thesis.***

\subsubsection*{Abstract}

The paper is concerned with Bayesian analysis under
prior-data conflict, i.e.\ the situation when observed data are
rather unexpected under the prior (and the sample size is not large
enough to eliminate the influence of the prior). Two approaches for
Bayesian linear regression modeling based on conjugate priors are
considered in detail, namely the standard approach, described, e.g., in
\textcite{2013:fahrmeier-kneib-lang-marx}, and an alternative adoption of the
general construction procedure for exponential family sampling
models. We recognize that -- in contrast to some standard i.i.d.\
models like the scaled normal model and the Beta-Binomial /
Dirichlet-Multinomial model, where prior-data conflict is completely
ignored -- the models may show some reaction to prior-data conflict,
however in a rather unspecific way. Finally we briefly sketch the
extension to a corresponding imprecise probability model, where,
%by enlarging the underlying set of distributions and the imprecision of the resulting interval-valued probabilities,
by considering sets of prior distributions instead of a single prior,
prior-data conflict can be handled in a very appealing and intuitive way.


\subsection{Introduction}
\label{sec:festschrift-intro}

Regression analysis is a central tool in applied statistics that
aims to answer the omnipresent question how certain variables
(called covariates / confounders, regressors, stimulus or
independent variables, here denoted by $\x$) influence a certain
outcome (called response or dependent variable, here denoted by $z$).
Due to the complexity of real-life data situations, basic linear
regression models, where the expectation of the outcome $z_i$ simply equals the linear predictor $\x_i^\tra\bbeta$,
have been generalized in numerous ways, ranging from generalised linear
models (\cite{2001:fahrmeier-tutz}, see also \cite{1985:fahrmeir-kaufmann} for classical work on asymptotics)
for non-normal distributions of $z_i\mid \x_i$, or linear mixed models %(LMM, ***zitate)
allowing the inclusion of clustered observations, over semi- and nonparametric models
\parencite{2009:kauermann,2007:fahrmeir,2007:scheipl}, up to
generalised additive (mixed) models and structured
additive regression \parencite{2009:fahrmeir,2006:fahrmeir,2007:kneib}.

Estimation in such highly complex models may be based on
different estimation techniques such as (quasi-) likelihood, general
estimation equations (GEE) or Bayesian methods. Especially the
latter offer in some cases the only way to attain a reasonable
estimate of the model parameters, due to the possibility to include
some sort of prior knowledge about these parameters, for instance by
``borrowing strength'' \parencite[e.g.,][]{1996:higgins}.

The tractability of large scale models with their ever increasing complexity
of the underlying models and data sets should not obscure that
still many methodological issues are a matter of debate.
Since the early days of modern Bayesian inference one central issue has,
of course, been the potentially strong dependence of the inferences on the prior.
In particular in situations where data is scarce or unreliable, the
actual estimate obtained by Bayesian techniques may rely heavily on
the shape of prior knowledge, expressed as prior probability
distributions on the model parameters. %parameters of interest.
Recently, new arguments came into this debate by new methods
for detecting and investigating \emph{prior-data conflict} \parencite{2006:evans,2008:bousquet},
i.e. situations where  ``[\ldots] the observed data is surprising
in the light of the sampling model and the prior,
[so that] we must be at least suspicious about the validity
of inferences drawn.'' \parencite[p.~893]{2006:evans}%
\footnote{See also the discussion on prior-data conflict in
Sections~\ref{sec:motivation:pdc}, \ref{sec:pdc-sensitivity},
and in the paper reproduced in Section~\ref{sec:jstp}.}

The present contribution investigates the sensitivity of inferences on potential prior-data conflict:
What happens in detail to the posterior distribution, and the estimates derived from it, if prior knowledge and
what the data indicates are severely conflicting?
If the sample size $n$ is not sufficiently large to discard the
possibly erroneous prior knowledge and thus to rely on data only,
prior-data conflict should affect the inference and
should  -- intuitively and informally --  result in an increased degree
of uncertainty in posterior inference. Probably most statisticians would thus expect
a higher variance of the posterior distribution in situations of prior-data conflict.

However, this is by no means automatically the case,
in particular when adopting conjugate prior models,
which are often used when data are
scarce, where only strong prior beliefs allow for a reasonably
precise answer in inference. Two simple and prominent examples
of complete insensitivity to prior-data conflict are recalled in
Section~\ref{sec:iid}: i.i.d.\ inferences on the mean of a scaled normal distribution
and on the probability distribution of a categorical variable by the Dirichlet-Multinomial model.%
\footnote{See the description of these two models in Sections~\ref{sec:norm-norm} and \ref{sec:diri-multi}, respectively.}

Sections~\ref{sec:scp} and \ref{sec:cccp} extend
the question of (in)sensitivity to prior-data to regression models.
We confine attention to linear regression analysis with conjugate priors,
because -- contrary to the more advanced regression model classes --
the linear model still allows a fully analytical access, making it possible to understand
potential restrictions imposed by the model in detail.
We discuss and compare two different conjugate models:
\begin{enumerate}[(i)]
\item the standard conjugate prior (SCP, Section~\ref{sec:scp}) as described in \textcite{2013:fahrmeier-kneib-lang-marx} or,
in more detail, in \textcite{1994:ohagan}; and
\item a conjugate prior, called ``canonically constructed conjugate prior'' (CCCP, Section~\ref{sec:cccp}) in the following,
which is derived by a general method used to construct conjugate priors to sample distributions that
belong to a certain class of exponential families, described, e.g., in \textcite{2000:bernardosmith}.%
\footnote{This is the regular conjugate framework of Section~\ref{sec:regularconjugates}.}
\end{enumerate}
Whereas the former is the more general prior model, allowing for
a very flexible modeling of prior information (which might be welcome or not),
the latter allows only a strongly restricted covariance structure for $\bbeta$,
however offering a clearer insight in some aspects of the update process.

In a nutshell, the result is that both conjugate models do react
to prior-data conflict by an enlarged factor to the variance-covariance matrix
of the distribution on the regression coefficients $\bbeta$; however, this reaction is unspecific, as it affects the
variance and covariances of all components of $\bbeta$ in a uniform way
-- even if the conflict occurs only in one single component.

Probably such an unspecific reaction of the variance is the most a (classical)
Bayesian statistician can hope for, and traditional probability theory based on precise probabilities can offer.
Indeed, \textcite{1987:kyburg:ess} notes that
\begin{quotation}
\begin{small}
%``
[\ldots] there appears to be no way, within the theory, of
distinguishing between the cases in which there are good statistical grounds
for accepting a prior distribution, and cases in which the prior distribution
reflects merely ungrounded personal opinion.%''\\
\end{small}
\end{quotation}
and the same applies, in essence, to the posterior distribution.

A more sophisticated modeling would need a more elaborated concept
of imprecision than is actually provided by looking at the variance
(or other characteristics) of a (precise) probability distribution.
Indeed, recently the theory of imprecise probabilities \parencite{1991:walley,2001:weichselberger}
is gaining strong momentum.%
\footnote{See Section~\ref{sec:ip-intro} for a short exposition of the theoretical foundations
and motivations for the use of imprecise probability in statistical inference.}
It emerged as a general methodology to cope
with the multidimensional character of uncertainty, also reacting
to recent insights and developments in decision theory\footnote{See
\textcite{2005:hsu-bhatt} for a neuro science corroboration of the constitutive
difference of stochastic and non-stochastic aspects of uncertainty
in human decision making, in the tradition of Ellsberg's \parencite*{1961:ellsberg} seminal experiments.}
and artificial intelligence,\footnote{See, e.g., \textcite{1996:walley::expert}
for the use of imprecise probability methods in expert systems.}
where the exclusive role of probability
as a methodology for handling uncertainty has eloquently been rejected \cite{1999:klir}:
\begin{quotation}
\begin{small}
%``
For three hundred years [\ldots] uncertainty was conceived solely in
terms of probability theory. This seemingly unique connection
between uncertainty and probability is now challenged [\ldots by several
other] theories, which are demonstrably capable of characterizing
situations under uncertainty. [\ldots]

[\ldots] it has become clear that there are several distinct types of
uncertainty. That is, it was realized that uncertainty is a
multidimensional concept. [\ldots\ That] multidimensional nature of
uncertainty was obscured when uncertainty was conceived solely in
terms of probability theory, in which it is manifested by only one
of its dimensions.
\end{small}
\end{quotation}

Current applications include, among many other, risk analysis,
reliability modeling and decision theory, see \textcite{2009:ISIPTA},
\textcite{2011:ISIPTA} and \textcite{2009:CoolenSchrijner} for recent collections on the subject.%
\footnote{See also, e.g., the list of applications of the IDM given in Section~\ref{sec:idm-and-near-ignorance}.}
As a welcome byproduct, imprecise probability models also provide
a formal superstructure on models considered in robust Bayesian analysis \parencite{2000:rios}, and
frequentist robust statistic in the tradition of \textcite{1973:huberstrassen},
see also \textcite{2009:augustin-hable} for a review.

By considering \emph{sets} of distributions, and corresponding
interval-valued probabilities for events, imprecise probability models allow
to express the quality of the underlying knowledge in an elegant way.
The higher the ambiguity, the larger c.p.\ the sets.
The traditional concept of probability is contained as a special case,
appropriate if and only if there is perfect stochastic information.
This methodology allows also for a natural handling of prior-data conflict.
If prior and data are in conflict, the set of posterior distributions are enlarged,
and inferences become more cautious.%
\footnote{For more details on the topic of imprecision and \pdc, see Section~\ref{sec:jstp}.}

In Section~\ref{sec:discussion}, we briefly report that the CCCP model has a structure
that allows a direct extension to an imprecise probability model
along the lines of Quaeghebeur and de Cooman's \parencite*{2005:quaeghebeurcooman} imprecise probability models
for i.i.d.\ exponential family models. Extending the models further
by applying arguments from \textcite{Walter2009a}%
\footnote{Reproduced in Section~\ref{sec:jstp}.}
yields a powerful generalisation
of the linear regression model that is also capable of a component-specific reaction to prior-data conflict.

\subsection{Prior-data Conflict in the i.i.d.\ Case}
\label{sec:iid}

As a simple demonstration that conjugate models might not react to
prior-data conflict reasonably, inference on the mean of data from a
scaled normal distribution and inference on the category
probabilities in multinomial sampling will be described in the
%following examples~\ref{ex:scn} and \ref{ex:mult}. %\\
following two subsections.

\subsubsection{Samples from a scaled Normal distribution} %$\norm(\mu,1)$}
\label{sec:ex-scn}
The conjugate distribution to an i.i.d.-sample $\x$ of
size $n$ from a scaled normal distribution with mean $\mu$, denoted
by $\norm(\mu,1)$ %as constructed by this method
is a normal distribution with mean $\mu\uz$ and variance ${\sigma\uz}^2$\footnote{Here, and in the
following, parameters of a prior distribution will be denoted by an upper index ${}\uz$, whereas
parameters of the respective posterior distribution by an upper index ${}\un$.}.
The posterior is then again a normal distribution with the following updated parameters:%\vspace*{-4ex}
\footnote{This is the Normal-Normal model from Section~\ref{sec:norm-norm}, where
$\sigma_0^2 = 1$, $\yz = \mu\uz$, and $\nz = 1/{\sigma\uz}^2$.}
\begin{align}
\mu\un &=  \frac{ \frac{1}{n}   }{ \frac{1}{n} + {\sigma\uz}^2 } \mu\uz
          +\frac{ {\sigma\uz}^2 }{ \frac{1}{n} + {\sigma\uz}^2 } \bar{x}
        =  \frac{ \frac{1}{{\sigma\uz}^2} }{ \frac{1}{{\sigma\uz}^2} + n } \mu\uz
          +\frac{ n }                      { \frac{1}{{\sigma\uz}^2} + n } \bar{x} \label{equ:scn-mu1}\\
{\sigma\un}^2 &= \frac{ {\sigma\uz}^2 \cdot \frac{1}{n} }{ {\sigma\uz}^2 + \frac{1}{n} }
               = \frac{1}{ \frac{1}{{\sigma\uz}^2} + n }\,. \label{equ:scn-sig1}
\end{align}
The posterior expectation (and mode) is thus a simple weighted average of the prior mean
$\mu\uz$ and the estimation from data $\bar{x}$, with weights
%$\frac{1}{{\sigma\uz}^2}$
$1/{\sigma\uz}^2$ and $n$, respectively.%
\footnote{The reason for using these seemingly strange weights will become clear later.}
The variance of the posterior distribution is getting smaller automatically.

Now, in a situation where data is scarce, but with prior information one is very confident about,
one would choose a low value for ${\sigma\uz}^2$, thus resulting in a high weight
for the prior mean $\mu\uz$ in the calculation of $\mu\un$.
The posterior distribution will be centered around a mean between $\mu\uz$
and $\bar{x}$, and it will be even more pointed as the prior, because
${\sigma\un}^2$ is considerably smaller than ${\sigma\uz}^2$, the factor to
${\sigma\uz}^2$ in \eqref{equ:scn-sig1} being quite smaller than one.

The posterior basically would thus say that one can be quite sure that the mean $\mu$
is around $\mu\un$, regardless if $\mu\uz$ and $\bar{x}$ were near to each other
or not, where the latter would be a strong hint for prior-data conflict.
The posterior variance does not depend on this; the posterior distribution is thus
insensitive to prior-data conflict.

Even if one is not so confident about one's prior knowledge and thus assigning a
relatively large variance to the prior, the posterior mean is less strongly influenced
by the prior mean, but the posterior variance still is getting smaller, no matter
if the data support the prior information or not.
%and this is the point: it gets smaller by the same ***amount / factor no matter
%if the prior knowledge
%\end{example}

The same insensitivity appears also in the widely used Dirichlet-Multinomial model
as presented in the following subsection:

\subsubsection{Samples from a Multinomial distribution} %$\mult(\btheta)$}
\label{sec:ex-mult}
Given a sample of size $n$ from a multinomial distribution, with
probabilities $\theta_j$ for categories or classes $j= 1,\ldots,k$, subsumed in
the vectorial parameter $\btheta$ (with $\sum_{j=1}^k \theta_j = 1$),
the conjugate prior on $\btheta$ is a Dirichlet distribution $\dir(\balpha)$.
%Written in terms of a reparameterisation used in , e.g. in \textcite{1996:walley::idm},
Written in terms of the canonical parameters $\nz$ and $\yz$ as in Section~\ref{sec:diri-multi},
%$\alpha\uz_j = s\uz \cdot t\uz_j$, such that $\sum_{j=1}^k t\uz_j = 1$, $(t\uz_1,\ldots,t\uz_k)^\tra =: t\uz$,
$\alpha_j = \nz \cdot \yz_j$, such that $\sum_{j=1}^k \yz_j = 1$, $(\yz_1,\ldots,\yz_k)^\tra =: \byz$.
Recall that the components of $\byz$ have a direct
interpretation as prior class probabilities, whereas $\nz$ is a parameter
indicating the confidence in the values of $\byz$, similar to the inverse
variance as in Section~\ref{sec:ex-scn}
(the quantity $\nz$ will appear also in Section \ref{sec:cccp}).%
\footnote{If $\btheta \sim \dir(\nz, \byz)$, then
$\V(\theta_j) = \frac{\yz_j (1-\yz_j)}{\nz+1}$. If $\nz$ is high,
then the variances of $\btheta$ will become low, thus indicating high
confidence in the chosen values of $\byz$.}

As seen in Section~\ref{sec:diri-multi},
the posterior distribution, obtained after updating via Bayes' Rule with a sample vector $\vec{n} = (n_1,\ldots,n_k)$,
$\sum_{j=1}^k n_j = n$ collecting the observed counts in each category,
is a Dirichlet distribution with parameters
\begin{align*}
\yn_j &= \frac{\nz}{\nz + n} \yz_j + \frac{n}{\nz + n} \cdot \frac{n_j}{n}\,, &
\nn   &= \nz + n \,.
\end{align*}
The posterior class probabilities $\byn$ are calculated as a weighted mean
of the prior class probabilities $\byz$ and $\frac{n_j}{n}$, the proportion in the sample,
with weights $\nz$ and $n$, respectively; the confidence parameter $\nz$ is
incremented by the sample size $n$.

Also here, there is no systematic reaction to prior-data conflict. The posterior
variance for each class probability $\theta_j$ is %calculates as
\begin{align*}
\V(\theta_j\mid \vec{n}) &= \frac{\yn_j (1 - \yn_j)}{\nn+1}
                          = \frac{\yn_j (1 - \yn_j)}{\nz+n+1}\,.
\end{align*}
The posterior variance depends heavily on $\yn_j (1 - \yn_j)$,
having values between $0$ and $\frac{1}{4}$, which do not change
specifically to prior data conflict. The denominator increases from
$\nz+1$ to $\nz+n+1$.

Imagine a situation with strong prior information suggesting a value of $\yz_j = 0.25$,
so one could choose $\nz = 5$, resulting in a prior class variance of $\frac{1}{32}$.
Consider a sample of size $n=10$ with all observations belonging to class $j$ (thus $n_j=10$),
being in clear contrast to the prior information. The posterior class probability
is then $\yn_j = 0.75$, resulting the enumerator value of the class variance to remain constant.
Therefore, due to the increasing denominator, the variance decreases to $\frac{3}{256}$,
in spite of the clear conflict between prior and sample information.
Of course, one can also construct situations where the variance increases, but this
happens only in case of an update of $\yz_j$ towards $\frac{1}{2}$.
If $\yz_j = \frac{1}{2}$, the variance will decrease for any degree of prior-data conflict.


\subsection{The Standard Approach for Bayesian Linear Regression (SCP)}
\label{sec:scp}

The regression model is noted as follows:
\begin{equation*}
z_i = \x_i^\tra\bbeta + \varepsilon_i \,,\quad \x_i \in \reals^{p} \,,\;
\bbeta \in \reals^p \,,\; \varepsilon_i \sim \norm(0,\sigma^2)\,,
\end{equation*}
where $z_i$ is the response, $\x_i$ the vector of the $p$ covariates for observation $i$,
and $\bbeta$ is the $p$-dimensional vector of adjacent regression coefficients.

The vector of regressors $\x_i$ for each observation $i$ is generally
considered to be non-stochastic, thus it holds that $z_i \sim \norm(\x_i^\tra\bbeta,\sigma^2)$,
or, for $n$ i.i.d.\ samples, $\z \sim \norm_n(\X\bbeta,\sigma^2 \mbf{I})$,
i.e., $\z \in \reals^n$, the column vector of the responses $z_i$,
has a multivariate normal distribution with vector of expectations $\X\bbeta$,
where $\X \in \reals^{n \times p}$ is the \emph{design matrix}
(of which row $i$ is the vector of covariates $\x_i^\tra$ for observation $i$),
and matrix of variances and covariances $\sigma^2 \mbf{I}$,
where $\mbf{I}$ is the identity or unit matrix of size $n$.%\\

Without loss of generality, one can either assume $x_{i1} = 1 \; \forall i$ such that
the first component of $\bbeta$ is the intercept parameter,%
\footnote{usually denoted by $\beta_0$; however,
we stay with the numbering $1,\ldots, p$ for the components of $\bbeta$.}
or consider only centered responses $\z$ and standardized covariates
to make the estimation of an intercept unnecessary.

%***vorher allgemein: $p(\beta,\,\sigma^2) = p(\beta\mid\sigma^2) p(\sigma^2)$, klassische conjugierte Priori ist NIG, etc...
In Bayesian linear regression analysis, the distribution of the response $\z$ is interpreted as
a distribution of $\z$ given the parameters $\bbeta$ and $\sigma^2$, and %a (class of)
prior distributions on $\bbeta$ and $\sigma^2$ must be considered.
For this, it is convenient to split the joint prior on $\beta$ and $\sigma^2$ as %follows
%\begin{align*}
%p(\beta,\,\sigma^2) &= p(\beta\mid\sigma^2) p(\sigma^2)
%\end{align*}
$p(\bbeta,\sigma^2) = p(\bbeta\mid\sigma^2) p(\sigma^2)$
and to consider conjugate distributions for both parts, respectively.

%\section{**Standard-Modell wie in O'Hagan / FKL***}

In the literature, the proposed conjugate prior for $\bbeta\mid\sigma^2$
is a normal distribution with expectation vector $\mz \in \reals^p$ and variance-covariance
matrix $\sigma^2 \Mz$, where $\Mz$ is a symmetric positive definite matrix of
size $p \times p$. The prior on $\sigma^2$ is an inverse gamma distribution
(i.e., $1/\sigma^2$ is gamma distributed) with parameters $\az$ and $\bz$, in the sense that
\begin{align*}
p(\sigma^2) \propto \frac{1}{(\sigma^2)^{\az + 1}} \exp\Big\{ -\frac{\bz}{\sigma^2} \Big\}\,.
\end{align*}
%$p(\sigma^{-2}) \propto (\sigma^{-2})^{a\uz + 1} \exp\{-b\uz \sigma^{-2} \}$.
The joint prior on $\btheta = (\bbeta,\, \sigma^2)^\tra$ is then denoted as a normal-inverse gamma (NIG) distribution.
The derivation of this prior and the proof of its
conjugacy can be found, e.g., in \textcite{2013:fahrmeier-kneib-lang-marx}, or in \textcite{1994:ohagan},
the latter using a different parameterisation of the inverse gamma part,
where $\az = \frac{d}{2}$ and $\bz = \frac{a}{2}$.

For the prior model, it holds thus that (if $\az > 1$ resp.\ $\az > 2$)
\begin{align}
\begin{aligned}
\E[\bbeta\mid\sigma^2] &= \mz \,,                  & \V(\bbeta\mid\sigma^2) &= \sigma^2  \Mz \,, \\
\E[\sigma^2]           &= \frac{\bz}{\az - 1} \,,  & \V(\sigma^2)           &= \frac{(\bz)^2}{(\az - 1)^2 (\az - 2)}\,.
\end{aligned}
\label{equ:scp-priori_betasigma}
\end{align}

As $\sigma^2$ is considered as nuisance parameter, the unconditional distribution
on $\bbeta$ is of central interest, because it subsumes the shape of prior knowledge on $\bbeta$
as expressed by the choice of parameters $\mz$, $\Mz$, $\az$ and $\bz$.
It can be shown that $p(\bbeta)$ is a multivariate noncentral $t$ distribution
with $2\az$ degrees of freedom, location parameter $\mz$ and dispersion parameter $\frac{\bz}{\az} \Mz$,
such that
\begin{align}
\E[\bbeta] &= \mz \,, &  \V(\bbeta) &= \frac{\bz}{\az - 1} \Mz = \E[\sigma^2] \Mz\,.
\label{equ:scp-priori_beta}
\end{align}

The joint posterior distribution $p(\btheta\mid z)$, due to conjugacy, is then
again a normal-inverse gamma distribution with the updated parameters
\begin{align*}
\mn &= \left( {\Mz}^{-1} + \XtX \right)^{-1} \left( {\Mz}^{-1} \mz + \Xtz \right) \,, \\
\Mn &= \left( {\Mz}^{-1} + \XtX \right)^{-1} \,, \\
\an &= \az + \frac{n}{2} \,, \\ %\qquad
\bn &= \bz + \frac{1}{2} \left( \ztz + {\mz}^\tra {\Mz}^{-1} \mz  - {\mn}^\tra {\Mn}^{-1} \mn \right) \,.
\end{align*}
The properties of the posterior distributions can thus be analyzed
by inserting the updated parameters into \eqref{equ:scp-priori_betasigma}
and \eqref{equ:scp-priori_beta}.


\subsubsection{Update of $\beta\mid\sigma^2$}
\label{sec:scp-update1}

The normal distribution part of the joint prior is updated as follows:%\vspace*{-0.5ex}
\begin{align*}
\E[\bbeta\mid\sigma^2, \z] &= \mn \\
                           &= \big( {\Mz}^{-1} + \XtX \big)^{-1} \!\big( {\Mz}^{-1} \mz + \Xtz \big) \\
                           &= (\mbf{I} - \mbf{A})\, \mz + \mbf{A}\, \bls\,,
\end{align*}
where $\mbf{A} = \big({\Mz}^{-1} + \XtX\big)^{-1} \XtX$. The posterior estimate of $\bbeta\mid\sigma^2$
thus can be seen as a matrix-weighted mean of the prior guess and the least-squares estimate $\bls$.
The larger the diagonal elements of $\Mz$ (i.e., the weaker the prior information),
the smaller the elements of ${\Mz}^{-1}$ and thus the `nearer' is $\mbf{A}$ to the identity matrix,
so that the posterior estimate is nearer to the least-squares estimate.

The posterior variance of $\bbeta\mid\sigma^2$ is %calculates as
\begin{align*}
\V(\bbeta\mid\sigma^2, \z) &= \sigma^2 \Mn = \sigma^2 \left( {\Mz}^{-1} + \XtX \right)^{-1}\,.
\end{align*}
As the elements of ${\Mn}^{-1}$ get larger with as compared to ${\Mz}^{-1}$,
the elements of $\Mn$ will, roughly speaking, become smaller than those of $\Mz$,
so that the variance of $\bbeta\mid\sigma^2$ decreases.

Therefore, the updating of $\bbeta\mid\sigma^2$ is obviously insensitive to prior-data conflict,
because the posterior distribution will not become flatter in case of a large
distance between $\E[\bbeta]$ and $\bls$. %due to this automatic reduction of variances.
Actually, as \textcite{1994:ohagan} derives,
for any $\phi = \vec{a}^\tra \bbeta$, $\vec{a} \in \reals^p$, i.e., any linear combination of elements of $\bbeta$, it holds that
$\V(\phi\mid\sigma^2,\z) \leq \V(\phi\mid\sigma^2)$, becoming a strict inequality if
$\X$ has full rank. In particular, the variance of each $\beta_i$ decreases
automatically with the update step.


\subsubsection{Update of $\sigma^2$}
\label{sec:scp-update2}

It can be shown \parencite{1994:ohagan} that
\begin{align}
\E[\sigma^2\mid \z] &= \frac{2\az - 2}{2\az + n - 2} \E[\sigma^2]
                     + \frac{n - p}   {2\az + n - 2} \sls
                     + \frac{p}       {2\az + n - 2} \spdc\,,
\label{equ:scp-decomposition}
\end{align}
where $\sls = \frac{1}{n-p} (\z - \X\bls)^\tra (\z - \X\bls)$ is
the least-squares based estimate for $\sigma^2$, and
\begin{align*}
\spdc &= \frac{1}{p} (\mz - \bls)^\tra \big(\Mz + (\XtX)^{-1} \big)^{-1} (\mz - \bls)\,.
\end{align*}

For the latter it holds that $\E[\spdc\mid\sigma^2] = \sigma^2$;
the posterior expectation of $\sigma^2$ can thus be seen
as a weighted mean of three estimates:
\begin{enumerate}[(i)]
\item the prior expectation for $\sigma^2$,
\item the least-squares estimate, and
\item an estimate based on a weighted squared difference of the prior mean $\mz$ and $\bls$,
the least-squares estimate for $\bbeta$.
\end{enumerate}
%
The weights depend on $\az$ (one prior parameter for the inverse gamma part),
the sample size $n$, and the dimension of $\bbeta$, respectively.
The role of the first weight gets more plausible when remembering the formula for the prior variance of $\sigma^2$
in \eqref{equ:scp-priori_betasigma}, where $\az$ appears in the denominator. A larger value of $\az$ means
thus smaller prior variance, in turn giving a higher weight for $\E[\sigma^2]$
in the calculation of $\E[\sigma^2\mid \z]$.
The weight to $\sls$ corresponds to the classical degrees of freedom, $n-p$. With the
the sample size approaching infinity, this weight will dominate the others, such that
$\E[\sigma^2\mid \z]$ approaches $\sls$.%, indeed ******.

Similar results hold for the posterior mode instead of the posterior expectation.

Here, the estimate $\spdc$ allows some reaction to prior-data conflict:
it measures the distance between $\mz$ (prior) and $\bls$ (data)
estimates for $\bbeta$, with a large distance resulting basically in a large value of $\spdc$
and thus an enlarged posterior estimate for $\sigma^2$.

The weighting matrix for the distances is playing an important role as well.
%
The influence of $\Mz$ is as follows: for components of $\bbeta$
one is quite certain about the assignment of $\mz$, the respective diagonal elements
of $\Mz$ will be low, so that these diagonal elements of the weighting matrix
will be high. Therefore, large distances in these dimensions will increase $\spdc$ strongly.
An erroneously high confidence in the prior assumptions on $\bbeta$ is thus
penalised by an increasing posterior estimate for $\sigma^2$.
%
The influence of $\XtX$ interprets as follows: covariates with a low spread
in $x$-values, giving an unstable base for the estimate $\bls$,
will result in low diagonal elements of $\XtX$.
%(for centered $\X$, the diagonals are the covariate's variances).
Via the double inverting, those diagonal elements of the weighting matrix
will remain low and thus give the difference a low weight.
Therefore, $\spdc$ will not excessively increase due to a large difference
in dimensions where the location of $\bls$ is to be taken cum grano salis.

As to be seen in the following subsection, the behavior of $\E[\sigma\mid \z]$ is of high
importance for posterior inferences on $\bbeta$.


\subsubsection{Update of $\beta$}
\label{sec:scp-update3}

The posterior distribution of $\bbeta$ is again a multivariate $t$, with expectation
\begin{align*}
\E[\bbeta\mid \z] &= \E\big[ \E[\bbeta\mid\sigma^2, \z] \mid \z \big]= \mn
\end{align*}
as described in Section~\ref{sec:scp-update1},
and variance %\vspace*{-2ex}
\begin{align}
\V[\beta\mid \z]
                  &= \frac{\bn}{\an - 1} \Mn \nonumber\\
                  &= \E[\sigma^2\mid \z]\, \Mn  \label{equ:scp-varbeta}\\
                  &= \bigg( \frac{2\az - 2}{2\az + n - 2} \E[\sigma^2]
                           +\frac{n - p}   {2\az + n - 2} \sls
                           +\frac{p}       {2\az + n - 2} \spdc \bigg) \nonumber\\
                  & \hspace*{5ex}\cdot
                     \left( {\Mz}^{-1} + \XtX \right)^{-1} \nonumber\\
                  &= \bigg( \frac{2\az - 2}{2\az + n - 2} \E[\sigma^2]
                           +\frac{n - p}   {2\az + n - 2} \sls
                           +\frac{p}       {2\az + n - 2} \spdc \bigg) \nonumber\\
                  & \hspace*{5ex}\cdot
                     \Big( \Mz - \Mz \X^\tra (\mbf{I} + \X \Mz \X^\tra)^{-1} \X \Mz \Big) \,, \nonumber%
%\vspace*{-2ex}%
\end{align}
not being directly expressible as a function of $\E[\sigma^2] \Mz$,
the prior variance of $\beta$. %\\ %, due to the double inversion of $\bs{M}\uz$.
%\begin{align*}
%\left( {\bs{M}\uz}^{-1} + \XtX \right)^{-1}
% &= \bs{M}\uz - \bs{M}\uz \X^\tra (\mbf{I} + \X \bs{M}\uz \X^\tra)^{-1} \X \bs{M}\uz
%\end{align*}

Due to the effect of $\E[\sigma^2\mid \z]$, the posterior variance-covariance matrix
of $\bbeta$ can increase in case of prior data conflict, if the rise of $\E[\bbeta\mid \z]$
(due to an even stronger rise of $\spdc$) can overcompensate the decrease in the elements of $\Mn$.
However, we see that the effect of prior-data conflict on the posterior variance of $\bbeta$ %distribution
is \emph{globally} and not component-specific; it influences the variances for \emph{all} components of $\bbeta$
with the same amount, even if the conflict was confined only to some or even just one single component.
Taking it to the extremes, if the prior assignment $\mz$ was (more or less) correct in all but one component,
with that one being far out, the posterior variances will increase for all components,
also for the ones with prior assignments that have turned out to be basically correct.




