\chapter{Concluding Remarks}
\label{cha:concluding}

We will conclude this thesis with a short summary of, and discussion of the central achievments in, this thesis
in Sections~\ref{sec:concluding-summary} and \ref{sec:concluding-discussion}, respectively.
In Section~\ref{sec:concluding-outlook}, we will sketch some opportunities for applications and avenues for further research.


\section{Summary}
\label{sec:concluding-summary}

% SUMMARY
%\subsection{Summary of Contents}

After a detailed motivating example and application in Section~\ref{sec:commoncause},
we presented some theoretical foundations of imprecise or interval probability in Section~\ref{cha:gbi},
with a focus on the approaches leading the way towards a generalisation of Bayesian inference.
There, we also gave some general motives for using imprecise probability methods,
afterwards focussing again on the Bayesian setting, where a foundational motive%
\footnote{The Bayesian approach to inference
requires an unattainable precision in prior assessments
for the foundational arguments for its use (e.g., coherence) to be valid
(see Section~\ref{sec:motivation:bayesian-foundational}).}
and two specific motives were described:
the case of weakly, or (near-) non-informative priors,
and the issue of \emph{prior-data conflict} (Section~\ref{sec:motivation:pdc}),
where strong prior beliefs are in conflict with trusted data,
but data is too sparse to overrule prior beliefs.

After a short view on some approaches understanding themselves as alternative to imprecise probability in Section~\ref{sec:ip-alternatives},
we proposed a general framework for imprecise Bayesian inference
based on sets of conjugate priors in Section~\ref{sec:generalmodel},% %this framework
\footnote{Based on the parametrisation of canonically constructed conjugate priors described in Section~\ref{sec:regularconjugates},
where a prior is identified through a strength parameter $\nz$ and a main parameter $\yz$,
the framework considers sets of priors $\MZ$ induced by sets of parameters $\PZ$, which consist of pairs $(\nz,\yz)$.
The set of posteriors $\MN$, obtained by updating each distribution in the set of priors $\MZ$,
can then be conveniently described through the set of posterior parameters $\PN$,
consisting of pairs of updated parameters $(\nn,\yn)$.}
serving as a bracket and reference point for most of the models investigated in this thesis.
%a number of contributions discussed in this thesis.
%the framework serving as a bracket for the remainder of the topics discussed in the thesis.
Some general results regarding inference properties of models
in this framework were presented (we will comment on these properties in the discussion below),
and the two issues of weakly informative priors and prior-data conflict,
problematic in usual Bayesian inference,
could subsequently be considered as modelling opportunities of generalised Bayesian inference:
\begin{enumerate}[(i)]
\item Imprecise Bayesian methods allow to model weak prior information
more adequately than the so-called non-informative priors usually employed 
(see item~\ref{enum:noninformative}.\ on page~\pageref{enum:noninformative}, and Section~\ref{sec:idm-and-near-ignorance}).
\item As we find that a common class of Bayesian inference procedures
(those based on the canonical conjugates described in Section~\ref{sec:regularconjugates})
provides insuffients inferences in case of \pdc\ %this issue of sensitivity to the prior
(see Section~\ref{sec:iid} for some basic examples, and
Section~\ref{sec:scp} and \ref{sec:cccp} for the case of Bayesian linear regression),
we developed imprecise probability methods that overcome this deficiency,
by mapping ambiguity in posterior inferences now also in dependence of \pdc\ 
(see Sections~\ref{sec:pdc-sensitivity}, and specifically \ref{sec:jstp}).
In these models, a conflict of prior beliefs and data
entails larger posterior parameter sets $\PN$,
leading to cautious inferences if, and only if, caution is needed.
\end{enumerate}

Section~\ref{sec:alternatives} considered some alternative models based on sets of priors,
and discussed their inference properties in comparison to the results in Section~\ref{sec:generalmodel}.
Section~\ref{sec:jstp} then motivated, developed, and illustrated
a central model type from Section~\ref{sec:generalmodel} in more detail;
afterwards, Section~\ref{sec:luck} briefly described a software implementation
of this model framework. %developed in Section~\ref{sec:jstp}.

Finally, Section~\ref{sec:isipta11} presented attempts to further refine
inference behaviour in the presence of prior-data conflict.
In a first approach, the model discussed in Section~\ref{sec:jstp} %Sections~\ref{sec:generalmodel} and \ref{sec:jstp}
was generalised by further tayloring the prior parameter set $\PZ$. %accordingly.
The second, fundamentally different, approach attained \pdc\ sensitivity
by combining \emph{inferences} based on two different priors,
an informative prior (expressing prior beliefs) and an uninformative prior (a near-ignorance prior),
through an imprecise weighting scheme.

As supplemental material, the Appendix (Chapter~\ref{cha:appendix}) contains
\begin{enumerate}[(i)]
\item a study of \pdc\ sensitivity in Bayesian linear regression,
presenting a simplified prior model that gives interesting insights into the updating step
for the regression parameters and offers opportunities for inferences based on sets of priors
(Section~\ref{sec:festschrift});
%\item ***a short summary of a previous approach to Bayesian linear regression
%with sets of priors which was published by \textcite{Walter2007a}, in Section~\ref{sec:isipta07};
\item some first technical results characterising a new prior parameter set shape,
described informally in Section~\ref{sec:concluding-outlook} below
(Section~\ref{sec:boatshape}).
\end{enumerate}


%DISCUSSION
\section{Discussion}
\label{sec:concluding-discussion}

%discussion of issues in Bayesian inference and discussion on IP methods given in Section~\ref{sec:motivation} %Chapter~\ref{cha:gbi}.
%****reasons to use IP models:
%****issues in Bayesian inference: weakly informative priors, \pdc, or in general sensitivity to the prior
%****realistic modeling of partial information

As the model overview in Section~\ref{sec:generalmodel} gives already a detailed discussion
of the imprecise probability models considered in this thesis,%
\footnote{General motives for the use of imprecise probability models
were discussed in Section~\ref{sec:motivation},
with a focus on motives from a Bayesian perspective in Section~\ref{sec:motivation:bayesian}.
These motives can be subsumed as follows:
Generalised Bayesian models allow, in contrast to classical Bayesian models,
a realistic modeling of partial prior information,
and can account for model uncertainty in a preferable way.
Critique on, and possible alternatives to, Generalised Bayesian inference models
were discussed in Section~\ref{sec:ip-alternatives},
while alternatives to the models covered in Section~\ref{sec:generalmodel} are discussed in Section~\ref{sec:alternatives}.}
we will try to emphasize some central points here only.
%summary, condensed here again, emphasising some points

The inference properties described in Section~\ref{sec:gbicp-properties-criteria}
for the generalised Bayesian model framework established in Section~\ref{sec:basicsetting}
are quite remarkable in their generality,
and illustrate the ability of imprecise probability models to realistically model partial information
%in a variety of situations.
for a wide field of inference tasks.
In these models, ambiguity in prior specifications influences ambiguity in posterior inferences
in a natural and comprehensible way.%
\footnote{In contrast, as mentioned in Section~\ref{sec:alternatives:whitcomb},
in imprecise probability models for discrete parameter spaces,
it is for example difficult to judge the weight of a certain prior model as compared to the data.}
%write again about general model framework (Section~\ref{sec:generalmodel}),
%inference properties etc.:

Regarding the further model criterion `\pdc\ sensitivity',
a central concept in this thesis,
we concluded that the shape of the prior parameter set $\PZ$
crucially influences model behaviour, 
and noted at the end of Section~\ref{sec:pdc-sensitivity} that
there is a clear trade-off between easy handling of $\PZ$ or $\PN$, and the desired model property.
Models with fixed prior strength parameter $\nz$,
like the IDM under prior information and the model by \textcite{2005:quaeghebeurcooman},
are very easy to handle,
with $\nz$, the lower ($\yzl$) and upper ($\yzu$) bound of the prior main parameter as the only parameters to elicit,
and $\PN$ being characterised again by the three values $\nn$, $\ynl$ and $\ynu$.%
\footnote{For sake of a clarity,
we consider only one-dimensional main parameters $\yz$ in the discussion here and below,
although the model framework allows for multidimensional $\yz$,
as illustrated by the examples involving the Dirichlet-Multinomial model.}
However, these models are insensitve to \pdc,
and the model with $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$
presented in Section~\ref{sec:4-gw-071216}
mitigates this deficiency, but at the cost of a more complex shape of $\PN$,
not being a cartesian product of $[\nnl, \nnu]$ and $[\ynl, \ynu]$ anymore.
The more refined behaviour discussed in Section~\ref{sec:isipta11}
is achieved by a more complex choice of $\PZ$,
where the range of $\yz$ values depends on $\nz$
via the contour functions $\yzl(\nz)$ and $\yzu(\nz)$
(see Section~\ref{sec:othershapes}).

Revisiting and continuing the discussion of parameter set shapes in Section~\ref{sec:ibbm-resume},
the general framework in principle allows arbitrary shapes for the prior parameter set $\PZ$.
However, freely elicting this set shape
(e.g., by allowing for arbitrary contour functions $\yzl(\nz)$ and $\yzu(\nz)$)
from limited prior information might be very difficult.
If only specific inferences are of interest,
it is possible that only a few aspects of the shape are relevant,
such that elicitation of the entire shape might actually not be necessary.
Furthermore, the derivation of posterior inferences from
such complex prior shapes can be difficult as well.
%discussion of shapes as at the end of Section~\ref{sec:pdc-sensitivity} and \ref{sec:ibbm-resume}***

%These considerations of parameter set shapes notwithstanding, %Nevertheless, 
%For all these models, whether the shape of $\PZ$ turns more to the simple or to the complex end, 
In general, %these
models based on sets of canonical conjugate priors form, in our view,
a `sweet spot' in the realm of (imprecise) statistical models,
by allowing a sophisticated model behaviour
combined with easy elicitation and computation.
%More verbosely***,
In particular,
these models are distinguished by the following characteristics:
%This is ***backed / underlined*** by the following points: 

\begin{itemize}
\item They are relatively easy to elicit:
there is a simple and straightforward interpretation of the model parameters $\nz$ and $\yz$
as prior strength and main parameter, respectively (see Sections~\ref{sec:regularconjugates} and \ref{070517-sec2-1}).
In general terms, the weighted average structure of the update step \eqref{eq:canonicalupdate}
and the model behaviour resulting from it is relatively clear.
\item They are easy to apply in a variety of inference problems:
it is possible to construct conjugate priors as needed for the problem at hand,
often resulting in closed form solutions for many quantities of interest.
\item Nevertheless, they give reasonable and sophisticated inferences:
as discussed above, uncertainty is adequately mirrored by the size of the set of posteriors $\MN$,
fulfilling the inference properties \ref{enum:n0vsn}.--\ref{enum:deltay}.
%criteria from Section~\ref{sec:gbicp-properties-criteria},
with the potential to implement \pdc\ sensitivity or near-noninformativeness,
as required by the inference task at hand.%
\footnote{We think this is a major advantage to the alternative models
discussed in Section~\ref{sec:alternatives},
especially to models based on the density ratio class.
As noted in Section~\ref{sec:alternatives:drc}, the magnitude of $\mathcal{M}_{l,u}$ %imprecision
does not decrease with $n$ in density ratio class models that are updated according to the Generalised Bayes' Rule.}
\item They allow for flexible modelling, as
it is possible to choose or tweak the prior set shape according to inference needs,
where, however, the trade-off mentioned above should be taken into account.
Specifically, we think the model presented in Section~\ref{sec:4-gw-071216}
(with $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$)%
\footnote{This kind of prior set shape was also employed in the motivational example
from Section~\ref{sec:commoncause}, where we argued for an interval-valued $\nz$
at the end of Section~\ref{sec:fixedlearningparameter},
and demonstrated the merits of such a parameter shape for this specific situation of \pdc\
in Section~\ref{sec:intervallearningparameter}.}
%***example in Section~\ref{sec:commoncause}: special case of \pdc\ (zero counts),
constitutes a sensible compromise between model complexity and inference properties.
\end{itemize}

% rectangle ``somewhat peculiar'' from a behavioral point of view (Section~\ref{sec:resume}).
However, some doubts with respect to the rectangular shape of $\PZ$ are nevertheless permitted.
From a strictly behavioral point of view
(e.g., if $\MZ$ should express an expert's prior beliefs),
taking $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$,
is, as we mentioned in Sections~\ref{sec:pdc-sensitivity} and \ref{sec:ibbm-resume}, a somewhat peculiar choice,
as it entails that we assign the same interval for the main parameter $\yz$
over a range of prior strengths.

Instead, it might be more reasonable to assume that the expert is able to give
a more precise interval estimate for lower prior strengths,
while being more cautious in his prior assignments for higher prior strengths
by choosing a wider interval for $\yz$ at $\nzu$.%
\footnote{As described in Section~\ref{sec:ibbm-resume},
such a choice can also lead to a more `tolerant' behaviour in case of \pdc.
If, e.g., $\yzu(\nzl) < \yzu(\nzu)$ was chosen and $\ttau(\x) > \yzu(\nzu)$,
the higher weight $\nzu$ for $\yzu(\nzu)$ in the update step
would move $\ynu(\nnu)$ more slowly towards $\ttau(\x)$ as compared to $\ynu(\nnl)$,
but for moderate degrees of \pdc, the faster movement of $\ynu(\nnl)$
would be offset by the `head start' of $\yzu(\nzu)$.
As the upper bound for $\yn$ is attained at $\ynu(\nnu)$
if $\ttau(\x) < \yzu(\nzu)$ (i.e., data is in agreement with the prior assignements),
the effect of extra imprecision would only appear as soon as
$\ynu(\nnl)$ `overtakes' $\ynu(\nnu)$ in the move towards $\ttau(\x)$, i.e., when $\ynu(\nnl) > \ynu(\nnu)$.}

However, it seems equally reasonable to choose %$\yzu(\nzl) > \yzu(\nzu)$,
a shorter interval for $\yz$ at $\nzu$ than at $\nzl$,
by the considerations described at the beginning of Section~\ref{sec:othershapes},
involving a thought experiment, or `pre-posterior thinking':
%***anteater****
If we designate bounds for $\yn$ constant in $\nn$ and a range of $\nn$ values
%where the corresponding posterior parameter set $\PN$
to express what we want to learn from certain hypothetical data,
the corresponding $\PZ$ derived by doing a `backwards update' has,
due to the update mechanism \eqref{eq:canonicalupdate},
a wider range for low values of $\nz$, and a narrower range for high $\nz$ values.%
\footnote{For this model, to elicit the pre-posterior bounds, certain hypothetical data have to be chosen.
Section~\ref{sec:othershapes} touches only very briefly on these hypothetical data
(that are a part of the model parameters) and how to choose them,
and there is ample opportunity for research here.
One could also further modify the model by allowing to choose
the `pre-posterior $\PN$' more freely, with bounds for $\yn$ not constant in $\nn$.}
For an actual sample size smaller than the hypothetical one,
this `anteater' shape is more `tolerant' as the rectangular shape.%
\footnote{The `anteater' shapes are somewhat similar to shapes that would result
if we required, for information on the main prior parameter $\yz$ symmetrical around $0$,
$|\nz \cdot \yz|$ to be constant.
Such a shape was proposed by \textcite{2012:benavolizaffalon}
for canonical priors to one-parameter exponential family likelihoods,
with the aim to generate near-noninformative prior sets $\MZ$.}
%
(A suggestion for another more sophisticated shape of $\PZ$,
with the aim to allow for extra precision if prior and data coincide especially well,
is discussed in the outlook below.)
%***short note on eggplant shapes in talk? (preliminary results)

% study by krautenbacher shows that elicitation of $\PZ$ can be difficult,
% compares with \textcite{2005:whitcomb}, similar results, see Section~\ref{sec:alternatives}
% should be cautious with skewed distributions, as expectation interval may be misleading (look at HD intervals, too!)
% remember that the canonical conjugate may not behave as expected
However, the favourable properties given in general terms in Section~\ref{sec:generalmodel}
should not obscure the restrictions that are imposed by the canonical conjugate priors
the framework is based on. 

The study by \textcite[see the discussion in Section~\ref{sec:alternatives:whitcomb}]{2011:krautenbacher}
reminds us that fitting a prior parameter set $\PZ$
to available prior information can be non-trivial,
and that also the generally well-understood conjugate distributions may exhibit
unintuitive inference behaviours when focussing on prior and posterior expectations only,
as the model framework tempts us to do.
It is thus advisable to not loose sight of the distributions in $\MZ$ and $\MN$ in their entirety, %as a whole,
e.g., by considering unions of highest density intervals for different credibility levels.

%can be mitigated by choosing $\MZ$ to be set of all convex mixtures.
The restrictions imposed by canonical conjugate priors can be mitigated,
or even completely overcome,
when choosing $\MZ$ not to contain parametric distributions only,
but to comprise all finite mixtures of parametric distributions,
i.e., considering $\MZ$ as the convex hull of the parametric distributions.%
\footnote{As noted in footnote~\ref{foot:denseinthespaceof}, p.~\pageref{foot:denseinthespaceof},
if the parametric distributions are normal distributions and $\PZ$ is large enough,
it can be assumed that $\MZ$ contains a very wide range of priors,
as mixtures of normal distributions are dense in the space of well-behaved probability distributions.}
However, as mentioned in Section~\ref{sec:basicsetting},
then only inferences that are linear in the parametric posteriors are easy to obtain,
and the model may deliver very imprecise, or even vacuous, results
for nonlinear functions of $p(\vartheta\mid\nn,\yn)$ like, e.g., the posterior variance.

It seems that models based on the density ratio class,
the model framework most similar to this variant of the model framework of Section~\ref{sec:basicsetting}
(similar in that they both allow non-parametric priors in the set of prior distributions,
although the model is generated by use of parametric distributions),
do not have this issue of tractability of nonlinear inferences
(see the discussion of the model by \textcite[\S 4]{2011:rinderknecht:diss} in Section~\ref{sec:alternatives:rinderknecht});
while this seems a major strength of the model,
it is in our view offset by a fundamental weakness,
the lack of a clear mechanism by which imprecision in coherent posterior inferences
can be modelled in dependence of sample size.%
\footnote{As mentioned in Sections~\ref{sec:alternatives:drc} and \ref{sec:alternatives:rinderknecht},
imprecision as measured by the magnitude of $\mathcal{M}_{l,u}$ is the same for any sample size $n$
if $\mathcal{M}_{l,u}$ is updated according to Bayes' Rule.
Only if the requirement of coherence, the foundation of the Generalised Bayes' Rule, is dropped,
density ratio class models are possible that allow for imprecision to depend on sample size $n$.
See also footnote~\ref{foot:coolenplusrinderknecht} on page~\pageref{foot:coolenplusrinderknecht},
where we suggested a model based on a combination of ideas from \textcite{1993:coolen} and \textcite{2011:rinderknecht:diss}.}

% double role $\nz$, see Section~\ref{sec:ibbm-resume} here, and again in outlook???
Another, more fundamental, handicap of the model framework from Section~\ref{sec:generalmodel}
is the double role of $\nz$ as mentioned at the end of Section~\ref{sec:ibbm-resume}.
There, the issue is framed in terms of the imprecise Beta-Binomial model,
but it is actually valid for the general case of updating
in imprecise probability models based on canonical conjugate priors:
On the one hand, $\nz$ governs the weighting of prior information $\yz$ with
respect to the data $\ttau(\x)$;
the larger $\nz$, the more the values of $\ynl$ and $\ynu$ are dominated by $\yzl$ and $\yzu$, respectively.
On the other hand, $\nz$ governs also the degree of posterior imprecision:
the larger $\nz$, the larger c.p.\ ${\rm MPI}\un = \ynu - \ynl$.
A larger $\nz$ thus leads to more imprecise posterior inferences,
although a high weight on the supplied prior information
should boost the trust in posterior inferences if $\ttau(\x) \in [\yzl, \yzu]$,
i.e., if prior beliefs turned out to be appropriate.

In Section~\ref{sec:weightedinf},
an approach separating these two roles of $\nz$ was developed,
by considering two separate models,
an uninformative and an informative model,%
\footnote{These two models could also be denoted by `cautious' and `bold', respectively.
While the cautious model tries to use only a minimal amount of information, %presuppose at least as possible**,
the bold model goes for more daring assumptions.}
with individual levels of precision induced by different choices of $\nz$. 
Inference intervals based on these two models are then combined %into a definite*** inference interval
using an imprecise weight, forming the actual inference interval.

This model of \emph{weighted inference} is a very general approach,
being applicable for a wide variety of inference tasks and accomodating all sorts of models that provide interval-valued inferences.
Combining favourable properties 
for the inference situation discussed in Section~\ref{sec:isipta11}
with feasible elicitation and handling (see Section~\ref{wi-prop}),
it is an approach going beyond the model framework of Section~\ref{sec:basicsetting},
and thus the general properties described in Section~\ref{sec:gbicp-properties-criteria}
do not necessarily hold.

\medskip

%emphasise modeling opportunities, and realistic description and treatment
%of model uncertainty as in \ref{sec:objections}
In summary, we think the model framework from Section~\ref{sec:basicsetting}
exploits the full expressive power of imprecise probability %based inference
in a very elegant way, by allowing a realistic description and treatment
of model uncertainty, and thus, as expressed in Section~\ref{sec:objections},
enable us to avoid heroic model assumptions, and the spuriously precise inferences they entail. 
%allowing to do away with heroic model assumptions, and the spuriously precise inferences they entail. 

We also think that there is still further potential to models based on sets of conjugate priors.
As described in the Outlook below,
further advances are within reach 
that, in addition to the properties \ref{enum:n0vsn}.--\ref{enum:deltay}.\ and \pdc\ sensitivity
(see Section~\ref{sec:gbicp-properties-criteria}),
allow for more precise posterior inferences when prior and data coincide especially well.%
\footnote{We already mentioned this this modelling goal in Section~\ref{sec:insights},
where we spoke of \emph{strong prior-data agreement}.}
%further refinements are within reach (see outlook below).



\section{Outlook}
\label{sec:concluding-outlook}

%***avenues for further research***
%Here, we come back to the ideas 
Here, we will summarise central ideas discussed in several concluding sections
(\ref{sec:6-gw-071216}, \ref{sec:insights}, \ref{sec:discussion-festschrift}),
carrying some of them further, presenting %giving an ***overview of** %in order to describe 
opportunities for applications and potential avenues for further research.
In particular, an idea for modelling of \emph{strong prior-data agreement} will be explained in more detail.

We will subsume our ideas for further research and developments in three steps:
First, we will consider some potential applications and areas of study for the
currently existing models in the framework from Section~\ref{sec:basicsetting}.
Then, we will sketch some ideas to extend the presently available models,
including a discussion of a novel parameter set shape that allows to cater for
\emph{strong prior-data agreement} (see the technical details in Section~\ref{sec:boatshape}).
These further developments still follow the lines %are **still a part of the framework***
of generalised Bayesian inference using sets of priors as described in Sections~\ref{sec:gbr} and \ref{sec:imprecisebayes}
that ensure coherence of inferences by updating the set of priors via the Generalised Bayes' Rule.
In the last part of this outlook, we will look instead beyond this framework
of coherent Bayesian inference, by discussing some general thoughts
about updating and learning in the context of statistical inference.

\medskip

%***\textbf{first: model as is, study and use for\dots }
%***comparison with other approaches in Section~\ref{sec:alternatives}, especially drc***
As seen in Section~\ref{sec:alternatives}, there are a number of alternative models
for statistical inference using sets of priors.
Besides the study by \textcite{2011:krautenbacher},
no studies comparing inferences from the models described in Setions~\ref{sec:jstp} and \ref{sec:isipta11}
with those based on the alternative models named in Section~\ref{sec:alternatives}
(most importantly, models based on the density ratio class)
in detail have yet been conducted to our knowledge.
Such studies could be rewarding
by giving more detailed insight into strengths and weaknesses of the respective models.
%***more studies for comparisons with the model from \ref{sec:generalmodel} to be done,
%e.g., with hierarchical models (Section~\ref{sec:hierarchical})?***
Further insight could also be gained from a comparison to the hierarchical model
developed by \textcite[see Section~\ref{sec:hierarchical}]{2008:cattaneo}.

There are of course many opportunities for application of the models described in this thesis,
and we will mention just two interesting general cases here.

%***use regression framework from Section~\ref{sec:cccp} in situations prone to \pdc.
The canonical conjugate prior for Bayesian linear regression constructed in Section~\ref{sec:cccp}
could be used for an imprecise regression analysis based on sets of priors.
Although being probably the most important concept in modern statistical inference,
in the imprecise probability literature,
so far only very few contributions considering regression analysis have been published.%
\footnote{Among the few exceptions are \textcite{Walter2007a}, \textcite{2010:utkin},
\textcite{2011:utkin}, \textcite{2012:cattaneo-wiencierz}, and \textcite{2013:utkin-wiencierz}.}
Work in this direction could contribute to a major step towards a wider application
of imprecise probability models in statistical practice.% 
\footnote{In contrast, classification models are a thriving subject area in imprecise probability.
For a recent overview see, e.g., \textcite{itip-classification}.}
With priors based on $\PZ$ of type~(\ref{enum:rectangular}),
the model could offer favourable inference properties in situations that are prone to \pdc\
(see the discussion in Section~\ref{sec:discussion-festschrift}).

%***model could be vey useful in statistical surveillance, cite IESS:surveillance?***,
Another potentially fruitful area of application is models for \emph{statistical surveillance}
\parencite[for a brief overview see, e.g.,][]{2011:IESS-surveillance}.
Here, the aim is to monitor a data-generating process over time
to detect changes in the process as early as possible,
without generating too many `false alarms'.
A typical example is disease monitoring,
where the number of cases of a certain infectious disease reported by clinicians
is continuously monitored on the national level,
with the aim to detect epidemic outbreaks in their early stages.
Such outbreak detection is usually analysed using likelihood ratios,
comparing new observations with a model derived from previous observations. 
As mentioned in footnote~\ref{foot:sequential}, page~\pageref{foot:sequential},
we could also consider this problem in terms of \pdc:
if a new batch of $m$ observations does not fit our current model
(a set $\MN$ subsuming prior information and previous data of size $n$),
the posterior model, based on $\mathcal{M}^{(n+m)}$, resonates this by increased imprecision,
triggering the alarm.
We would expect rectangular parameter set shapes to perform quite well in such a task,
but also other set shapes could prove useful here.

\medskip

%***\textbf{then: extend the model, but keep GBR framework}
Now, we will present some ideas for further development within the framework
described in Section~\ref{sec:basicsetting}, i.e.,
for models based on sets of canonical conjugate priors
that are updated via the Generalised Bayes' Rule;
afterwards, we will also consider some potential approaches outside this framework.

As a first important development,
the models discussed in this thesis rely on a precise sampling model,
%from \textcite[4.4]{itip-statinf}
but the generalised Bayesian inference framework also allows for imprecise sampling models
\parencite[see, in particular,][\S 8.5]{1991:walley},
also discussed under the notion of \emph{likelihood robustness} \parencite[e.g.,][]{2000:shyamalkumar} in the robust Bayesian framework.
This could be done, as mentioned in Section~\ref{sec:insights},
in an analogous way to \textcite{2007:utkinaugustin} and \textcite{2009:Troffaes:Coolen},
allowing to consider coarse or imprecise data adequately in the model,
not idealising the data in a similar way as we ceased to idealise prior information by allowing for imprecision.
%The detailed consequences of this development remain to be studied.
%questions on (data) robustness of IP models, see isipta13 paper by Marco

%new weakly informative prior models / prior near-ignorance models by Mangili \& Benavoli (ISIPTA'13) better in 3.2??

%***study multidimensional stuff in more detail: definition of pdc now dimension-wise, other shapes?
%***in general, how to elicit more complex shapes, especially difficult for multidimensional $\yz$
Another important area for further study concerns the case of multidimensional $\yz$.
Elicitation of such high-dimensional parameter sets $\PZ$ poses interesting challenges.
The simple, `hyper-rectangle' set suggested in Section~\ref{sec:imprecise-alpha}, Equation~\eqref{eq:hyperparams:boxmodel},
although an effective choice for the application considered there,
might not be adequate in all circumstances.
However, elicitation of more general sets $\YZ \times [\nzl, \nzu]$,
or even arbitrary subsets of $\Y \times \posreals$, can be very difficult,
and further inquiries into this problem could be rewarding,
also in connection to the application of the prior constructed in Section~\ref{sec:cccp},
as problems in regression typically involve many covariates.

Also, for $\PZ$ as in Equation~\eqref{eq:hyperparams:boxmodel},
\pdc\ is mirrored by increased imprecision for each dimension separately
(see also Example~\ref{ex:jstp-8}, Figure~\ref{fig:idm-nvar-nopdc}).
It is an open question if other prior set shapes entail
different consequences with respect to multidimensional \pdc\ sensitivity. 
%***take only pair of $\nz$ values? behavioural implications?
A possible approach to simplify elicitation in high-dimensional cases
(that could also be useful for lower-dimensional $\yz$) is to not consider
$\NZ = [\nzl, \nzu]$, but $\NZ = \{\nzl, \nzu\}$,
i.e., taking only a pair of $\nz$ values.
This would not only simplify elicitation, but also make computation of posterior inferences
probably more feasible.
However, similar to the discussion of the rectangular shape in Section~\ref{sec:concluding-discussion} above,
the behavioural implications of such a choice of parameter set would have to be studied in detail.

%***boatshape stuff: cater also for strong prior-data agreement***
Before we will discuss more general arguments about learning and updating,
we will now describe a novel idea for generating parameter prior sets $\PZ$
that, in addition to \pdc\ sensitivity, allow to mirror \emph{strong prior-data agreement}
(i.e., the case when prior and data coincide especially well)
by increased posterior precision.

As mentioned in Sections~\ref{sec:pdc-sensitivity} and \ref{sec:ibbm-resume},
the shape of $\PZ$ determines the shape of $\PN$ and, via $\MN$, has a crucial influence on posterior inferences.
In general lines, this influence is clear
(see the studies on the behaviour of rectangular shapes in Sections~\ref{sec:jstp} and \ref{sec:isipta11}),
but it is nevertheless quite difficult to elicit more general set \emph{shapes} and to ascertain their consequences.

Although the updating of the canonical parameters according to \eqref{eq:canonicalupdate},
i.e., the weighted average update step for $\yz$, and the increment step for $\nz$,
seems very intuitive (and is central to the behaviour of the model,
see the list of inference properties in Section~\ref{sec:gbicp-properties-criteria}),
the shape change of $\PZ$ to $\PN$ through this update step,
and its effects on posterior inferences, is difficult to grasp.
This is due to the fact that,
while the update of a single coordinate $(\nz,\yz)$ is a simple shift,
this shift is different for each of the coordinates in a set;
in fact, the shift of the $\yz$ component changes with both $\nz$ and $\yz$.%
\footnote{As given by \eqref{eq:canonicalupdate},
for the $\nz$ coordinate holds $\nz \mapsto \nz+n$,
while for the $\yz$ coordinate holds $\yz \mapsto \frac{\nz\yz + \tau(\x)}{\nz+n} = \yz + \frac{\tau(\x)-n\yz}{\nz+n}$.}
These different shifts for the coordinates within a set
lead to the change of shape from $\PZ$ to $\PN$.

As this shape change is so problematic for understanding of the update step,
a different parametrisation of the canonical priors
in which each coordinate has the same shift in updating
would be advantageous,
such that updating of parameter sets
could be expressed as a shift of the entire set within the parameter space.

%yes, there is, see preliminary results by Mik Bickis (``personal communication'' if no citable work?).
In fact, such a parametrisation has been developed by Mi\c{k}elis Bickis (\cite*{2011:bickis:geomip}, personal communication),
and he is currently preparing a manuscript elaborating the details of his findings.
In this parametrisation, a canonical prior is represented by a coordinate $(\ezz,\eoz)$,
where $\eoz$ replaces the main prior parameter $\yz$,
while $\ezz$ is just a different name for $\nz$.%
%there, update step for $\nz$ is the same, and shift for parameter replacing $\yz$ is the same for all. 
\footnote{This parametrisation is described in more detail in Section~\ref{sec:boatshape},
along with some first technical results that confirm the desired properties for the novel shape suggested there.}
In the parametrisation from Section~\ref{sec:regularconjugates},
$\yz$ had the convenient property of being equal to $\E\big[\E[\ttau(\x) \mid \psi] \mid \nz, \yz\big]$,
giving the (prior) expectation, or a prior guess, for the mean sample statistic $\ttau(\x)$.
%rays of constant expectation (i.e., $\approx \yn$)
Naturally, $\eoz$ cannot have this property,
but in the transformed space for the Beta-Binomial model, the points on rays emanating from the coordinate $(-2,0)$
will give a constant expectation of $\E[\ttau(\x) \mid \psi]$,
such that interpretation of parameter sets in terms of $(\ezz,\eoz)$ is still relatively easy.
%As detailed in Section~\ref{sec:boatshape},
With the set shape unchanged by the update, 
tailoring shapes for desired inference behaviour is much easier in this representation.
%because the \emph{shapes} remain constant.
%GW and FC have thought of a shape that has \pdc\ sensitivity,
Indeed, Frank Coolen and the author of this thesis have devised
a set shape %in this parametrisation
that allows for both \pdc\ sensitivity
%and gives `bonus precision' if prior and data agree especially well,
and more precise inferences in case of strong prior-data agreement,
a behaviour deemed very desirable in Section~\ref{sec:insights}
and in the discussion of the Generalised Bayes' Rule in Section~\ref{sec:updating}.
We are thus able to offer a solution to this issue that allows to remain within the generalised Bayesian framework
by using the Generalised Bayes' Rule for updating.

As our preliminary studies show some very appealing results in case of the Beta-Binomial model,
for which a possible parametrisation of our shape is discussed in Section~\ref{sec:boatshape},
we are confident that these encouraging results also hold for the Normal-Normal model
and in the general case of canonical conjugates.
A joint publication of Mi\c{k}elis Bickis, Frank Coolen and the author of this thesis is planned
that will elaborate these findings in more detail.

\medskip

%\textbf{finally, general thoughts about updating, learning, etc}
Concluding the outlook, we will now turn to more general thoughts about updating and learning
in the context of statistical inference using sets of priors.

%elaborate thoughts in Section~\ref{sec:updating} again,
%see also comments at end of Section~\ref{sec:6-gw-071216},
%***We discussed some general critique on the Generalised Bayes' Rule
%and inferences based on it in Section~\ref{sec:updating}
%and at the end of Section~\ref{sec:6-gw-071216}???.
%***subsuming? ***We will thus pick out only some aspects here, discussing them in a larger context.***

As we wrote in Section~\ref{sec:6-gw-071216},
it should well be remembered that the models considered in this thesis
consciously confined the whole argumentation to a certain Bayesian setting:
we studied how far one can go \emph{if} one relies strictly on the Generalised Bayes' Rule,
transferring sets of priors to sets of posteriors element by element via Bayes' Rule \eqref{eq:bayesrule}.
Considering the criticisms regarding the Generalised Bayes' Rule discussed in Section~\ref{sec:updating},
alternative learning rules could thus provide superiour model behaviour.%
\footnote{In Section~\ref{sec:alternatives}, we discussed some of the approaches
mentioned in footnote~\ref{foot:learningrules}, page~\pageref{foot:learningrules},
that consider alternative learning rules.}

However, we argue that some aspects of inferences based on the Generalised Bayes' Rule that are critizised
in the literature
%\footnote{See in particular Section~\ref{sec:updating}.***}
can be confronted, or at least mitigated, by a careful choice of the sets of priors.

We consider this, to some extent, to be the case for the critique that 
the Generalised Bayes' Rule is too inflexible when prior information is confronted with surprising data,
as it insists on coherence of posterior inferences with prior assumptions that,
in the light of the data, may turn out to be inadequate.
The models discussed in Sections~\ref{sec:pdc-sensitivity} and \ref{sec:4-gw-071216}
mitigate this issue by allowing for \pdc\ sensitivity.
The resulting posterior sets are still, in essence, a compromise between the prior set and the data,
but mirror the conflict between them by increased imprecision.
This increased imprecision can then serve as a `warning light',
highlighting the doubts with regards to the posterior model in such a conflict,
which may ultimatively motivate the analyst to reconsider her prior assignments.

Another critique based on the perceived inflexibility of the Generalised Bayes' Rule
relates to posterior inferences being often `too imprecise'.
For the case of weak prior information, this has been confronted
by approaches by \textcite{1996:walley::idm}, \textcite{2009:bickis}, \textcite{2012:benavolizaffalon},
and, most recently, by \textcite{2013:mangilibenavoli}.
Although starting from a set of near-noninformative priors, %priors expressing near-ignorance.
these approaches lead to reasonably precise posterior inferences.
Our approach for a novel parameter set $\PZ$, %based on a different parametrisation
described above and in Section~\ref{sec:boatshape},
may be able to fend off this criticism also for informative priors by
offering especially precise inferences when prior information and data are in accordance.

Other critical aspects of the Generalised Bayes' Rule are more difficult to tackle.
Especially the caveat by \textcite{2003:augustin},
showing that the decision theoretic justification of Bayes' Rule
as producing prior risk minimising decision functions does not extend to the case of sets of priors,
should motivate us to look beyond the Generalised Bayes' Rule.
The issue here is that the Generalised Bayes' Rule
may not lead to optimal inference procedures,
as the optimality of the corresponding decision functions is not guaranteed.%
\footnote{For a brief overview on decision theory in the context of imprecise probability methods, see \textcite{itip-decision}.}
From this angle, the desire for alternative learning rules
gains a more solid footing in our view.

Indeed, we already considered some models going beyond the generalised Bayesian framework.
Apart from our suggestion in footnote~\ref{foot:coolenplusrinderknecht}, page~\pageref{foot:coolenplusrinderknecht},
that could lead to an interesting density ratio class model combining sophisticated elicitation with reasonable handling of imprecision,
we think that a very attractive approach to inference where prior information can be included in the reasoning
is the model by \textcite[see Section~\ref{sec:hierarchical}]{2008:cattaneo}.
Along the lines of \textcite{2011:4:isipta},
it would even be possible to apply the hierarchical modelling to the framework of Section~\ref{sec:basicsetting}.

An approach beyond the Generalised Bayes' Rule that we studied in more detail
is the weighting model from Section~\ref{sec:weightedinf}.
As mentioned in Section~\ref{sec:concluding-discussion} above,
it was motivated by the dual role of $\nz$ in the framework from Section~\ref{sec:basicsetting},
controlling both posterior precision and derivation from prior assignments
that can lead to unintuitive results in case of strong prior data agreement (see Section~\ref{sec:ibbm-resume}).
Another idea for models outside the generalised Bayesian framework
related to the role of $\nz$ was discussed in Section~\ref{sec:insights},
where we argued that a possible way to react to prior-data
conflict could be to consider the combined information of prior and data
to be of less value than either the data themselves
or than both information sources separately.
When strong prior beliefs collide with contradicting data,
this could lead to severe doubt about what is actually going on.
To model this behaviour, one could consider allowing the prior strength parameter $\nz$
to take on negative values, opening up a rich field for research and discussions.


We also hope that further studies like, e.g., those in temporal coherence as mentioned in Section~\ref{sec:updating},
refining the concept of coherence towards allowing very substantial revisions in case of surprising data, %
%\footnote{We briefly mentioned this concept of \emph{temporal coherence} %\parencite{1985:goldstein::temporal}
%in Section~\ref{sec:updating}.}
will pave the way for models resonating the %(partly philosophical)
reasoning of Hampel \parencite*{2009:hampel:knowledge,2011:hampel},
who argues that in order to represent learning,
our models must allow to revise 
the whole system of background knowledge 
in the light of surprising observations.
%we must be able to model situations
%where the whole system of background knowledge is revised
%in the light of surprising observations.











