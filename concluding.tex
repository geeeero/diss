\chapter{Concluding Remarks}
\label{cha:concluding}

We will conclude this thesis with a short summary of, and discussion of the central achievments in, this thesis
in Sections~\ref{sec:concluding-summary} and \ref{sec:concluding-discussion}, respectively.
In Section~\ref{sec:concluding-outlook}, we will sketch some opportunities for applications and avenues for further research.


\section{Summary***}
\label{sec:concluding-summary}

% SUMMARY
%\subsection{Summary of Contents}

After a detailed motivational example and application in Section~\ref{sec:commoncause},
we presented some theoretical foundations of imprecise or interval probability in Section~\ref{cha:gbi},
with a focus on the approaches leading the way towards a generalisation of Bayesian inference.
There, we also gave some general motives for using imprecise probability methods,
afterwards focussing again on the Bayesian setting, where a foundational motive%
\footnote{The Bayesian approach to inference
requires an unattainable precision in prior assessments
for the foundational arguments for its use (e.g., coherence) to be valid
(see Section~\ref{sec:motivation:bayesian-foundational}).}
and two specific motives were described:
the case of weakly, or (near-) non-informative priors,
and the issue of \emph{prior-data conflict} (Section~\ref{sec:motivation:pdc}),
where strong prior beliefs are in conflict with trusted data,
but data is too sparse to overrule prior beliefs.

After a short view on some approaches in alternative to imprecise probability,
we discussed a general framework for imprecise Bayesian inference
based on sets of conjugate priors in Section~\ref{sec:generalmodel},
this framework serving as a bracket for most of the models ***discussed / investigated*** in this thesis.
%a number of contributions discussed in this thesis.
%the framework serving as a bracket for the remainder of the topics discussed in the thesis.
Some general results regarding inference properties of models
in this framework are presented (we will comment on these properties in the discussion below),
and the two issues of weakly informative priors and prior-data conflict,
problematic in usual Bayesian inference,
can now be considered as modelling opportunities of generalised Bayesian inference:
\begin{enumerate}[(i)]
\item Imprecise Bayesian methods allow to model weak prior information
more adequately than the so-called non-informative priors usually employed 
(see item~\ref{enum:noninformative} on page~\pageref{enum:noninformative}, and Section~\ref{sec:idm-and-near-ignorance}).
\item As a common class of Bayesian inference procedures
(those based on the canonical conjugates described in Section~\ref{sec:regularconjugates})
in our view deals very badly with \pdc\ %this issue of sensitivity to the prior
(see Section~\ref{sec:iid} for some basic examples, and
Section~\ref{sec:scp} and \ref{sec:cccp} for the case of Bayesian linear regression),
we developed imprecise probability methods that overcome this deficiency,
by mapping ambiguity in posterior inferences now also in dependence of \pdc\ 
(see Sections~\ref{sec:pdc-sensitivity}, and specifically \ref{sec:jstp}).
In these models, a conflict of prior beliefs and data
entails larger posterior parameter sets $\PN$,
leading to cautious inferences if, and only if, caution is needed.
\end{enumerate}

Section~\ref{sec:alternatives} described some alternative models based on sets of priors,
and discussed their inference properties in comparison to the results in Section~\ref{sec:generalmodel}.
Section~\ref{sec:jstp} then motivated, developed, and illustrated
an central model type from Section~\ref{sec:generalmodel} in more detail;
afterwards, Section~\ref{sec:luck} briefly presented a software implementation
of the model framework developed in Section~\ref{sec:jstp}.

Finally, Section~\ref{sec:isipta11} presents attempts to further refine
inference behaviour in the presence of prior-data conflict.
In a first approach, the models discussed in Sections~\ref{sec:generalmodel} and \ref{sec:jstp}
are generalised by tayloring the prior parameter set $\PZ$ accordingly.
The second, fundamentally different, approach attains \pdc\ sensitivity
by combining \emph{inferences} based on two different priors,
an informative prior (expressing prior beliefs) and an uninformative prior (a near-ignorance prior),
through an imprecise weighting scheme.

As supplemental material, the Appendix (Chapter~\ref{cha:appendix}) contains
\begin{enumerate}[(i)]
\item a study of \pdc\ sensitivity in Bayesian linear regression,
presenting a simplified prior model that gives interesting insights into the updating step
for the regression parameters and offers opportunities for inferences based on sets of priors
(Section~\ref{sec:festschrift});
\item a short summary of a previous approach to Bayesian linear regression
with sets of priors which was published by \textcite{Walter2007a}, in Section~\ref{sec:isipta07};
\item some first technical results characterising a new prior parameter set shape,
described informally in Section~\ref{sec:concluding-outlook} below
(Section~\ref{sec:boatshape}).
\end{enumerate}


%DISCUSSION
\section{***Discussion}
\label{sec:concluding-discussion}

%discussion of issues in Bayesian inference and discussion on IP methods given in Section~\ref{sec:motivation} %Chapter~\ref{cha:gbi}.
%****reasons to use IP models:
%****issues in Bayesian inference: weakly informative priors, \pdc, or in general sensitivity to the prior
%****realistic modeling of partial information

As the model overview in Section~\ref{sec:generalmodel} gives already a good discussion
of the imprecise probability models considered in this thesis,%
\footnote{General motives for the use of imprecise probability models
were discussed in Section~\ref{sec:motivation},
with a focus on motives from a Bayesian perspective in Section~\ref{sec:motivation:bayesian}.
These motives can be subsumed as follows:
Generalised Bayesian models allow, in contrast to classical Bayesian models,
a realistic modeling of partial prior information,
and can account for model uncertainty in a preferable way.
Critique on and possible alternatives to Generalised Bayesian inference models
were discussed in Section~\ref{sec:ip-alternatives},
while alternatives to the models covered in Section~\ref{sec:generalmodel} are discussed in Section~\ref{sec:alternatives}.}
we will try to emphasize some central points here only.
%summary, condensed here again, emphasising some points

The inference properties described in Section~\ref{sec:gbicp-properties-criteria}
for the generalised Bayesian model framework established in Section~\ref{sec:basicsetting}
are quite remarkable,***
and illustrate the ability of imprecise probability models to realistically model partial information,
in a variety of situations.
In these models, ambiguity in prior specifications influences ambiguity in posterior inferences
in a natural and comprehensible way.
%write again about general model framework (Section~\ref{sec:generalmodel}),
%inference properties etc.:

Regarding the further model criterion `\pdc\ sensitivity',
central to this thesis,
we concluded that the shape of the prior parameter set $\PZ$
crucially influences model behaviour, 
and noted at the end of Section~\ref{sec:pdc-sensitivity} that
there is a clear trade-off between easy handling of $\PZ$ or $\PN$, and the desired model property.
Models with fixed $\nz$, like the IDM under prior information and the model by \textcite{2005:quaeghebeurcooman},
are very easy to handle, with $\nz$, $\yzl$ and $\yzu$ as the only parameters to elicit,
and $\PN$ being characterised again by the three values $\nn$, $\ynl$ and $\ynu$.
However, these models are insensitve to \pdc,
and the model with $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$
presented in Section~\ref{sec:4-gw-071216}
mitigates this deficiency, but at the cost of a more complex shape of $\PN$,
not being a cartesian product of $[\nnl, \nnu]$ and $[\ynl, \ynu]$ anymore.
The more refined behaviour discussed in Section~\ref{sec:isipta11}
is achieved by a more complex choice of $\PZ$,
where the range of $\yz$ values depends on $\nz$
via the contour functions $\yzl(\nz)$ and $\yzu(\nz)$
(see Section~\ref{sec:othershapes}).

Continuing*** the discussion of parameter set shapes as in Section~\ref{sec:ibbm-resume},
the general framework in principle allows arbitrary shapes for the prior parameter set $\PZ$.
However, freely elicting this set shape
(e.g., by allowing for arbitrary contour functions $\yzl(\nz)$ and $\yzu(\nz)$)
from limited prior information might be very difficult.
If only specific inferences are of interest,
it is possible that only a few aspects of the shape are relevant,
such that elicitation of the entire shape might not be necessary.
Furthermore, the derivation of posterior inferences from
such complex prior shapes can be difficult as well.
%discussion of shapes as at the end of Section~\ref{sec:pdc-sensitivity} and \ref{sec:ibbm-resume}***

%These considerations of parameter set shapes notwithstanding, %Nevertheless, 
%For all these models, whether the shape of $\PZ$ turns more to the simple or to the complex end, 
In general, %these
models based on sets of canonical conjugate priors form, in our view,
a `sweet spot' in the realm of (imprecise) statistical models,
by allowing a sophisticated model behaviour
combined with easy elictitation and computation.
More verbosely***,
these models are distinguished by the following characteristics:
%This is ***backed / underlined*** by the following points: 

\begin{itemize}
\item They are relatively easy to elicit:
there is a simple and straightforward interpretation of the model parameters $\nz$ and $\yz$
as prior strength and main parameter, respectively (Sections~\ref{sec:regularconjugates} and \ref{070517-sec2-1}).
In general terms, the weighted average structure of the update step \eqref{eq:canonicalupdate}
and the model behaviour resulting from it is relatively clear.
\item They are easy to apply in a variety of inference problems:
it is possible to construct conjugate priors as needed for the problem at hand,
often resulting in closed form solutions for many quantities of interest.
\item Nevertheless, they give reasonable and sophisticated inferences:
as discussed above, uncertainty is adequately mirrored by the size of $\MN$,
fulfilling the inference properties \ref{enum:n0vsn}--\ref{enum:deltay}
%criteria from Section~\ref{sec:gbicp-properties-criteria},
with the potential to implement \pdc\ sensitivity or near-noninformativeness,
as required by the inference task at hand.%
\footnote{We think this is a major advantage to the alternative models
discussed in Section~\ref{sec:alternatives},
especially to models based on the density ratio class.
As noted in Section~\ref{alternatives:drc}, imprecision does not decrease with $n$ in
density ratio class models that are updated according to the Generalised Bayes' Rule.}
\item They allow for flexible modelling, as
it is possible to choose or tweak the prior set shape according to inference needs,
where, however, the trade-off mentioned above should be ***kept in mind***.
Specifically, we think the model presented in Section~\ref{sec:4-gw-071216}
($\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$)%
\footnote{This kind of prior set shape was also employed in the motivational example
from Section~\ref{sec:commoncause}, where we argued for an interval-valued $\nz$
at the end of Section~\ref{sec:fixedlearningparameter},
and demonstrated the merits of such a parameter shape for this specific situation of \pdc\
in Section~\ref{sec:intervallearningparameter}.}
%***example in Section~\ref{sec:commoncause}: special case of \pdc\ (zero counts),
constitues a sensible compromise between model complexity and inference properties.
\end{itemize}

***However, some doubts regarding the rectangular shape of $\PZ$ are ***in order.
From a strictly behavioral point of view
(e.g., considering $\MZ$ to express an expert's belief),
taking $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$,
is, as we mentioned in Sections~\ref{sec:pdc-sensitivity} and \ref{sec:ibbm-resume}, a somewhat peculiar choice.
It indicates*** that we assign the same interval for the main parameter $\yz$
over a range of prior strengths.

It might be more reasonable to assume that the expert is able to give
a more precise interval estimate for lower prior strengths,
while being more cautious in his prior assignments for higher prior strengths
by choosing a wider interval for $\yz$ at $\nzu$.%
\footnote{As described in Section~\ref{sec:ibbm-resume},
such a choice could also lead to a more `tolerant' behaviour in case of \pdc.
If, e.g., $\yzu(\nzl) < \yzu(\nzu)$ was chosen and $\ttau(\x) > \yzu(\nzu)$,
the higher weight $\nzu$ for $\yzu(\nzu)$ in the update step
would move $\ynu(\nnu)$ more slowly towards $\ttau(\x)$ as compared to $\ynu(\nnu)$,
but for moderate degrees of \pdc, the faster movement of $\ynu(\nnl)$
would be offset by the head start of $\yzu(\nzu)$.
As the upper bound for $\yn$ is attained at $\ynu(\nnu)$
if $\ttau(\x) < \yzu(\nzu)$ (i.e., data is in agreement with the prior assignements),
the effect of extra imprecision would only appear as soon as $\ynu(\nnl) > \ynu(\nnu)$.}

However, it seems equally reasonable to choose $\yzu(\nzl) > \yzu(\nzu)$,
by the considerations described*** at the beginning of Section~\ref{sec:othershapes},
involving a thought experiment, i.e., doing a pre-posterior analysis:
%***anteater****
If we designate bounds for $\yn$ constant in $\nn$ and a range of $\nn$ values,
%where the corresponding posterior parameter set $\PN$
expressing what we want to learn from certain hypothetical data,
the corresponding $\PZ$ derived by doing a `backwards update' has,
due to the update mechanism \eqref{eq:canonicalupdate},
a wider range for high values of $\nz$, and a narrower range for lower $\nz$ values.%
\footnote{For this model, to elicit the pre-posterior bounds, certain hypothetical data have to be chosen.
Section~\ref{sec:othershapes} touches only very briefly on these hypothetical data
(that are a part of the model parameters) and how to choose it,
and there is ample opportunity for research here.
One could also modify the model by allowing to choose
the `pre-posterior $\PN$' more freely ($\yn$ not constant in $\nn$).}
For an actual sample size smaller than the hypothetical one,
also this shape is more `tolerant' as the rectangular shape.

The resulting shapes are similar to shapes that would result
if we required, for information on the main prior parameter $\yz$ symmetrical around $0$,
$|\nz \cdot \yz|$ to be constant.
Such a shape was proposed by \textcite{2012:benavolizaffalon}
for canonical priors to one-parameter exponential family likelihoods,
with the aim to generate near-noninformative prior sets $\MZ$.



***short note on eggplant shapes in talk? (preliminary results)

%``somewhat peculiar'' from a behavioral point of view (Section~\ref{sec:resume}).

***study by ***krautenbacher*** shows that elicitation of $\PZ$ can be difficult,
compares with \textcite{2005:whitcomb}, similar results, see Section~\ref{sec:alternatives}****

***comparison with other approaches in Section~\ref{sec:alternatives}***

***more studies for comparison with the model from \ref{sec:generalmodel} to be done,
also with hierarchical models (Section~\ref{sec:hierarchical})?***


***double role $\nz$, see Section~\ref{sec:ibbm-resume} here, and again in outlook???



Overall***, we think the model framework from Section~\ref{sec:generalmodel}
allows to use the full expressive power of imprecise probability %based inference
in a very elegant way, by giving a realistic description and treatment
of model uncertainty, and thus, as expressed in Section~\ref{sec:objections},
allowing to do away with heroic model assumptions, and their spuriously precise inferences they entail*** nach sich ziehen***.
%emphasise modeling opportunities, and realistic description and treatment
%of model uncertainty as in \ref{sec:objections}

We also think that there is still further potential to models based on sets of conjugate priors.
As described in the Outlook below***,
further advances are within reach 
that, in addition to the properties \ref{enum:n0vsn}--\ref{enum:deltay} and \pdc\ sensitivity
(see Section~\ref{sec:gbicp-properties-criteria}),
allow for more precise posterior inferences when prior and data coincide especially well.%
\footnote{We already mentioned this this modelling goal in Section~\ref{sec:insights},
where we spoke of \emph{strong prior-data agreement}.}
%further refinements are within reach (see outlook below).



\section{Outlook***}
\label{sec:concluding-outlook}

***avenues for further research***

Here, we come back to the ideas discussed in several concluding sections
(\ref{sec:6-gw-071216}, \ref{sec:insights}, \ref{sec:discussion-festschrift}),
subsuming them in three groups

***\textbf{first: model as is, use for\dots }

***model could be vey useful in statistical surveillance, cite IESS:surveillance?***,
as mentioned in footnote~\ref{foot:sequential}, page~\pageref{foot:sequential}),
write here some more: optimal stopping rules, outbreak detection is usually(?)
framed as a testing problem (will new data refute the current model?),
but can also be seen similar to \pdc\ (is new data in conflict with current (prior) model?).
it might be interesting to see if rectangular sets or other shapes are good for this.  

***use regression framework from Section~\ref{sec:cccp} in situations prone to \pdc.

***take only pair of $\nz$ values? behavioural implications?


***\textbf{then: extend the model, but keep GBR framework}

***extend to coarse or imprecise data, which is a very important field:
do not idealise also the data (we stopped idealising the prior already)!

***from \textcite[4.4]{itip-statinf}
Most models in generalised Bayesian inference rely on a precise sampling model,
but the general framework also allows for imprecise sampling models \parencite[see, in particular,][\S 8.5]{1991:walley},
also discussed under the notion of \emph{likelihood robustness} \parencite[e.g.,][]{2000:shyamalkumar} in the robust Bayesian framework.


***from isipta11 concluding:
\begin{small}
For a deeper understanding of prior-data conflict, it may also be helpful
to extend our methods to coarse data, in an analogous way to \textcite{2007:utkinaugustin} and
\textcite{2009:Troffaes:Coolen}, and to look at other model classes of prior distributions, most
notably at contamination neighbourhoods. Of particular interest here may
be to combine both types of prior models, considering contamination
neighbourhoods of our exponential family based-models with sets of
parameters, as developed in the Neyman-Pearson setting by
\textcite[\S~5]{2002:augustin}.
\end{small}

***new weakly informative prior models / prior near-ignorance models by Mangili \& Benavoli (ISIPTA'13) better in 3.2??***


***boatshape stuff: cater also for strong prior-data agreement***

as mentioned in ***isipta'11***, shape of $\MZ$ has a crucial influence on inferences,
and it might be very difficult to elicit a set \emph{shape} and ascertain*** the consequences of a certain shape.

although the updating of the canonical parameters \eqref{eq:canonicalupdate},
i.e., the weighted average update step for $\yz$, and the increment step for $\nz$,
seems very intuitive (and is central to the behaviour of the model,
see the list of inference properties in Section~\ref{sec:gbicp-properties-criteria}),
the shape change of $\MZ$ to $\MN$ through this updating and its effects on posterior inferences are difficult to grasp.

although update of a single coordinate $(\nz,\yz)$ is a simple shift,
this shift is different for different coordinates.

as shape change is so problematic for understanding,
is there a different parametrisation such that shape remains constant,
i.e., not only single coordinates shift, but also entire shapes
(same shift for all coordinates)?

yes, there is, see preliminary results by Mik Bickis (``personal communication'' if no citable work?***).
there, update step for $\nz$ is the same, and shift for parameter replacing $\yz$ is the same for all. 

describe some more?

rays of constant expectation (i.e., $\approx \yn$)

this parametrisation allows to tailor shapes for desired inference behaviour much easier.
indeed, GW and FC have thought of a shape that has \pdc\ sensitivity,
and gives `bonus precision' if prior and data agree especially well,
as mentioned a desirable property in Section~\ref{sec:insights}
and in the discussion of GBR in Section~\ref{sec:updating}

very appealing first results for Beta-Binomial model and Normal-Normal model,
hope for a joint publication of GW, FC and MB explaining this in detail.


\textbf{finally, general thoughts about updating, learning, etc}

considerations in Sectione~\ref{sec:insights}:
prior and data in conflict is of less value than the sum of both:
$\nz$ negative?


elaborate thoughts in Section~\ref{sec:updating} again,
see also comments at end of Section~\ref{sec:6-gw-071216},

dual role of $\nz$? (Section~\ref{sec:ibbm-resume})

belief revision versus updating?

questions on (data) robustness of IP models, see isipta13 paper by Marco


