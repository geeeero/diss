\chapter{Concluding Remarks}
\label{cha:concluding}

We will conclude this thesis with a short summary of, and discussion of the central achievments in, this thesis
in Sections~\ref{sec:concluding-summary} and \ref{sec:concluding-discussion}, respectively.
In Section~\ref{sec:concluding-outlook}, we will sketch some opportunities for applications and avenues for further research.


\section{Summary***}
\label{sec:concluding-summary}

% SUMMARY
%\subsection{Summary of Contents}

After a detailed motivational example and application in Section~\ref{sec:commoncause},
we presented some theoretical foundations of imprecise or interval probability in Section~\ref{cha:gbi},
with a focus on the approaches leading the way towards a generalisation of Bayesian inference.
There, we also gave some general motives for using imprecise probability methods,
afterwards focussing again on the Bayesian setting, where a foundational motive%
\footnote{The Bayesian approach to inference
requires an unattainable precision in prior assessments
for the foundational arguments for its use (e.g., coherence) to be valid
(see Section~\ref{sec:motivation:bayesian-foundational}).}
and two specific motives were described:
the case of weakly, or (near-) non-informative priors,
and the issue of \emph{prior-data conflict} (Section~\ref{sec:motivation:pdc}),
where strong prior beliefs are in conflict with trusted data,
but data is too sparse to overrule prior beliefs.

After a short view on some approaches in alternative to imprecise probability,
we discussed a general framework for imprecise Bayesian inference
based on sets of conjugate priors in Section~\ref{sec:generalmodel},
this framework serving as a bracket for most of the models ***discussed / investigated*** in this thesis.
%a number of contributions discussed in this thesis.
%the framework serving as a bracket for the remainder of the topics discussed in the thesis.
Some general results regarding inference properties of models
in this framework are presented (we will comment on these properties in the discussion below),
and the two issues of weakly informative priors and prior-data conflict,
problematic in usual Bayesian inference,
can now be considered as modelling opportunities of generalised Bayesian inference:
\begin{enumerate}[(i)]
\item Imprecise Bayesian methods allow to model weak prior information
more adequately than the so-called non-informative priors usually employed 
(see item~\ref{enum:noninformative} on page~\pageref{enum:noninformative}, and Section~\ref{sec:idm-and-near-ignorance}).
\item As a common class of Bayesian inference procedures
(those based on the canonical conjugates described in Section~\ref{sec:regularconjugates})
in our view deals very badly with \pdc\ %this issue of sensitivity to the prior
(see Section~\ref{sec:iid} for some basic examples, and
Section~\ref{sec:scp} and \ref{sec:cccp} for the case of Bayesian linear regression),
we developed imprecise probability methods that overcome this deficiency,
by mapping ambiguity in posterior inferences now also in dependence of \pdc\ 
(see Sections~\ref{sec:pdc-sensitivity}, and specifically \ref{sec:jstp}).
In these models, a conflict of prior beliefs and data
leads to larger posterior parameter sets $\PN$
that ***bedingen, fuehren zu* more cautious inferences results,
%ultimately giving more cautious inferences result,
the model thus being cautious if, and only if, caution is needed.
\end{enumerate}
%\pdc\ sensitivity: cautious inferences if, and only if, caution is needed
Section~\ref{sec:luck} then presents a software implementation
of the model framework developed in Section~\ref{sec:jstp}.

Finally, Section~\ref{sec:isipta11} presents attempts to further refine
inference behaviour in the presence of prior-data conflict.
In a first approach, the models discussed in Sections~\ref{sec:generalmodel} and \ref{sec:jstp}
are generalised by tayloring the prior parameter set $\PZ$ accordingly.
The second, fundamentally different, approach attains \pdc\ sensitivity
by combining \emph{inferences} based on two different priors,
an informative prior (expressing prior beliefs) and an uninformative prior (a near-ignorance prior),
through an imprecise weighting scheme.

As supplemental material, the Appendix (Chapter~\ref{cha:appendix}) contains
\begin{enumerate}[(i)]
\item a study of \pdc\ sensitivity in Bayesian linear regression,
presenting a simplified prior model that gives interesting insights into the updating step
for the regression parameters and offers opportunities for inferences based on sets of priors
(Section~\ref{sec:festschrift});
\item a short summary of a previous approach to Bayesian linear regression
with sets of priors which was published by \textcite{Walter2007a}, in Section~\ref{sec:isipta07};
\item some first technical results characterising a new prior parameter set shape,
described informally in Section~\ref{sec:concluding-outlook} below
(Section~\ref{sec:boatshape}).
\end{enumerate}


%DISCUSSION
\section{***Discussion}
\label{sec:concluding-discussion}

%discussion of issues in Bayesian inference and discussion on IP methods given in Section~\ref{sec:motivation} %Chapter~\ref{cha:gbi}.
%****reasons to use IP models:
%****issues in Bayesian inference: weakly informative priors, \pdc, or in general sensitivity to the prior
%****realistic modeling of partial information

As the model overview in \ref{sec:generalmodel} gives already a good discussion
of the imprecise probability models considered in this thesis,%
\footnote{General motives for the use of imprecise probability models
were discussed in Section~\ref{sec:motivation},
with a focus on motives from a Bayesian perspective in Section~\ref{sec:motivation:bayesian}.
These motives can be subsumed as follows:
Generalised Bayesian models allow, in contrast to classical Bayesian models,
a realistic modeling of partial prior information,
and can account for model uncertainty in a preferable way.
Critique and possible alternatives were discussed in Section~\ref{sec:ip-alternatives}.}
we will try to emphasize some central points here only.
%summary, condensed here again, emphasising some points

The inference properties described in Section~\ref{sec:gbicp-properties-criteria}
for the generalised Bayesian model framework established in Section~\ref{sec:basicsetting}
are quite remarkable,***
and illustrate the ability of imprecise probability models to realistically model partial information,
in a variety of situations.
In these models, ambiguity in prior specifications influences ambiguity in posterior inferences
in a natural and comprehensible way.
%write again about general model framework (Section~\ref{sec:generalmodel}),
%inference properties etc.:

Regarding the further model criterion `\pdc\ sensitivity',
central to this thesis,
we concluded that the shape of the prior parameter set $\PZ$
crucially influences model behaviour, 
and noted at the end of Section~\ref{sec:pdc-sensitivity} that
there is a clear trade-off between easy handling of $\PZ$ or $\PN$, and the desired model property.
Models with fixed $\nz$, like the IDM under prior information and the model by \textcite{2005:quaeghebeurcooman},
are very easy to handle, with $\nz$, $\yzl$ and $\yzu$ as the only parameters to elicit,
and $\PN$ being characterised again by the three values $\nn$, $\ynl$ and $\ynu$.
However, these models are insensitve to \pdc,
and the model with $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$
presented in Section~\ref{sec:4-gw-071216}
mitigates this deficiency, but at the cost of a more complex shape of $\PN$,
not being a cartesian product of $[\nnl, \nnu]$ and $[\ynl, \ynu]$ anymore.
The more refined behaviour discussed in Section~\ref{sec:isipta11}
is achieved by a more complex choice of $\PZ$,
where the range of $\yz$ values depends on $\nz$
via the contour functions $\yzl(\nz)$ and $\yzu(\nz)$
(see Section~\ref{sec:othershapes}).

Continuing*** the discussion of parameter set shapes as in Section~\ref{sec:ibbm-resume},
the general framework in principle allows arbitrary shapes for the prior parameter set $\PZ$.
However, freely elicting this set shape
(e.g., by allowing for arbitrary contour functions $\yzl(\nz)$ and $\yzu(\nz)$)
from limited prior information might be very difficult.
If only specific inferences are of interest,
it is possible that only a few aspects of the shape are relevant,
such that elicitation of the entire shape might not be necessary.
Furthermore, the derivation of posterior inferences from
such complex prior shapes can be difficult as well.
%discussion of shapes as at the end of Section~\ref{sec:pdc-sensitivity} and \ref{sec:ibbm-resume}***

%These considerations of parameter set shapes notwithstanding, %Nevertheless, 
%For all these models, whether the shape of $\PZ$ turns more to the simple or to the complex end, 
In general, %these
models based on sets of canonical conjugate priors form, in our view,
a `sweet spot' in the realm of (imprecise) statistical models,
by allowing a sophisticated model behaviour
combined with easy elictitation and computation.
More verbosely***,
these models are distinguished by the following characteristics:
%This is ***backed / underlined*** by the following points: 

\begin{itemize}
\item They are relatively easy to elicit:
there is a simple and straightforward interpretation of the model parameters.
In general terms, the weighted average structure of the update step and the model behaviour resulting from it is relatively clear.
\item They are easy to apply in a variety of inference problems:
it is possible to construct conjugate priors as needed for the problem at hand,
often resulting in closed form solutions for many quantities of interest.
\item Nevertheless, they give reasonable and sophisticated inferences:
as discussed above, uncertainty is adequately mirrored by the size of $\MN$,
fulfilling the inference properties \ref{enum:n0vsn}--\ref{enum:deltay}
%criteria from Section~\ref{sec:gbicp-properties-criteria},
with the potential to implement \pdc\ sensitivity or near-noninformativeness,
as required by the inference task at hand.
\item They allow for flexible modelling, as
it is possible to choose or tweak the prior set shape according to inference needs,
where, however, the trade-off mentioned above should be ***kept in mind***.
Specifically, we think the model presented in Section~\ref{sec:4-gw-071216}
($\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$)%
\footnote{This kind of prior set shape was also employed in the motivational example
from Section~\ref{sec:commoncause}, where we argued for an interval-valued $\nz$
at the end of Section~\ref{sec:fixedlearningparameter},
and demonstrated the merits of such a parameter shape for this specific situation of \pdc\
in Section~\ref{sec:intervallearningparameter}.}
%***example in Section~\ref{sec:commoncause}: special case of \pdc\ (zero counts),
constitues a sensible compromise between model complexity and inference properties.
\end{itemize}



***comparison with other approaches in Section~\ref{sec:alternatives}***

***study by ***krautenbacher*** shows that elicitation of $\PZ$ can be difficult,
compares with \textcite{2005:whitcomb}, similar results, see Section~\ref{sec:alternatives}****

***more studies for comparison with the model from \ref{sec:generalmodel} to be done,
also with hierarchical models (Section~\ref{sec:hierarchical})?***

***double role $\nz$



Overall***, we think the model framework from Section~\ref{sec:generalmodel}
allows to use the full expressive power of imprecise probability %based inference
in a very elegant way, by giving a realistic description and treatment
of model uncertainty, and thus, as expressed in Section~\ref{sec:objections},
allowing to do away with heroic model assumptions, and their spuriously precise inferences they entail*** nach sich ziehen***.
%emphasise modeling opportunities, and realistic description and treatment
%of model uncertainty as in \ref{sec:objections}

We also think that there is still further potential to models based on sets of conjugate priors.
As described in the Outlook below***,
further advances are within reach 
that, in addition to the properties \ref{enum:n0vsn}--\ref{enum:deltay} and \pdc\ sensitivity
(see Section~\ref{sec:gbicp-properties-criteria}),
allow for relatively precise posterior inferences when prior and data coincide especially well.
%further refinements are within reach (see outlook below).



\section{Outlook***}
\label{sec:concluding-outlook}

***avenues for further research***

***\textbf{first: model as is, use for\dots }

***model could be vey useful in statistical surveillance, cite IESS:surveillance?***,
as mentioned in footnote~\ref{foot:sequential}, page~\pageref{foot:sequential}),
write here some more: optimal stopping rules, outbreak detection is usually(?)
framed as a testing problem (will new data refute the current model?),
but can also be seen similar to \pdc\ (is new data in conflict with current (prior) model?).
it might be interesting to see if rectangular sets or other shapes are good for this.  

***use regression framework from Section~\ref{sec:cccp} in situations prone to \pdc.


***\textbf{then: extend the model, but keep GBR framework}

***extend to coarse or imprecise data, which is a very important field:
do not idealise also the data (we stopped idealising the prior already)!

from isipta11 concluding:
\begin{small}
For a deeper understanding of prior-data conflict, it may also be helpful
to extend our methods to coarse data, in an analogous way to \textcite{2007:utkinaugustin} and
\textcite{2009:Troffaes:Coolen}, and to look at other model classes of prior distributions, most
notably at contamination neighbourhoods. Of particular interest here may
be to combine both types of prior models, considering contamination
neighbourhoods of our exponential family based-models with sets of
parameters, as developed in the Neyman-Pearson setting by
\textcite[\S~5]{2002:augustin}.
\end{small}

***new weakly informative prior models / prior near-ignorance models by Mangili \& Benavoli (ISIPTA'13)


***boatshape stuff: cater also for strong prior-data agreement***

as mentioned in ***isipta'11***, shape of $\MZ$ has a crucial influence on inferences,
and it might be very difficult to elicit a set \emph{shape} and ascertain*** the consequences of a certain shape.

although the updating of the canonical parameters \eqref{eq:canonicalupdate},
i.e., the weighted average update step for $\yz$, and the increment step for $\nz$,
seems very intuitive (and is central to the behaviour of the model,
see the list of inference properties in Section~\ref{sec:gbicp-properties-criteria}),
the shape change of $\MZ$ to $\MN$ through this updating and its effects on posterior inferences are difficult to grasp.

although update of a single coordinate $(\nz,\yz)$ is a simple shift,
this shift is different for different coordinates.

as shape change is so problematic for understanding,
is there a different parametrisation such that shape remains constant,
i.e., not only single coordinates shift, but also entire shapes
(same shift for all coordinates)?

yes, there is, see preliminary results by Mik Bickis (``personal communication'' if no citable work?***).
there, update step for $\nz$ is the same, and shift for parameter replacing $\yz$ is the same for all. 

describe some more?

rays of constant expectation (i.e., $\approx \yn$)

this parametrisation allows to tailor shapes for desired inference behaviour much easier.
indeed, GW and FC have thought of a shape that has \pdc\ sensitivity,
and gives `bonus precision' if prior and data agree especially well,
as mentioned a desirable property in Section~\ref{sec:insights}
and in the discussion of GBR in Section~\ref{sec:updating}

very appealing first results for Beta-Binomial model and Normal-Normal model,
hope for a joint publication of GW, FC and MB explaining this in detail.


\textbf{finally, general thoughts about updating, learning, etc}

elaborate thoughts in Section~\ref{sec:updating} again,
see also comments at end of Section~\ref{sec:6-gw-071216},

dual role of $\nz$? (Section~\ref{sec:ibbm-resume})

belief revision versus updating?

questions on (data) robustness of IP models, see isipta13 paper by Marco


