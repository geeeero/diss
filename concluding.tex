\chapter{Concluding Remarks}
\label{cha:concluding}

We will conclude this thesis with a short summary of, and discussion of the central achievments in, this thesis
in Sections~\ref{sec:concluding-summary} and \ref{sec:concluding-discussion}, respectively.
In Section~\ref{sec:concluding-outlook}, we will sketch some opportunities for applications and avenues for further research.


\section{Summary***}
\label{sec:concluding-summary}

% SUMMARY
%\subsection{Summary of Contents}

After a detailed motivational example and application in Section~\ref{sec:commoncause},
we presented some theoretical foundations of imprecise or interval probability in Section~\ref{cha:gbi},
with a focus on the approaches leading the way towards a generalisation of Bayesian inference.
There, we also gave some general motives for using imprecise probability methods,
afterwards focussing again on the Bayesian setting, where a foundational motive%
\footnote{The Bayesian approach to inference
requires an unattainable precision in prior assessments
for the foundational arguments for its use (e.g., coherence) to be valid
(see Section~\ref{sec:motivation:bayesian-foundational}).}
and two specific motives were described:
the case of weakly, or (near-) non-informative priors,
and the issue of \emph{prior-data conflict} (Section~\ref{sec:motivation:pdc}),
where strong prior beliefs are in conflict with trusted data,
but data is too sparse to overrule prior beliefs.

After a short view on some approaches in alternative to imprecise probability in Section~\ref{sec:ip-alternatives},
we discussed a general framework for imprecise Bayesian inference
based on sets of conjugate priors in Section~\ref{sec:generalmodel},
this framework serving as a bracket and reference point for most of the models investigated in this thesis.
%a number of contributions discussed in this thesis.
%the framework serving as a bracket for the remainder of the topics discussed in the thesis.
Some general results regarding inference properties of models
in this framework were presented (we will comment on these properties in the discussion below),
and the two issues of weakly informative priors and prior-data conflict,
problematic in usual Bayesian inference,
could subsequently be considered as modelling opportunities of generalised Bayesian inference:
\begin{enumerate}[(i)]
\item Imprecise Bayesian methods allow to model weak prior information
more adequately than the so-called non-informative priors usually employed 
(see item~\ref{enum:noninformative} on page~\pageref{enum:noninformative}, and Section~\ref{sec:idm-and-near-ignorance}).
\item As a common class of Bayesian inference procedures
(those based on the canonical conjugates described in Section~\ref{sec:regularconjugates})
in our view deals with \pdc\ in an unfavourable way %this issue of sensitivity to the prior
(see Section~\ref{sec:iid} for some basic examples, and
Section~\ref{sec:scp} and \ref{sec:cccp} for the case of Bayesian linear regression),
we developed imprecise probability methods that overcome this deficiency,
by mapping ambiguity in posterior inferences now also in dependence of \pdc\ 
(see Sections~\ref{sec:pdc-sensitivity}, and specifically \ref{sec:jstp}).
In these models, a conflict of prior beliefs and data
entails larger posterior parameter sets $\PN$,
leading to cautious inferences if, and only if, caution is needed.
\end{enumerate}

Section~\ref{sec:alternatives} described some alternative models based on sets of priors,
and discussed their inference properties in comparison to the results in Section~\ref{sec:generalmodel}.
Section~\ref{sec:jstp} then motivated, developed, and illustrated
a central model type from Section~\ref{sec:generalmodel} in more detail;
afterwards, Section~\ref{sec:luck} briefly described a software implementation
of the model framework developed in Section~\ref{sec:jstp}.

Finally, Section~\ref{sec:isipta11} presented attempts to further refine
inference behaviour in the presence of prior-data conflict.
In a first approach, the model discussed in Section~\ref{sec:jstp} %Sections~\ref{sec:generalmodel} and \ref{sec:jstp}
was generalised by further tayloring the prior parameter set $\PZ$. %accordingly.
The second, fundamentally different, approach attained \pdc\ sensitivity
by combining \emph{inferences} based on two different priors,
an informative prior (expressing prior beliefs) and an uninformative prior (a near-ignorance prior),
through an imprecise weighting scheme.

As supplemental material, the Appendix (Chapter~\ref{cha:appendix}) contains
\begin{enumerate}[(i)]
\item a study of \pdc\ sensitivity in Bayesian linear regression,
presenting a simplified prior model that gives interesting insights into the updating step
for the regression parameters and offers opportunities for inferences based on sets of priors
(Section~\ref{sec:festschrift});
\item ***a short summary of a previous approach to Bayesian linear regression
with sets of priors which was published by \textcite{Walter2007a}, in Section~\ref{sec:isipta07};
\item some first technical results characterising a new prior parameter set shape,
described informally in Section~\ref{sec:concluding-outlook} below
(Section~\ref{sec:boatshape}).
\end{enumerate}


%DISCUSSION
\section{Discussion***}
\label{sec:concluding-discussion}

%discussion of issues in Bayesian inference and discussion on IP methods given in Section~\ref{sec:motivation} %Chapter~\ref{cha:gbi}.
%****reasons to use IP models:
%****issues in Bayesian inference: weakly informative priors, \pdc, or in general sensitivity to the prior
%****realistic modeling of partial information

As the model overview in Section~\ref{sec:generalmodel} gives already a good discussion
of the imprecise probability models considered in this thesis,%
\footnote{General motives for the use of imprecise probability models
were discussed in Section~\ref{sec:motivation},
with a focus on motives from a Bayesian perspective in Section~\ref{sec:motivation:bayesian}.
These motives can be subsumed as follows:
Generalised Bayesian models allow, in contrast to classical Bayesian models,
a realistic modeling of partial prior information,
and can account for model uncertainty in a preferable way.
Critique on and possible alternatives to Generalised Bayesian inference models
were discussed in Section~\ref{sec:ip-alternatives},
while alternatives to the models covered in Section~\ref{sec:generalmodel} are discussed in Section~\ref{sec:alternatives}.}
we will try to emphasize some central points here only.
%summary, condensed here again, emphasising some points

The inference properties described in Section~\ref{sec:gbicp-properties-criteria}
for the generalised Bayesian model framework established in Section~\ref{sec:basicsetting}
are quite remarkable in their generality,***
and illustrate the ability of imprecise probability models to realistically model partial information
%in a variety of situations.
for a wide field of inference tasks.
In these models, ambiguity in prior specifications influences ambiguity in posterior inferences
in a natural and comprehensible way.%
\footnote{In contrast, as mentioned in Section~\ref{sec:alternatives:other}***,
in imprecise probability models for discrete parameter spaces,
it is for example difficult to judge the weight of a certain prior model as compared to the data.}
%write again about general model framework (Section~\ref{sec:generalmodel}),
%inference properties etc.:

Regarding the further model criterion `\pdc\ sensitivity',
a central concept in this thesis,
we concluded that the shape of the prior parameter set $\PZ$
crucially influences model behaviour, 
and noted at the end of Section~\ref{sec:pdc-sensitivity} that
there is a clear trade-off between easy handling of $\PZ$ or $\PN$, and the desired model property.
Models with fixed $\nz$, like the IDM under prior information and the model by \textcite{2005:quaeghebeurcooman},
are very easy to handle, with $\nz$, $\yzl$ and $\yzu$ as the only parameters to elicit,
and $\PN$ being characterised again by the three values $\nn$, $\ynl$ and $\ynu$.%
\footnote{For sake of a clarity,
we consider only one-dimensional main parameters $\yz$ in the discussion here and below,
although the model framework allows for multidimensional $\yz$,
as illustrated by the examples involving the Dirichlet-Multinomial model.}
However, these models are insensitve to \pdc,
and the model with $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$
presented in Section~\ref{sec:4-gw-071216}
mitigates this deficiency, but at the cost of a more complex shape of $\PN$,
not being a cartesian product of $[\nnl, \nnu]$ and $[\ynl, \ynu]$ anymore.
The more refined behaviour discussed in Section~\ref{sec:isipta11}
is achieved by a more complex choice of $\PZ$,
where the range of $\yz$ values depends on $\nz$
via the contour functions $\yzl(\nz)$ and $\yzu(\nz)$
(see Section~\ref{sec:othershapes}).

Revisiting and continuing*** the discussion of parameter set shapes in Section~\ref{sec:ibbm-resume},
the general framework in principle allows arbitrary shapes for the prior parameter set $\PZ$.
However, freely elicting this set shape
(e.g., by allowing for arbitrary contour functions $\yzl(\nz)$ and $\yzu(\nz)$)
from limited prior information might be very difficult.
If only specific inferences are of interest,
it is possible that only a few aspects of the shape are relevant,
such that elicitation of the entire shape might actually not be necessary.
Furthermore, the derivation of posterior inferences from
such complex prior shapes can be difficult as well.
%discussion of shapes as at the end of Section~\ref{sec:pdc-sensitivity} and \ref{sec:ibbm-resume}***

%These considerations of parameter set shapes notwithstanding, %Nevertheless, 
%For all these models, whether the shape of $\PZ$ turns more to the simple or to the complex end, 
In general, %these
models based on sets of canonical conjugate priors form, in our view,
a `sweet spot' in the realm of (imprecise) statistical models,
by allowing a sophisticated model behaviour
combined with easy elicitation and computation.
%More verbosely***,
In particular,
these models are distinguished by the following characteristics:
%This is ***backed / underlined*** by the following points: 

\begin{itemize}
\item They are relatively easy to elicit:
there is a simple and straightforward interpretation of the model parameters $\nz$ and $\yz$
as prior strength and main parameter, respectively (see Sections~\ref{sec:regularconjugates} and \ref{070517-sec2-1}).
In general terms, the weighted average structure of the update step \eqref{eq:canonicalupdate}
and the model behaviour resulting from it is relatively clear.
\item They are easy to apply in a variety of inference problems:
it is possible to construct conjugate priors as needed for the problem at hand,
often resulting in closed form solutions for many quantities of interest.
\item Nevertheless, they give reasonable and sophisticated inferences:
as discussed above, uncertainty is adequately mirrored by the size of $\MN$,
fulfilling the inference properties \ref{enum:n0vsn}--\ref{enum:deltay}
%criteria from Section~\ref{sec:gbicp-properties-criteria},
with the potential to implement \pdc\ sensitivity or near-noninformativeness,
as required by the inference task at hand.%
\footnote{We think this is a major advantage to the alternative models
discussed in Section~\ref{sec:alternatives},
especially to models based on the density ratio class.
As noted in Section~\ref{sec:alternatives:drc}, the magnitude of $\mathcal{M}_{l,u}$ %imprecision
does not decrease with $n$ in density ratio class models that are updated according to the Generalised Bayes' Rule.}
\item They allow for flexible modelling, as
it is possible to choose or tweak the prior set shape according to inference needs,
where, however, the trade-off mentioned above should be ***factored in. %kept in mind***.
Specifically, we think the model presented in Section~\ref{sec:4-gw-071216}
(with $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$)%
\footnote{This kind of prior set shape was also employed in the motivational example
from Section~\ref{sec:commoncause}, where we argued for an interval-valued $\nz$
at the end of Section~\ref{sec:fixedlearningparameter},
and demonstrated the merits of such a parameter shape for this specific situation of \pdc\
in Section~\ref{sec:intervallearningparameter}.}
%***example in Section~\ref{sec:commoncause}: special case of \pdc\ (zero counts),
constitutes a sensible compromise between model complexity and inference properties.
\end{itemize}

% rectangle ``somewhat peculiar'' from a behavioral point of view (Section~\ref{sec:resume}).
However, some doubts with respect to the rectangular shape of $\PZ$ are nevertheless warranted.
From a strictly behavioral point of view
(e.g., if $\MZ$ should express an expert's prior beliefs),
taking $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$,
is, as we mentioned in Sections~\ref{sec:pdc-sensitivity} and \ref{sec:ibbm-resume}, a somewhat peculiar choice,
as it entails that we assign the same interval for the main parameter $\yz$
over a range of prior strengths.

Instead, it might be more reasonable to assume that the expert is able to give
a more precise interval estimate for lower prior strengths,
while being more cautious in his prior assignments for higher prior strengths
by choosing a wider interval for $\yz$ at $\nzu$.%
\footnote{As described in Section~\ref{sec:ibbm-resume},
such a choice can also lead to a more `tolerant' behaviour in case of \pdc.
If, e.g., $\yzu(\nzl) < \yzu(\nzu)$ was chosen and $\ttau(\x) > \yzu(\nzu)$,
the higher weight $\nzu$ for $\yzu(\nzu)$ in the update step
would move $\ynu(\nnu)$ more slowly towards $\ttau(\x)$ as compared to $\ynu(\nnl)$,
but for moderate degrees of \pdc, the faster movement of $\ynu(\nnl)$
would be offset by the `head start' of $\yzu(\nzu)$.
As the upper bound for $\yn$ is attained at $\ynu(\nnu)$
if $\ttau(\x) < \yzu(\nzu)$ (i.e., data is in agreement with the prior assignements),
the effect of extra imprecision would only appear as soon as
$\ynu(\nnl)$ `overtakes' $\ynu(\nnu)$ in the move towards $\ttau(\x)$, i.e. $\ynu(\nnl) > \ynu(\nnu)$.}

However, it seems equally reasonable to choose %$\yzu(\nzl) > \yzu(\nzu)$,
a shorter interval for $\yz$ at $\nzu$ than at $\nzl$,
by the considerations described at the beginning of Section~\ref{sec:othershapes},
involving a thought experiment or `pre-posterior thinking':
%***anteater****
If we designate bounds for $\yn$ constant in $\nn$ and a range of $\nn$ values
%where the corresponding posterior parameter set $\PN$
to express what we want to learn from certain hypothetical data,
the corresponding $\PZ$ derived by doing a `backwards update' has,
due to the update mechanism \eqref{eq:canonicalupdate},
a wider range for low values of $\nz$, and a narrower range for high $\nz$ values.%
\footnote{For this model, to elicit the pre-posterior bounds, certain hypothetical data have to be chosen.
Section~\ref{sec:othershapes} touches only very briefly on these hypothetical data
(that are a part of the model parameters) and how to choose them,
and there is ample opportunity for research here.
One could also further modify the model by allowing to choose
the `pre-posterior $\PN$' more freely, with bounds for $\yn$ not constant in $\nn$.}
For an actual sample size smaller than the hypothetical one,
this `anteater' shape is more `tolerant' as the rectangular shape.%
\footnote{The `anteater' shapes are somewhat similar to shapes that would result
if we required, for information on the main prior parameter $\yz$ symmetrical around $0$,
$|\nz \cdot \yz|$ to be constant.
Such a shape was proposed by \textcite{2012:benavolizaffalon}
for canonical priors to one-parameter exponential family likelihoods,
with the aim to generate near-noninformative prior sets $\MZ$.}

A suggestion for another more sophisticated shape of $\PZ$,
with the aim to allow for extra precision if prior and data coincide especially well,
is discussed in the outlook below.
%***short note on eggplant shapes in talk? (preliminary results)

% study by krautenbacher shows that elicitation of $\PZ$ can be difficult,
% compares with \textcite{2005:whitcomb}, similar results, see Section~\ref{sec:alternatives}
% should be cautious with skewed distributions, as expectation interval may be misleading (look at HD intervals, too!)
% remember that the canonical conjugate may not behave as expected
However, the favourable properties given in general terms in Section~\ref{sec:generalmodel}***
should not obscure the restrictions that are imposed by the canonical conjugate priors
the framework is based on. 

The study by \textcite[see the discussion in Section~\ref{sec:alternatives:other}***]{2011:krautenbacher}
reminds us that fitting a prior parameter set $\PZ$
to available prior information can be non-trivial,
and that also the generally well-understood conjugate distributions may exhibit
unintuitive inference behaviours when focussing on prior and posterior expectations only,
as the model framework tempts us to do.
It is thus advisable to not loose sight of the distributions in $\MZ$ and $\MN$ in their entirety, %as a whole,
e.g., by considering unions of highest density intervals for different credibility levels.

%can be mitigated by choosing $\MZ$ to be set of all convex mixtures.
The restrictions imposed by canonical conjugate priors can be mitigated,
or even completely overcome,
when choosing $\MZ$ not to contain parametric distributions only,
but to comprise all finite mixtures of parametric distributions,
i.e., considering $\MZ$ as the convex hull of the parametric distributions.%
\footnote{As noted in footnote~\ref{foot:denseinthespaceof}***, page~\pageref{foot:denseinthespaceof}***,
if the parametric distributions are normal distributions and $\PZ$ is large enough,
it can be assumed that $\MZ$ contains a very wide range of priors,
as mixtures of normal distributions are dense in the space of well-behaved probability distributions.***}
However, as mentioned in Section~\ref{sec:basicsetting},
then only inferences that are linear in the parametric posteriors are easy to obtain,
and the model may deliver very imprecise, or even vacuous, results
for nonlinear functions of $p(\vartheta\mid\nn,\yn)$ like, e.g., the posterior variance.

It seems that models based on the density ratio class,
the model framework most similar to this variant of the model framework of Section~\ref{sec:generalmodel}
(similar in that they both allow non-parametric priors in the set of prior distributions,
although the model is generated by use of parametric distributions),
do not have this issue of tractability of nonlinear inferences
(see the discussion of the model by \textcite[\S 4]{2011:rinderknecht} in Section~\ref{sec:alternatives:other}***);
while this seems a major strength of the model,
it is in our view offset by a fundamental weakness,
the lack of a clear mechanism by which imprecision in coherent posterior inferences
can be modelled in dependence of sample size.%
\footnote{As mentioned in Sections~\ref{sec:alternatives:drc} and \ref{sec:alternatives:other},
imprecision as measured by the magnitude of $\mathcal{M}_{l,u}$ is the same for any sample size $n$
if $\mathcal{M}_{l,u}$ is updated according to Bayes' Rule.
Only if the requirement of coherence, the fundation of the Generalised Bayes' Rule, is dropped,
density ratio class models are possible that allow for imprecision to depend on sample size $n$.
See also footnote~\ref{foot:coolenplusrinderknecht} on page~\pageref{foot:coolenplusrinderknecht},
where we suggested a model based on a combination of ideas from \textcite{1993:coolen} and \textcite{2011:rinderknecht:diss}.}

% double role $\nz$, see Section~\ref{sec:ibbm-resume} here, and again in outlook???
Another, more fundamental, handicap of the model framework from Section~\ref{sec:generalmodel}
is the double role of $\nz$ as mentioned at the end of Section~\ref{sec:ibbm-resume}.
There, the issue is framed in terms of the imprecise Beta-Binomial model,
but it is actually valid for the general case of updating
in imprecise probability models based on canonical conjuagte priors:
On the one hand, $\nz$ governs the weighting of prior information $\yz$ with
respect to the data $\ttau(\x)$;
the larger $\nz$, the more the values of $\ynl$ and $\ynu$ are dominated by $\yzl$ and $\yzu$.
On the other hand, $\nz$ governs also the degree of posterior imprecision:
the larger $\nz$, the larger c.p.\ ${\rm MPI}\un = \ynu - \ynl$.
A larger $\nz$ thus leads to more imprecise posterior inferences,
although a high weight on the supplied prior information
should boost the trust in posterior inferences if $\ttau(\x) \in [\yzl, \yzu]$,
i.e., if prior beliefs turned out to be appropriate.

In Section~\ref{sec:weightedinf},
an approach separating these two roles of $\nz$ was developed,
by considering two separate models,
an uninformative and an informative model,%
\footnote{These two models could also be denoted by `cautious' and `bold', respectively,
the cautious model trying to use only a minimal amount of information, %presuppose at least as possible**,
whith the bold model going for more daring assumptions.}
with individual levels of precision induced by different choices of $\nz$. 
Inference intervals based on these two models were then combined into a definite*** inference interval
using an imprecise weight.

This model of \emph{weighted inference} is a very general approach,
being applicable for a wide variety of inference tasks and accomodating all sorts of models that provide interval-valued inferences.
Combining favourable properties 
for the inference situation discussed in Section~\ref{sec:isipta11}
with feasible elicitation and handling (see Section~\ref{wi-prop}),
it is, however, going beyond the model framework of Section~\ref{sec:generalmodel},
and thus the general properties described in Section~\ref{sec:gbicp-properties-criteria}
cannot be guaranteed for it.***

\medskip

%emphasise modeling opportunities, and realistic description and treatment
%of model uncertainty as in \ref{sec:objections}
In summary, we think the model framework from Section~\ref{sec:generalmodel}
exploits the full expressive power of imprecise probability %based inference
in a very elegant way, by allowing a realistic description and treatment
of model uncertainty, and thus, as expressed in Section~\ref{sec:objections},
enable us to avoid heroic model assumptions, and the spuriously precise inferences they entail. 
%allowing to do away with heroic model assumptions, and the spuriously precise inferences they entail. 

We also think that there is still further potential to models based on sets of conjugate priors.
As described in the Outlook below***,
further advances are within reach 
that, in addition to the properties \ref{enum:n0vsn}--\ref{enum:deltay} and \pdc\ sensitivity
(see Section~\ref{sec:gbicp-properties-criteria}),
allow for more precise posterior inferences when prior and data coincide especially well.%
\footnote{We already mentioned this this modelling goal in Section~\ref{sec:insights},
where we spoke of \emph{strong prior-data agreement}.}
%further refinements are within reach (see outlook below).



\section{Outlook***}
\label{sec:concluding-outlook}

%***avenues for further research***
%Here, we come back to the ideas 
Here, we will summarise central ideas discussed in several concluding sections
(\ref{sec:6-gw-071216}, \ref{sec:insights}, \ref{sec:discussion-festschrift}),
carrying some of them further, presenting %giving an ***overview of** %in order to describe 
opportunities for applications and potential avenues for further research.
In particular, an idea for modelling of \emph{strong prior-data agreement} will be explained in more detail.

We will subsume our ideas for further research and developments in three groups:
First, we will consider some potential applications and areas of study for the
currently existing models in the framework from Section~\ref{sec:generalmodel}.
Then, we will sketch some ideas to extend the presently available models,
including a discussion of a novel parameter set shape that allows to cater for
\emph{strong prior-data agreement} (see the technical details in Section~\ref{sec:boatshape}***).
These further developments still follow the lines %are **still a part of the framework***
of generalised Bayesian inference using sets of priors as described in Sections~\ref{sec:gbr} and \ref{sec:imprecisebayes},
respecting*** coherence by updating the set of priors via the Generalised Bayes' Rule.
In the last part of this outlook, we will look instead beyond this framework
of coherent Bayesian inference, by discussing some general thoughts
about updating and learning in the context of statistical inference.

\medskip

%***\textbf{first: model as is, study and use for\dots }
%***comparison with other approaches in Section~\ref{sec:alternatives}, especially drc***
As seen in Section~\ref{sec:alternatives}, there are a number of alternative models
for statistical inference using sets of priors.
Besides the study by \textcite{2011:krautenbacher},
no studies comparing inferences from the models described in Setions~\ref{sec:jstp} and \ref{sec:isipta11}
with those based on the alternative models named in Section~\ref{sec:alternatives}
(most importantly, models based on the density ratio class)
in detail have been conducted.
%***more studies for comparisons with the model from \ref{sec:generalmodel} to be done,
%e.g., with hierarchical models (Section~\ref{sec:hierarchical})?***
Further insight could also be gained from a comparison to the hierarchical model
developed by \textcite{2008:cattaneo} (see Section~\ref{sec:hierarchical}).

There are of course many opportunities for application of the models described in this thesis,
and we will mention just two interesting general cases here.

%***use regression framework from Section~\ref{sec:cccp} in situations prone to \pdc.
The canonical conjugate prior for Bayesian linear regression constructed in Section~\ref{sec:cccp}
could be used for an imprecise regression analysis based on sets of priors.
Although being probably the most important concept in modern statistical inference,
in the imprecise probability literature,
so far only very few contributions*** considering regression analysis have been published,%
\footnote{Among the few exceptions are \textcite{Walter2007a}, \textcite{2011:utkin}, and \textcite{2012:cattaneo-wiencierz}.}
so work in this direction could contribute to a major step towards a wider application
of imprecise probability models in statistical practice.% 
\footnote{In contrast, classification models are a thriving subject area in imprecise probability.
For a recent overview see, e.g., \textcite{itip-classification}.}
With priors based on $\PZ$ of type~(\ref{enum:rectangular}),
the model could offer favourable inference properties in situations that are prone to \pdc\
(see the discussion Section~\ref{sec:discussion-festschrift}).

%***model could be vey useful in statistical surveillance, cite IESS:surveillance?***,
Another potentially fruitful area of application is models for \emph{statistical surveillance}
\parencite[see, e.g.,][]{2011:IESS-surveillance}.
Here, the aim is to monitor a data-generating process over time
to detect changes in the process as early as possible,
without generating too many `false alarms'.
A typical example is disease monitoring,
where the number of cases of a certain infectious disease reported by clinicians
is continuously monitored on the national level,
with the aim to detect epidemic outbreaks in their early stages.
Such outbreak detection is usually analysed using likelihood ratios,
comparing new observations with a model derived from previous observations. 
As mentioned in footnote~\ref{foot:sequential}, page~\pageref{foot:sequential},
we could also consider this problem in terms of \pdc:
if a new batch of observations does not fit our current model
(a set $\MN$ subsuming prior information and previous data),
the posterior based on $\MN$ resonates this by increased imprecision,
triggering the alarm.
We would expect rectangular parameter set shapes to perform quite well in such a task,
but also also other set shapes could prove useful here.

\medskip

%***\textbf{then: extend the model, but keep GBR framework}
Now, we will present a number of ideas for further developments within the framework
described in Section~\ref{sec:basicsetting}, i.e.,
involving sets of priors defined via canonical conjugates
that are updated via the Generalised Bayes' Rule.

As a first important development,
the models discussed in this thesis rely on a precise sampling model,
%from \textcite[4.4]{itip-statinf}
but the generalised Bayesian inference framework also allows for imprecise sampling models
\parencite[see, in particular,][\S 8.5]{1991:walley},
also discussed under the notion of \emph{likelihood robustness} \parencite[e.g.,][]{2000:shyamalkumar} in the robust Bayesian framework.
This could be done, as mentioned in Section~\ref{sec:insights},
in an analogous way to \textcite{2007:utkinaugustin} and \textcite{2009:Troffaes:Coolen},
allowing to consider coarse or imprecise data adequately in the model,
not idealising the data in a similar way as we ceased to idealise prior information by allowing for imprecision.
%The detailed consequences of this development remain to be studied.

%new weakly informative prior models / prior near-ignorance models by Mangili \& Benavoli (ISIPTA'13) better in 3.2??

%***study multidimensional stuff in more detail: definition of pdc now dimension-wise, other shapes?
%***in general, how to elicit more complex shapes, especially difficult for multidimensional $\yz$
Another important area for further developments regards***concerns the case of multidimensional $\yz$.
Elicitation of such high-dimensional parameter sets $\PZ$ poses interesting challenges.
The simple, `hyper-rectangle' set suggested in Equation~\eqref{eq:hyperparams:boxmodel} (Section~\ref{sec:imprecise-alpha})
might not be adequate in all circumstances, but elicitation of more general sets $\YZ \times [\nzl, \nzu]$,
or even arbitrary subsets of $\Y \times \posreals$ can be very difficult.
Related to this, for $\PZ$ as in Equation~\eqref{eq:hyperparams:boxmodel},
\pdc\ is mirrored by increased imprecision for each dimension separately
(see Example~\ref{ex:jstp-8}, Figure~\ref{fig:idm-nvar-nopdc}).
It is an open question if other prior set shapes entail
different consequences with respect to \pdc\ sensitivity. 
%***take only pair of $\nz$ values? behavioural implications?
A possible approach to simplify elicitation in high-dimensional cases
that could also be useful for lower-dimensional $\yz$ is to not consider
$\NZ = [\nzl, \nzu]$, but $\NZ = \{\nzl, \nzu\}$,
i.e., taking only a pair of $\nz$ values.
This would not only simplify elicitation, but also make computation of posterior inferences
probably more feasible.
However, as with the discussion of the rectangular shape in Section~\ref{sec:concluding-discussion} above,
the behavioural implications of such a choice of parameter set would have to be studied in detail.


***boatshape stuff: cater also for strong prior-data agreement***

as mentioned in ***isipta'11***, shape of $\MZ$ has a crucial influence on inferences,
and it might be very difficult to elicit a set \emph{shape} and ascertain*** the consequences of a certain shape.

although the updating of the canonical parameters \eqref{eq:canonicalupdate},
i.e., the weighted average update step for $\yz$, and the increment step for $\nz$,
seems very intuitive (and is central to the behaviour of the model,
see the list of inference properties in Section~\ref{sec:gbicp-properties-criteria}),
the shape change of $\MZ$ to $\MN$ through this updating and its effects on posterior inferences are difficult to grasp.

although update of a single coordinate $(\nz,\yz)$ is a simple shift,
this shift is different for different coordinates.

as shape change is so problematic for understanding,
is there a different parametrisation such that shape remains constant,
i.e., not only single coordinates shift, but also entire shapes
(same shift for all coordinates)?

yes, there is, see preliminary results by Mik Bickis (``personal communication'' if no citable work?***).
there, update step for $\nz$ is the same, and shift for parameter replacing $\yz$ is the same for all. 

describe some more?

rays of constant expectation (i.e., $\approx \yn$)

this parametrisation allows to tailor shapes for desired inference behaviour much easier.
indeed, GW and FC have thought of a shape that has \pdc\ sensitivity,
and gives `bonus precision' if prior and data agree especially well,
as mentioned a desirable property in Section~\ref{sec:insights}
and in the discussion of GBR in Section~\ref{sec:updating}

very appealing first results for Beta-Binomial model and Normal-Normal model,
hope for a joint publication of GW, FC and MB explaining this in detail.


\textbf{finally, general thoughts about updating, learning, etc}

construct cattaneo's hierarchical models (Section~\ref{sec:hierarchical})
based on the models discussed here

model suggestion in footnote~\ref{foot:coolenplusrinderknecht}, page~\pageref{foot:coolenplusrinderknecht}

GBR, see Section~\ref{sec:updating}?

considerations in Sectione~\ref{sec:insights}:
prior and data in conflict is of less value than the sum of both:
$\nz$ negative?


elaborate thoughts in Section~\ref{sec:updating} again,
see also comments at end of Section~\ref{sec:6-gw-071216},

dual role of $\nz$? (Section~\ref{sec:ibbm-resume})

belief revision versus updating?

Hampel: how can we *** new evidence?

questions on (data) robustness of IP models, see isipta13 paper by Marco


