\chapter{Concluding Remarks}
\label{cha:concluding}

We will conclude this thesis with a short summary of, and discussion of the central achievments in, this thesis
in Section~\ref{sec:concluding-summary}.
In Section~\ref{sec:concluding-outlook}, we will sketch some opportunities for applications and avenues for further research.

As supplemental material, the Appendix (Chapter~\ref{cha:appendix}) contains
\begin{enumerate}[(i)]
\item a study of \pdc\ sensitivity in Bayesian linear regression,
presenting a simplified prior model that gives interesting insights into the updating step
for the regression parameters and offers opportunities for inferences based on sets of priors
(Section~\ref{sec:festschrift});
\item a short summary of a previous approach to Bayesian linear regression
with sets of priors, published as \parencite{Walter2007a}, in Section~\ref{sec:isipta07};
\item some first technical results characterising a new prior parameter set shape $\PZ$,
described informally in Section~\ref{sec:concluding-outlook} below
(Section~\ref{sec:boatshape}).
\end{enumerate}


\section{Summary and Discussion***}
\label{sec:concluding-summary}

% SUMMARY
\subsection{Summary of Contents}

After a detailed motivational example and application in Section~\ref{sec:commoncause},
we presented the basic theory of imprecise or interval probability in Section~\ref{cha:gbi},
with a focus on the approaches leading the way towards a generalisation of Bayesian inference.
There, we also gave some general motives for using imprecise probability methods,
afterwards focussing again on the Bayesian setting, where a foundational motive%
\footnote{The Bayesian approach to inference
requires an unattainable precision in prior assessments
for the foundational arguments for its use (e.g., coherence) to be valid
(see Section~\ref{sec:motivation:bayesian-foundational}).}
and two specific motives were described:
the case of weakly, or (near-) non-informative priors,
and the issue of \emph{prior-data conflict} (Section~\ref{sec:motivation:pdc}),
where strong prior beliefs are in conflict with trusted data,
but data is too sparse to overrule prior beliefs.

After a short view on some approaches in alternative to imprecise probability,
we discussed a general framework for imprecise Bayesian inference
based on sets of conjugate priors in Section~\ref{sec:generalmodel},
this framework serving as a bracket for a number of contributions discussed in this thesis.
%the framework serving as a bracket for the remainder of the topics discussed in the thesis.
Some general results regarding inference properties of models
in this framework are presented (we will comment on these properties in the discussion below),
and the two issues of weakly informative priors and prior-data conflict
in usual Bayesian inference
can now be considered as modelling opportunities of generalised Bayesian inference:
\begin{enumerate}[(i)]
\item Imprecise Bayesian methods allow to model weak prior information
more adequately than the so-called non-informative priors usually employed 
(see item~\ref{enum:noninformative} on page~\pageref{enum:noninformative}, and Section~\ref{sec:idm-and-near-ignorance}).
\item As a common class of Bayesian inference procedures
(those based on the canonical conjugates described in Section~\ref{sec:regularconjugates})
in our view deals very badly with \pdc\ %this issue of sensitivity to the prior
(see Section~\ref{sec:iid} for some basic examples, and
Section~\ref{sec:scp} and \ref{sec:cccp} for the case of Bayesian linear regression),
we developed imprecise probability methods that overcome this deficiency
(see Sections~\ref{sec:pdc-sensitivity} and \ref{sec:jstp}),
by mapping ambiguity in posterior inferences due to prior-data conflict
via larger posterior parameter sets $\MN$,
ultimately resulting in more cautious inferences. % if, and only if, caution is needed.
\end{enumerate}
Section~\ref{sec:luck} then presents a software implementation
of the model framework developed in Section~\ref{sec:jstp},
and Section~\ref{sec:isipta11} presents attempts to further refine
inference behaviour in the presence of prior-data conflict.


****The handling of prior-data conflict

****reasons to use IP models:

****issues in Bayesian inference: weakly informative priors, \pdc, or in general sensitivity to the prior

****realistic modeling of partial information


%DISCUSSION
\subsection{Discussion}

The model overview in \ref{sec:generalmodel} gives already a good discussion
of the models considered in this thesis,
such that we will try to emphasize some central points here only.
%summary, condensed here again, emphasising some points

The inference properties described in Section~\ref{sec:gbicp-properties-criteria}
for the generalised Bayesian model framework established in Section~\ref{sec:basicsetting}
are quite remarkable,***
and illustrate the ability of imprecise probability models to realistically model partial information,
in a variety of situations.
In these models, ambiguity in prior specifications influences ambiguity in posterior inferences
in a natural and comprehensible way.
%write again about general model framework (Section~\ref{sec:generalmodel}),
%inference properties etc.:

Regarding further model ***anforderunge*** like pdc sensitivity,
there is a clear trade-off between easy handling of $\MZ$ or $\MN$, and desired model properties.

***explain this more

Nevertheless, 
models based on sets of canonical conjugate priors form, in our view,
a `sweet spot' in the realm of (imprecise) statistical models:
easy to elicit (simple and straightforward interpretation of parameters,
weigted average structure, model behaviour clear)
and to apply (may construct priors as needed, closed form solutions for many quantities of interest),
while giving reasonable inferences (uncertainty mirrored by size $\mathcal{M}$,
inference criteria from Section~\ref{sec:gbicp-properties-criteria}, especially \pdc\ sensitivity),
and allows flexible modelling (tweak prior sets according to needs),

further refinements are within reach (see outlook below).

\pdc\ sensitivity: cautious inferences if, and only if, caution is needed

example in Section~\ref{sec:commoncause}: special case of \pdc\ (zero counts),


discussion of shapes as at the end of Section~\ref{sec:pdc-sensitivity} and \ref{sec:ibbm-resume}***

emphasise modeling opportunities, and realistic description and treatment
of model uncertainty as in \ref{sec:objections}




***comparison with other approaches here or in Section~\ref{sec:alternatives}?***

study by ***krautenbacher*** shows that elicitation of $\MZ$ can be difficult,
compares with \textcite{2005:whitcomb}, similar results****

more studies for comparison with the model from \ref{sec:generalmodel} to be done,
also with hierarchical models (Section~\ref{sec:hierarchical})?***







\section{Outlook***}
\label{sec:concluding-outlook}

***avenues for further research***

***\textbf{first: model as is, use for\dots }

***model could be vey useful in statistical surveillance, cite IESS:surveillance?***,
as mentioned in footnote~\ref{foot:sequential}, page~\pageref{foot:sequential}),
write here some more: optimal stopping rules, outbreak detection is usually(?)
framed as a testing problem (will new data refute the current model?),
but can also be seen similar to \pdc\ (is new data in conflict with current (prior) model?).
it might be interesting to see if rectangular sets or other shapes are good for this.  

***use regression framework from Section~\ref{sec:cccp} in situations prone to \pdc.


***\textbf{then: extend the model, but keep GBR framework}

***extend to coarse or imprecise data, which is a very important field:
do not idealise also the data (we stopped idealising the prior already)!

from isipta11 concluding:
\begin{small}
For a deeper understanding of prior-data conflict, it may also be helpful
to extend our methods to coarse data, in an analogous way to \textcite{2007:utkinaugustin} and
\textcite{2009:Troffaes:Coolen}, and to look at other model classes of prior distributions, most
notably at contamination neighbourhoods. Of particular interest here may
be to combine both types of prior models, considering contamination
neighbourhoods of our exponential family based-models with sets of
parameters, as developed in the Neyman-Pearson setting by
\textcite[\S~5]{2002:augustin}.
\end{small}

***new weakly informative prior models / prior near-ignorance models by Mangili \& Benavoli (ISIPTA'13)


***boatshape stuff: cater also for strong prior-data agreement***

as mentioned in ***isipta'11***, shape of $\MZ$ has a crucial influence on inferences,
and it might be very difficult to elicit a set \emph{shape} and ascertain*** the consequences of a certain shape.

although the updating of the canonical parameters \eqref{eq:canonicalupdate},
i.e., the weighted average update step for $\yz$, and the increment step for $\nz$,
seems very intuitive (and is central to the behaviour of the model,
see the list of inference properties in Section~\ref{sec:gbicp-properties-criteria}),
the shape change of $\MZ$ to $\MN$ through this updating and its effects on posterior inferences are difficult to grasp.

although update of a single coordinate $(\nz,\yz)$ is a simple shift,
this shift is different for different coordinates.

as shape change is so problematic for understanding,
is there a different parametrisation such that shape remains constant,
i.e., not only single coordinates shift, but also entire shapes
(same shift for all coordinates)?

yes, there is, see preliminary results by Mik Bickis (``personal communication'' if no citable work?***).
there, update step for $\nz$ is the same, and shift for parameter replacing $\yz$ is the same for all. 

describe some more?

rays of constant expectation (i.e., $\approx \yn$)

this parametrisation allows to tailor shapes for desired inference behaviour much easier.
indeed, GW and FC have thought of a shape that has \pdc\ sensitivity,
and gives `bonus precision' if prior and data agree especially well,
as mentioned a desirable property in Section~\ref{sec:insights}
and in the discussion of GBR in Section~\ref{sec:updating}

very appealing first results for Beta-Binomial model and Normal-Normal model,
hope for a joint publication of GW, FC and MB explaining this in detail.


\textbf{finally, general thoughts about updating, learning, etc}

elaborate thoughts in Section~\ref{sec:updating} again,
see also comments at end of Section~\ref{sec:6-gw-071216},

dual role of $\nz$? (Section~\ref{sec:ibbm-resume})

belief revision versus updating?

questions on (data) robustness of IP models, see isipta13 paper by Marco


