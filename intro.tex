\chapter{Introduction}


\begin{itemize}
\item overview, which paper used where\ldots
\item statistical inference ??? (\textbf{StatInf chapter}, \cite{itip-statinf})
\item Bayesian inference, conjugate priors, etc. (\textbf{TechRep Dirichlet}, \cite{Walter2012b})
\item motivate priors etc.\ with ress paper: \cite{Walter2013a}
\item short part on decision theory, updating rules?
\item \pdc\ (Evans \& Moshonov), examples (\textbf{Festschrift paper}, \cite{Walter2010a})
\item other motivations for IP?
\end{itemize}

\section{Overview}

***structure, which papers where***

Sec.~\ref{sec:stat-inference}: parts of \cite{itip-statinf}

Sec.~\ref{sec:bayes-inference}: parts of \cite{itip-statinf}

Sec.~\ref{sec:inferencetasks}: parts of \cite{itip-statinf}

Sec.~\ref{sec:beta-binom}: parts of \cite{itip-statinf}

Sec.~\ref{sec:norm-norm}: parts of \cite{itip-statinf}

Sec.~\ref{sec:diri-multi}: \cite{Walter2012b}

Sec.~\ref{sec:commoncause}: parts of \cite{Walter2013a}


\section{Statistical Inference}
\label{sec:stat-inference}

%This thesis being concerned with generalized Bayesian Inference

In this Section, we will give a short introduction to statistical inference
and models that are used to describe random samples.
%These introductory sections may also serve as a guide to the notation utilized.

Statistical inference is about learning from data.
It is basically concerned with inductive reasoning,
i.e.\ establishing a general rule from observations.
As is long known as the problem of induction \cite{1739:hume},
it is impossible to justify inductive reasoning by pure reason,
and therefore one cannot infer general statements (laws) with absolute truth from single observations.
%***statistical models are built for a certain purpose. (citexxx George P. Box, Kaplan?)\\
%***models try to mirror certain aspects of reality: those deemed relevant to answer %the question(s) at hand etc.
The statistical remedy for this inevitable and fundamental dilemma of any type of inductive reasoning is
(postulated, maybe virtual) \emph{randomness} of the sampling process that generates the data.
If, and only if, the sample is, or can be understood as, drawn randomly,
probability theory allows to quantify the error of statistical propositions concluded from the sample.

Specifically, to model the randomness, a \emph{statistical model} is formulated.
It is a tuple $(\mathcal{X}, \mathcal{Q})$, consisting of the \emph{sample space} $\mathcal{X}$,
i.e.\ the domain of the random quantity $X$ under consideration,
and a set $\mathcal{Q}$ of probability distributions,%
\footnote{Most models of statistical inference rely on $\sigma$-additive probability distributions.
Therefore, technically, in addition an appropriate ($\sigma$-)field $\sigma(\mathcal{X})$,
describing the domain of the underlying probability measure, has to be specified.
In most applications there are straightforward canonical choices for $\sigma(\mathcal{X})$,
and thus $\sigma$-fields are not explicitly discussed here.}
collecting all probability distributions that are judged to be potential candidates for the distribution of $X$.
In this setting $\mathcal{Q}$ is called \emph{sampling model} and every element $p\in \mathcal{Q}$ \emph{(potential) sampling distribution}.
The inferential task is to learn the true element $p^* \in \mathcal{Q}$ from multiple observations of the random process producing $X$.


\subsection{Parametric Models}
\label{sec:parametricmodels}

In this thesis, generally, so-called \emph{parameteric models} are considered,
where $\mathcal{Q}$ is pa\-ra\-me\-trized by a parameter $\vartheta$ of finite dimension,
assuming values in the so-called \emph{parameter space} $\Theta$, $\Theta \subseteq \reals^q, \ q < \infty$,
i.e.\ ${\cal Q}=\left(p_\vartheta\right)_{\vartheta \in \Theta}$.
Here, the different sampling distributions $p_\vartheta$ are implicitly understood as belonging to a specific class of distributions
(e.g., normal distributions, as in Section~\ref{sec:normaldist} below),
the basic type of which is assumed to be known completely,
and only some characteristics $\vartheta$ (e.g.\ the mean) of the distributions are unknown.

Throughout, we will assume (as is the case for all common applications)
that the underlying candidate distributions $p_\vartheta$ of the random quantity $X$
are either discrete or absolutely continuous with respect to the Lebesgue measure
(see, e.g. \cite[pp.~32f, 38]{1993:karr} for some technical details) for every $\vartheta \in \Theta$.
Then it is convenient to express every $p_\vartheta$ in the discrete case by its \emph{mass function} $f_\vartheta$,
with $f_\vartheta(x):=p_\vartheta(X=x),\forall x \in \mathcal{X}$,
and in the continuous case by its \emph{probability density function} (pdf) $f_\vartheta$,
where $f_\vartheta$ is such that $p_\vartheta(X \in [a,b]) = \int_a^b f_\vartheta(x) \dd x$.

An \emph{i.i.d.\ sample of size} $n$ (where \emph{i.i.d.}\ abbreviates independent, identically distributed)
\emph{based on the parametric statistical model} %
$(\mathcal{X}, (p_\vartheta)_{\vartheta \in \Theta})$ is a vector
\[
{\X}=(X_1, \ldots, X_n)^T %\label{120722-1}
\]
of independent random quantities $X_i$ with the same distribution $p_\vartheta$.
Then $\X$ is defined on $\mathcal{X}^n$ with probability distribution $p_\vartheta^{\otimes n}$
as the $n$-dimensional product measure describing the independent observations.
For Bayesian approaches as discussed here,
independence is often replaced by exchangeability (see, e.g., \cite[\S 4.2]{2000:bernardosmith}).
$p_\vartheta^{\otimes n}$ thus has the probability mass or density function
\[
\prod_{i=1}^{n} f_\vartheta(x_i) =: f_\vartheta(x_1, \ldots, x_n).%\label{120710-2}
\]
The term \emph{sample} is then also used for the concretely observed value(s) $\x=(x_1, \ldots, x_n)^T$.%
\footnote{Throughout, random quantities are denoted by capital letters, their values, or realizations, by small letters.}

Next, we will present two examples for basic parametric models that will be repeatedly discussed further on.


\subsection{The Normal Distribution}
\label{sec:normaldist}

A common model for observations that in principle can assume any value on the real line
is the \emph{normal distribution} with parameters $\mu$ and $\sigma^2$, also called the \emph{Gaussian distribution}.
Typical examples for data of this kind are scores in intelligence testing, or technical measurements in general.%
\footnote{The normal distribution is distinguished by the central limit theorem
(see, e.g., \cite[\S 7.3]{1993:karr}, or \cite[\S 9]{1968:breiman}), stating that, under regularity conditions,
the distribution of an appropriately scaled sum of $n$ standardized random variables converges to a normal distribution
for $n \longrightarrow \infty$.}

For each observation $x_i$, $i=1,\ldots, n$, the normal probability density is
\begin{align*}
f_{(\mu, \sigma^2)}(x_{i}) &= \frac{1}{\sqrt{2\pi\sigma^2}}\exp\Big\{ -\frac{1}{2\sigma^2}(x_{i} - \mu)^2 \Big\}\,,
\end{align*}
with the two parameters $\mu \in \reals$ and $\sigma^2 \in \reals_{>0}$
being in fact the mean and the variance of (the distribution of) $x_i$, respectively.
As a shortcut, we write $x_i \sim \norm(\mu, \sigma^2)$.
%a normal density with $\mu=0$ and $\sigma^2=1$ is displayed
%in Figure~\ref{inference:fig:normal-vs-cauchy}, page~\pageref{inference:fig:normal-vs-cauchy}.

With the independence assumption, the density of $\mbf{x} = (x_1,\ldots,x_n)$ amounts to
\begin{align}\label{eq:normaldens}
f_{(\mu, \sigma^2)}(\x)
 &= \prod_{i=1}^n f_{(\mu, \sigma^2)}(x_{i})
  = (2\pi\sigma^2)^{-\frac{n}{2}}\exp\Big\{ -\frac{1}{2\sigma^2}\sum_{i=1}^n (x_{i} - \mu)^2 \Big\}\,.
\end{align}

Later on, we restrict considerations to the case where the variance $\sigma_0^2$ is known, denoted by $x_i \sim \norm(\mu, \sigma_0^2)$.
Inference may thus concern the parameter $\mu$ directly, or future observations $x_{n+1}, x_{n+2}, \ldots$ in a chain of i.i.d.\ observations.


\subsection{The Multinomial Distribution}
\label{sec:multinomdist}

The \emph{multinomial distribution} is a common model for samples where only a limited number of distinct values can be observed.
%like in the data from an election poll, or from a pharmacological study asking about a certain set of adverse reactions.
These distinct values are often named \emph{categories} (hence the term \emph{categorical data}),
and are usually numbered from $1$ to $k$, without imposing any natural ordering on these values.
We have therefore a discrete distribution,
giving the probability for observing certain category counts $(n_1, \ldots, n_k) = \mbf{n}$ in a sample of $n$ observations in total.
Thus, $\sum_{j=1}^k n_j = n$.

We start the definition of the multinomial distribution by decomposing the collection of $n$ observations into its constituents,
single observations of either of the categories $1,\ldots,k$.
Such a single observation, often named \emph{multivariate Bernoulli observation}, can be encoded as a vector $x_i$ of length $k$,
where the $j$-th element, $x_{ij}$, equals $1$ if category $j$ has been observed, and all other elements being $0$.
Given the vectorial parameter $\btheta$ of length $k$,
where the component $\theta_j$ models the probability of observing category $j$ in a single draw
(therefore $\sum_{j=1}^k \theta_j = 1$),
the probability for observing $x_i$ can be written as
\begin{align*}
f_{\btheta}(x_i) = \prod_{j=1}^k \theta_j^{x_{ij}}\,.
\end{align*}
Assuming independence, the probability for observing a certain sequence $\mbf{x}$ of $n$ observations can thus be written as
\begin{align*}
f_{\btheta}(\mbf{x}) &= \prod_{i=1}^n f_{\btheta}(x_i)
                \propto \prod_{i=1}^n \prod_{j=1}^k \theta_j^{x_{ij}}
                      = \prod_{j=1}^k \theta_j^{\sum_{i=1}^n x_{ij}}
                      = \prod_{j=1}^k \theta_j^{n_j}\,,
\end{align*}
where $n_j = \sum_{i=1}^n x_{ij}$ tells us how often category $j$ was observed in the sample.

For the probability to observe a certain category count $(n_1, \ldots, n_k) = \mbf{n}$,
we have to account for the different possible orderings in $\mbf{x}$ leading to the same count vector $\mbf{n}$.
Therefore,
\begin{align}\label{eq:multinomdens}
f_{\btheta}(\mbf{n}) &= \binom{n}{n_1,\ldots,n_k} \prod_{j=1}^k \theta_j^{n_j}
                      = \frac{n!}{n_1!\cdot \ldots \cdot n_k!} \prod_{j=1}^k \theta_j^{n_j}\,.
%               \propto \prod_{j=1}^k \theta_j^{n_j}\,.
\end{align}
As a shortcut, we write $\mbf{n} \sim \mult(\btheta)$.


\section{Statistical Inference with the Bayesian Paradigm}
\label{sec:bayes-inference}

As the inference models discussed in this thesis are all based on the Bayesian approach to statistical inference,
we will now give a short introduction to the basic principles of Bayesian inference.

The Bayesian approach allows (possibly subjective) knowledge on the parameter $\vartheta$ to be expressed by a probability distribution on%
\footnote{Again we implicitly assume that $\Theta$ is complemented by an appropriate $\sigma$-field $\sigma(\Theta)$.}
$\Theta$, with probability mass or density function $p(\vartheta)$ called \emph{prior distribution}.
Interpreting the elements $f_\vartheta(x)$ of the sampling model as conditional distributions of the sample given the parameter,
denoted by $f(\mbf{x}\mid\vartheta)$ and called \emph{likelihood},
turns the problem of statistical inference into a problem of probabilistic deduction,
where the \emph{posterior distribution}, i.e.\ the distribution of the parameter given the sample data,
can be calculated by Bayes' Rule.%
\footnote{Donald Gillies \cite{1987:gillies, 2000:gillies} argues that Bayes' Theorem was actually developed
in order to confront the problem of induction as posed by Hume \cite{1739:hume}.}
Thus, in the light of the sample $\mbf{x}= (x_1, \ldots, x_n)$ the prior distribution is updated by Bayes' Rule
to obtain the posterior distribution with density or mass function
\begin{align}
\label{eq:bayesrule}
p(\vartheta\mid\mbf{x}) \propto f(\mbf{x}\mid\vartheta) \cdot p(\vartheta)\,.
\end{align}
The posterior distribution is understood as comprising all the information from the sample and the prior knowledge.
It therefore underlies all further inferences on the parameter $\vartheta$,
like point estimators, interval estimators,
or the \emph{posterior predictive distribution},
which is the distribution of further observations based on $p(\vartheta\mid\mbf{x})$
(see Eq.~\eqref{eq:posteriorpredictive} below).


\subsection{Regular Conjugate Families of Distributions}

Traditional Bayesian inference is frequently based on so-called \emph{conjugate priors} related to a specific likelihood.
Such priors have the convenient property that the posterior resulting from~\eqref{eq:bayesrule}
belongs to the same class of parametric distributions as the prior, and thus only the parameters have to be updated,
which makes calculation of the posterior and thus the whole Bayesian inference easily tractable.%
\footnote{This motivation for the use of conjugate priors can be founded on formal arguments.
As will be explained below, posterior expectation of the parameter of interest
is actually a linear function of a sufficient statistic of the data and the prior expectation.
It turns out that, under some regularity conditions, requiring linearity of posterior expectation
implies the use of conjugate priors \cite[p.~276]{2000:bernardosmith}.}

Fortunately, there are general results guiding the construction of conjugate priors in models used most frequently in practice,
namely in the case where the sample distribution belongs to a so-called \emph{(regular) canonical exponential family}
\cite[pp.~202 and 272f]{2000:bernardosmith}. %Def.~4.12 and Prop.~5.6
This indeed covers many sample distributions relevant in a statistician's everyday life,
like Normal and Multinomial models, Poisson models, or Exponential and Gamma models.
After presentation of the general framework, we will discuss its instantiation for the Normal
and the Multinomial sampling models as introduced in Sections~\ref{sec:normaldist} and \ref{sec:multinomdist} above.

A sample distribution
(from now on understood directly as the distribution $p_\vartheta^{\otimes n}$ of an i.i.d.\ sample $\mbf{x}$ of size $n$)
is said to belong to the \emph{(regular) canonical exponential family} if its density or mass function satisfies the decomposition
\begin{align}
\label{eq:expofam-sampledens}
f(\x \mid \theta) &\propto \exp\big\{\langle \psi, \tau(\x) \rangle - n \mbf{b}(\psi)\big\}\,,
\end{align}
where $\psi \in \Psi \subset \reals^q$ is a transformation of the (possibly vectorial) parameter $\theta \in \Theta$,
and $\mbf{b}(\psi)$ a scalar function of $\psi$ (or, in turn, of $\theta$).
$\tau(\x)$ is a function of the sample $\x$ that fulfills $\tau(\x) = \sum_{i=1}^n \tau^*(x_i)$,
with $\tau^*(x_i) \in \mathcal{T} \subset \reals^q$,
while $\langle\cdot, \cdot\rangle$ denotes the scalar product.
From these ingredients, a conjugate prior on $\psi$ can be constructed as%
\footnote{In our notation, ${}\uz$ denotes prior parameters; ${}\un$ posterior parameters.}
\begin{align}
\label{eq:canonicalprior}
p(\psi \mid \nz, \yz) \dd\psi
 &\propto \exp\Big\{ \nz \Big[ \langle \yz, \psi \rangle - \mbf{b}(\psi) \Big] \Big\} \dd\psi\,,
\end{align}
where $\nz$ and $\yz$ are now the parameters by which a certain prior can be specified.
The domain of $\yz$ is $\Y$, the interior of the convex hull of $\mathcal{T}$;
$\nz$ must take strictly positive values for the prior to be proper (integrable to $1$).

An interpretation for these parameters will be given shortly.
First, let us calculate the posterior density for $\psi$.
The prior parameters $\yz$ and $\nz$ are updated to their posterior values $\yn$ and $\nn$ in the following way:
\begin{align}\label{eq:canonicalupdate}
\yn &= \frac{\nz}{\nz + n} \cdot \yz + \frac{n}{\nz + n} \cdot \frac{\tau(\x)}{n}\,, &
\nn &= \nz + n\,,
\end{align}
such that the posterior can be written as
\begin{align}\label{eq:canonicalposterior}
p(\psi \mid \x, \nz, \yz)
 =: p(\psi \mid \nn, \yn)
 &\propto \exp\Big\{ \nn \Big[ \langle \yn, \psi \rangle - \mbf{b}(\psi) \Big] \Big\} \dd\psi\,.
\end{align}
In this setting, $\yz$ and $\yn$ can be seen as the parameter describing the main characteristics of the prior and the posterior,
and thus we will call them \emph{main prior} and \emph{main posterior parameter}, respectively.
$\yz$ can also be understood as a prior guess for the random quantity $\ttau(\x) := \tau(\x)/n$ summarizing the sample,
as $\E[\ttau(\x) \mid \psi] = \nabla\mbf{b}(\psi)$,
where in turn $\E[\nabla\mbf{b}(\psi) \mid \nz, \yz] = \yz$ \cite[Prop.~5.7, p.~275]{2000:bernardosmith}.

Characteristically, $\yn$ is a weighted average of this prior guess $\yz$ and the sample `mean' $\ttau(\x)$,
with weights $\nz$ and $n$, respectively.%
\footnote{This weighted average property of Bayesian updating with conjugate priors is an important issue
we comment on ***later / in Section~\ref{}***.}
Therefore, $\nz$ can be seen as ``prior strength'' or ``pseudocounts'',
reflecting the weight one gives to the prior as compared to the sample size $n$.
To make this more explicit, $\nz$ can be interpreted as the size of an imaginary sample
that corresponds to the trust on the prior information in the same way
as the sample size of a real sample
corresponds to the trust in conclusions based on such a real sample \cite[p.~258]{Walter2009a}.

The posterior $p(\psi \mid \nn, \yn)$ can be transformed back to a distribution on $\theta$
in order to deal with a commonly known parameter or distribution family for it
(as we will do, e.g., in Section~\ref{sec:beta-binom} below.)
Besides the posterior itself, also the posterior predictive distribution
\begin{align}
\label{eq:posteriorpredictive}
f(\x^*\mid\x, \nz, \yz) &= \int f(\x^* \mid \psi) p(\psi \mid \nn, \yn) \dd\psi\,,
\end{align}
a distribution on future samples $\x^*$ after having seen a sample $\x$,
forms the basis for the different inference tasks. %as listed in Section~\ref{inference:sec:basictasks}.
Next, we will thus briefly describe a systematic of inference tasks.


\subsection{Inference Tasks}
\label{sec:inferencetasks}

We may structure the different inference tasks by
the type of statement one wants to infer from the data.
As such, this systematic is not exclusive to Bayesian inference methods,
and neither to the parametric models considered in Section~\ref{sec:parametricmodels},
but it will be formulated in terms of parameters in a Bayesian setting here.

We distinguish two groups of inferences, namely
\begin{enumerate}
\item static conclusions and
\item predictive conclusions.%
%\footnote{Static conclusions are still more common in statistical applications.
%In the light of the success of procedures from machine learning in statistics, the predictive view,
%early propagated, e.g., by \cite{1993:geisser}, has attracted increasing attention.}
\end{enumerate}

\emph{Static conclusions} refer directly to properties of the sampling model, typically to its parameter(s).
The following procedures, which are based directly on the posterior \eqref{eq:canonicalposterior} in the Bayesian paradigm,
are the most common:
\begin{enumerate}
\item[1a)] \emph{Point estimators},
where a certain parameter value is selected to describe the sample.
\item[1b)] \emph{Interval estimators},
where the information is condensed in a certain subset of the parameter space $\Theta$, typically in an interval when $\Theta \subseteq \reals$.
\item[1c)] \emph{Hypotheses tests},
where the information in the sample is only used to decide between two mutually exclusive statements about parameter(s) called \emph{hypotheses},
usually denoted by $H_0$ and $H_1$.
\item[1e)] \emph{Decision making},
where, more generally, the conclusion may be to select certain utility maximizing or loss minimizing acts from a set of possible acts.
Indeed, all three tasks listed before can be formally understood as a special case of decision making,
by considering an appropriate set of acts and a loss function fitting the inferential objective.
We will flesh this out to some extent in the examples below ******.%
\footnote{For more details, see, e.g., \cite[\S 2]{2007:robert},
where, e.g., loss functions usually utilized in statistical settings are described in \S 2.5, p.~77ff.}
\end{enumerate}

\emph{Predictive conclusions} instead summarize the information by statements on properties of typical further units,
either by describing the whole distribution (as with the posterior predictive \eqref{eq:posteriorpredictive},
or by certain summary measures.
Similar to static conclusions, one can thus consider, e.g.,
\begin{enumerate}
\item[2a)] \emph{Point prediction},
where a certain sample value is selected as the most likely to occur.
This is especially useful in the case of discrete sampling distributions,
where this procedure amounts to classification of further sample units.
\item[2b)] \emph{Interval prediction},
where instead a certain subset of the sample space $\mathcal{X}$ is determined,
into which furter sample units are like to fall.
An example are prediction bands in regression analysis.
\end{enumerate}

The concretion of this framework for Bayesian inference with canonical conjugate priors
is now demonstrated for the sampling models presented in Sections~\ref{sec:normaldist} and \ref{sec:multinomdist}.
As a first simple example, we will consider inference in the Binomial model,
being the special case of the Multinomial model with only two categories.
Then, we will briefly turn to the Normal model,
before we present the more complex considerations for the Multinomial model with $k>2$ categories.
The latter model %for the Multinomial model (Section~\ref{sec:multinomdist})
is then used for inferences in common-cause modeling,
which will serve as a real-world example illustrating the powers and shortcomings of standard Bayesian inference,
ultimately motivating the shift to imprecise Bayesian inference.


\subsection{The Beta-Binomial Model}
\label{sec:beta-binom}

As the special case of the multinomial model \eqref{eq:multinomdens} with only two categories, we will consider the Binomial model
\begin{align}
\label{eq:binomdens}
f(\x\mid\theta) &= {n \choose s}\theta^s (1-\theta)^{n-s}\,,
\end{align}
where $\x$, the vector of $n$ observations, is composed of scalar $x_i$'s being either $0$ or $1$
denoting `failure' or `success' in an experiment with these two outcomes.
$s = \sum_{i=1}^n x_i$ is the number of successes,
and the (unknown) parameter $\theta \in (0,1)$ is the probability for `success' in a single trial.
\eqref{eq:binomdens} can be written in the canonical exponential family form \eqref{eq:expofam-sampledens}:
\begin{align*}
f(\x\mid\theta) &\propto \exp\Big\{ \log\Big(\frac{\theta}{1-\theta}\Big) s - n \big(-\log(1-\theta)\big) \Big\}
\end{align*}
We have thus $\psi = \log(\theta/(1-\theta))$, $\mbf{b}(\psi) = -\log(1-\theta)$, and $\tau(\x) = s$.
The function $\log(\theta/(1-\theta))$ is known as the \emph{logit}, denoted by $\logit(\theta)$.

From these ingredients, a conjugate prior on $\psi$ can be constructed along \eqref{eq:canonicalprior},
leading here to
\begin{align*}
p\Big(\log\Big(\frac{\theta}{1-\theta}\Big) \mid \nz, \yz\Big) \dd\psi %\dd\log\big(\frac{\theta}{1-\theta}\big)
 &\propto \exp\Big\{ \nz \Big[ \yz \log\Big(\frac{\theta}{1-\theta}\Big) + \log(1-\theta) \Big] \Big\} \dd\psi\,.
 %\dd\log\big(\frac{\theta}{1-\theta}\big)\,.
\end{align*}
This prior, transformed to the parameter of interest $\theta$,
\begin{align*}
p(\theta\mid \nz, \yz)\dd\theta &\propto \theta^{\nz\yz - 1} (1-\theta)^{\nz(1-\yz) - 1} \dd\theta\,,
\end{align*}
is a Beta distribution with parameters $\nz\yz$ and $\nz(1-\yz)$, in short, %$\theta \sim \Be(\nz\yz, \nz(1-\yz))$.
\begin{align*}
\theta &\sim \be(\nz\yz, \nz(1-\yz))\,.
\end{align*}

The combination of a Binomial sampling model with this conjugate Beta prior is called \emph{Beta-Binomial model}.
Here, $\yz = \E[\theta]$ can be interpreted as prior guess of $\theta$,
while $\nz$ governs the concentration of probability mass around $\yz$,
with large values of $\nz$ giving high concentration of probability mass.
Due to conjugacy, the posterior on $\theta$ is a $\be(\nn\yn, \nn(1-\yn))$,
where the posterior parameters $\nn$ and $\yn$ are given by \eqref{eq:canonicalupdate}.

A \textbf{point estimator} for $\theta$ can be extracted from the posterior distribution $p(\theta\mid\x)$
by considering $\Theta$ as the set of possible acts, and choosing a \emph{loss function}.
The loss function $l$ gives a functional form for the severity of deviations of an estimator to its goal;
here, it values the distance of a point estimator $\hat{\theta}$ to $\theta$.
The \emph{quadratic loss function} $l(\hat{\theta}, \theta) = (\hat{\theta}-\theta)^2$
values small deviations relatively low, whereas large deviations are given a high weight.
As can be shown (see, e.g., \cite[p.~352f]{2002:casella}),
the quadratic loss function leads to the posterior expectation as the Bayesian point estimator.
Here, $\E[\theta\mid\x, \nz, \yz] = \E[\theta\mid\nn, \yn] = \yn$,
and so the posterior expectation of $\theta$ is a weighted average
of the prior expectation $\E[\theta\mid\nz, \yz] = \yz$ and the sample proportion $s/n$, with weights $\nz$ and $n$, respectively.
Taking the \emph{absolute loss function} $l(\hat{\theta}, \theta) = |\hat{\theta}-\theta|$
leads to the median of the posterior distribution as the point estimator.
Here, $\med(\theta\mid\nn, \yn)$ has no closed form solution, and must be determined numerically.
The \emph{indicator loss function}
\begin{align*}
l(\hat{\theta}, \theta) = \begin{cases} 0 & |\hat{\theta}-\theta| \le \epsilon \\ 1 & \text{else}\end{cases}\,,
\end{align*}
for $\epsilon \rightarrow 0$, leads to the maximum of the posterior,
often abbreviated as MAP (maximum a posteriori) estimator \cite[\S 5.1.5, p.~257]{2000:bernardosmith}, \cite[\S 4.1.2, p.~166]{2007:robert}.
For a $\be(\nn\yn, \nn(1-\yn))$, the mode is
\begin{align*}
\mode p(\theta\mid\nn, \yn) &= \frac{\nn\yn - 1}{\nn - 2} = \frac{\nz\yz - 1 + s}{\nz - 2 + n}\,,
\end{align*}
and thus is a weighted average of the prior mode $\frac{\nz\yz - 1}{\nz -2}$ $(\nz > 2)$
and the sample proportion $s/n$, with weights $\nz - 2$ and $n$, respectively.

Note that asymptotic optimality properties of maximum likelihood estimators (consistency, efficiency)
are usually preserved for these Baysian point estimators \cite[Note~1.8.4, p.~48f]{2007:robert}.

In the Bayesian approach, \textbf{interval estimation} is rather simple,
as the posterior distribution delivers a direct measure of probability for arbitrary subsets of the parameter space $\Theta$.
Mostly, so-called \emph{highest posterior density} (HPD) intervals are considered,
where for a given probability level $\gamma$ the shortest interval covering this probability mass is calculated.
For unimodal densities, this is equivalent to finding a threshold $\alpha$ such that
the probability mass for all $\theta$ with $p(\theta\mid\nn, \yn) \ge \alpha$ equals $\gamma$, hence the name.%
\footnote{See, e.g., \cite[\S 5.1.5, pp.~259f]{2000:bernardosmith}, or \cite[Def.~5.5.3, p.~260]{2007:robert}.}
For the Beta posterior, a HPD interval for $\theta$ must be determined by numeric optimization.
For approximately symmetric (around $\frac{1}{2}$) Beta posteriors, a good approximation is the symmetric credibility interval,
delimited by the $\frac{1-\gamma}{2}$- and the $\frac{1+\gamma}{2}$-quantile of the posterior.

The \textbf{testing of hypotheses} concerning the parameter of interest $\theta$ can be done
by comparing posterior probabilities of two (disjunct) subsets of the parameter space.
Like in frequentist Neyman-Pearson testing, these are denoted by $H_0$ and $H_1$,
but unlike there, in Bayesian testing these hypotheses play a symmetric role.

****more on Bayes factors, etc.***

High values of $P(H_1\mid\x) / P(H_0\mid\x)$ then indicate high plausibility of $H_1$ as compared to $H_0$.

Bayesian testing: \cite[\S 5.2--5.4]{2007:robert}

Bayes factor definition: \cite[\S 5.2.2, Def.~5.2.5, p.~227]{2007:robert}

Article on Bayes factors as entity of its own: Kass \& Raftery 1995: Bayes factor and model uncertainty, JASA 90, 773--795

improper priors are problematic in Bayesian testing \cite[\S 5.2.5, p.~232ff]{2007:robert}

\cite[p.~235f]{2007:robert}
\begin{quote}
The difficulties encountered with noninformative priors in testing
also point out that a testing problem cannot be treated in a coherent way if no prior information is available,
that is, that the information brought by the observations alone is usually not enough
to infer about the truth of a hypothesis in categorical fashion (\emph{yes}/\emph{no}).
This obviously reinforces the the motivation for a Bayesian treatment of such testing problems,
since it is the only coherent approach taking advantage of the residual information.
\end{quote}

****end more on Bayes factors, etc.***

The \textbf{posterior predictive} distribution, giving the probability for $s^*$ successes in $n^*$ future trials
after having seen $s$ successes in $n$ trials, is
\begin{align*}
f(s^* \mid \nn, \yn) &= {n^* \choose s^*} \frac{\B\big(s^* + \nn \yn, n^* - s^* + \nn (1 - \yn)\big)}{\B\big(\nn\yn, \nn (1 - \yn)\big)}\,,
\end{align*}
known as the \emph{Beta-Binomial distribution}.


\subsection{The Normal-Normal Model}
\label{sec:norm-norm}

The normal density \eqref{eq:normaldens}, here with the variance $\sigma^2$ known to be $\sigma^2_0$,
also adheres to the exponential family form:
\begin{align*}
f(\x \mid \mu, \sigma_0^2)
 &\propto \exp\Big\{ \frac{\mu}{\sigma_0^2} \sum_{i=1}^n x_i - \frac{n\mu^2}{2\sigma_0^2} \Big\}.
\end{align*}
So we have here $\psi = \frac{\mu}{\sigma_0^2}$, $\mbf{b}(\psi) = \frac{\mu^2}{2\sigma_0^2}$, and $\tau^*(x_i) = x_i$.
From these ingredients, a conjugate prior can be constructed with \eqref{eq:canonicalprior}, leading to
\begin{align*}
p\Big(\frac{\mu}{\sigma_0^2} \mid \nz, \yz\Big) \dd\frac{\mu}{\sigma_0^2}
 &\propto \exp\Big\{ \nz \Big( \langle \yz, \frac{\mu}{\sigma_0^2} \rangle - \frac{\mu^2}{2\sigma_0^2} \Big) \Big\} \dd\frac{\mu}{\sigma_0^2}\,.
\end{align*}
This prior, transformed to the parameter of interest $\mu$ and with the square completed,
\begin{align*}
p(\mu \mid \nz, \yz) \dd\mu
 &\propto \frac{1}{\sigma_0^2} \exp\Big\{ -\frac{\nz}{2\sigma_0^2} (\mu - \yz)^2 \Big\} \dd\mu\,,
\end{align*}
is a normal distribution with mean $\yz$ and variance $\frac{\sigma_0^2}{\nz}$,
i.e.\ $\mu \sim \norm(\yz, \frac{\sigma_0^2}{\nz})$.

With \eqref{eq:canonicalupdate}, the parameters for the posterior distribution are
\begin{align}
\yn &= \E[\mu\mid\nn, \yn] = \frac{\nz}{\nz + n} \cdot \yz + \frac{n}{\nz + n} \cdot \bar{\x} \label{eq:normalposte}\\
\frac{\sigma_0^2}{\nn} &= \V(\mu\mid\nn, \yn) = \frac{\sigma_0^2}{\nz + n}\,. \label{eq:normalpostvar}
\end{align}
The posterior expectation of $\mu$ thus is a weighted average of the prior expectation $\yz$ and the sample mean $\bar{\x}$,
with weights $\nz$ and $n$, respectively.
The effect of the update step on variance is that it decreases by the factor $\nz/(\nz+n)$.

Here, all of the three choices of loss functions mentioned for the Beta-Binomial model
lead to the same \textbf{point estimator} $\hat{\mu} = \yn$,
as in normal distributions mean, median, and mode coincide.

As \textbf{interval estimation}, the HPD interval can be calculated, due to symmetry of the normal posterior,
as $[z\un_\frac{1-\gamma}{2}, z\un_\frac{1+\gamma}{2}]$, where, e.g.,
$z\un_\frac{1-\gamma}{2}$ is the $\frac{1-\gamma}{2}$-quantile of the normal distribution with mean $\yn$ and variance $\frac{\sigma_0^2}{\nn}$.

The \textbf{testing of hypotheses} about $\mu$ works again by comparing posterior probabilities of two disjunct subsets of the parameter space.
Note that the frequentist analogue to such a test is the (one-sided) one-sample Z-test (or Gaussian test).

The \textbf{posterior predictive} distribution for $n^*$ future observations denoted by $\x^*$ is again a normal distribution,
$\x^*\mid(\nn,\yn) \sim \norm\big(\yn, \frac{\sigma_0^2}{\nn}(\nn + n^*)\big)$,
centered at the posterior mean \eqref{eq:normalposte},
and with variance increasing with the posterior variance \eqref{eq:normalpostvar}
and the number of observations to be predicted.


\subsection{The Dirichlet-Multinomial Model}
\label{sec:diri-multi}

The construction of the canonical conjugate prior along \eqref{eq:canonicalprior}
for the Multinomial model $\mult(\btheta)$ with $k>2$ is more complex than for the case $k=2$
as covered in Section~\ref{sec:beta-binom}.
It is a well-known result that this construction leads to the commonly used Dirichlet prior;
however, it is usually not derived in detail.%
\footnote{E.g., \cite[Table~1]{2005:quaeghebeurcooman} tabulate, without proof,
priors constructed for a number of sample models.
Note that the table contains a sign error in $\mbf{b}(\bpsi)$ for the Multinomial sampling model.}
We will cover the construction of $p(\bpsi\mid\nz, \yz)$,
and also the transformation to $p(\btheta\mid\nz, \yz)$,
in more detail here.

We will use the formulation of $\mult(\btheta)$ as the multivariate Bernoulli distribution
like in Section~\ref{sec:multinomdist}.
The distribution of a single multivariate Bernoulli observation is equivalent to a Multinomial distribution with sample size $1$,
and i.i.d.\ repetitions of a multivariate Bernoulli distribution lead to the Multinomial distribution.
Since i.i.d.\ repetitions do not interfere with conjugacy,
we may construct the canonical conjugate prior by considering a single multivariate Bernoulli observation.

To do without side conditions like $\sum_{j=1}^k \theta_j = 1$ as needed in Section~\ref{sec:multinomdist},
we will define here as a single multivariate Bernoulli observation distinguishing $k+1$ categories $j = 0,1,\ldots,k$
(instead of $k$ categories $j = 1,\ldots,k$ as in Section~\ref{sec:multinomdist})
a vector $\x$ with $k$ components indexed $1,\ldots,k$
such that
\begin{align*}
\x \in \{0,1\}^k \cap \big\{ \x: \sum_{j=1}^k x_j \in \{0,1\}  \big\}\,,
\end{align*}
and $x_0 := 1 - \sum_{j=1} x_j$.

The parameter vector is treated in the same way, i.e.,
we consider now $\btheta$ with $k$ components $\theta_j, j=1,\ldots,k$ such that
\begin{align*}
\btheta \in (0,1)^k \cap \{\btheta: 0 < \sum_{j=1}^k \theta_j < 1\}\,,
\end{align*}
and thus $\theta_0 := 1-\sum_{j=1}^k \theta_j$.

Formulating the density \eqref{eq:multinomdens} accordingly, and rewriting it towards~\eqref{eq:expofam-sampledens},
we get
\begin{align*}
p(\x\mid\btheta)&= \Bigg( \prod_{j=1}^k \theta_j^{x_j}\Bigg) \bigg( 1 - \sum_{j=1}^k \theta_j\bigg)^{1-\sum_{j=1}^k x_j}%\\
                 = \theta_0 \prod_{j=1}^k \left(\frac{\theta_j}{\theta_0}\right)^{x_j} \\
                &= \exp\Bigg\{ \sum_{j=1}^k x_j \ln\bigg(\frac{\theta_j}{\theta_0}\bigg) - \big(-\ln(\theta_0)\big) \Bigg\}\,.
\end{align*}
With $\bpsi$ and $\mbf{b}(\bpsi)$ derived from the sample model as
\begin{align*}
\psi_j &= \ln\left(\frac{\theta_j}{\theta_0}\right),\, j=1,\ldots,k & \text{and}& &
\mbf{b}(\bpsi) &= -\ln(\theta_0),\,% \theta_0 = 1 - \sum_{j=1}^k \theta_j\,,
\end{align*}
the conjugate prior is at first constructed as a density over $\bpsi$,
dropping the upper index ${}\uz$ in $\nz$ and the vectorial $\mbf{y}\uz$ for ease of notation:
\begin{align*}
p(\bpsi\mid n,\mbf{y})\, \dd\bpsi
 &\propto \exp\Bigg\{n\, \Bigg[ \sum_{j=1}^k y_j \ln\bigg(\frac{\theta_j}{\theta_0}\bigg) - \big(-\ln(\theta_0)\big) \Bigg] \Bigg\} \,\dd\bpsi\,,.
\end{align*}
Written as a density over $\btheta$, we have
\begin{align*}
p(\btheta\mid n,\mbf{y})\, \dd\btheta
 &\propto \exp\Bigg\{n\, \Bigg[ \sum_{j=1}^k y_j \ln\bigg(\frac{\theta_j}{\theta_0}\bigg) - \big(-\ln(\theta_0)\big) \Bigg] \Bigg\}
  \cdot \left| \det\left(\frac{\dd\bpsi}{\dd\btheta}\right) \right|\, \dd\btheta\,,
\end{align*}
with the the elements of the Jacobian matrix $\frac{\dd\bpsi}{\dd\btheta}$ being
\begin{align*}
\frac{\dd\psi_i}{\dd\theta_i} &= \frac{1}{d\theta_i} \ln\left(\frac{\theta_i}{1 - \sum_{j=1}^k \theta_j}\right)
                               = \frac{1-\sum_{j=1}^k \theta_j}{\theta_i}
                                 \cdot \frac{1 - \sum_{j=1}^k \theta_j + \theta_i}{(1 - \sum_{j=1}^k \theta_j)^2}
                               = \frac{\theta_0 + \theta_i}{\theta_0 \theta_i}\\
\frac{\dd\psi_h}{\dd\theta_i} &= \frac{1}{d\theta_i} \ln\left(\frac{\theta_h}{1 - \sum_{j=1}^k \theta_j}\right)
                               = \frac{1-\sum_{j=1}^k \theta_j}{\theta_h} \cdot \frac{\theta_h}{(1 - \sum_{j=1}^k \theta_j)^2}
                               = \frac{1}{\theta_0}\,, \quad h \neq i
\end{align*}
%\allowdisplaybreaks
Thus,
\begin{align*}
\det\left(\frac{\dd\bpsi}{\dd\btheta}\right)
&= \det \begin{pmatrix}\frac{\theta_0 + \theta_1}{\theta_0 \theta_1} & \frac{1}{\theta_0} & \hdots & \frac{1}{\theta_0} \\
                       \frac{1}{\theta_0} & \frac{\theta_0 + \theta_2}{\theta_0 \theta_2} & \ddots & \vdots \\
                       \vdots & \ddots & \ddots & \frac{1}{\theta_0} \\[1ex]
                       \frac{1}{\theta_0} & \hdots & \frac{1}{\theta_0} & \frac{\theta_0 + \theta_k}{\theta_0 \theta_k}
        \end{pmatrix} \\
&= \left(\frac{1}{\theta_0}\right)^k\, \det
        \begin{pmatrix}\frac{\theta_0}{\theta_1} + 1 & 1 & \hdots & 1 \\
                       1 & \frac{\theta_0 }{\theta_2} + 1 & \ddots & \vdots \\
                       \vdots & \ddots & \ddots & 1 \\[1ex]
                       1 & \hdots & 1 & \frac{\theta_0}{\theta_k} + 1
        \end{pmatrix} \\
&\stackrel{*}{=} \left(\frac{1}{\theta_0}\right)^k \prod_{j=1}^k \frac{\theta_0}{\theta_j}
                 \cdot \Bigg( 1 + \begin{pmatrix}1 & \hdots & 1\end{pmatrix}
                                  \begin{pmatrix}\frac{\theta_1}{\theta_0} &  & 0 \\
                                                  & \ddots & \\
                                                 0 & & \frac{\theta_k}{\theta_0}\end{pmatrix}
                                  \begin{pmatrix}1 \\ \vdots\\ 1\end{pmatrix}\Bigg)\\
&= \prod_{j=1}^k \frac{1}{\theta_j} \cdot \Bigg( 1 + \sum_{i=1}^k \frac{\theta_i}{\theta_0} \Bigg)
 = \Bigg( \prod_{j=1}^k \frac{1}{\theta_j}\Bigg) \frac{ \theta_0 + \sum_{i=1}^k \theta_i}{\theta_0}\\
&= \Bigg( \prod_{j=1}^k \frac{1}{\theta_j}\Bigg) \frac{1}{\theta_0}
 = \prod_{j=0}^k \frac{1}{\theta_j}\,,
\end{align*}
where equality ${}^*$ holds by the theorem \cite[Theorem A 16 (x), Appendix A3, p.~494]{MR2370506},
%in Rao, Toutenburg, Shalabh, and Heumann, \emph{Linear models and generalizations}, Springer, Berlin 2008: %bibtex at end of document
stating that
\begin{align*}
\det (A + a\, a^\tra) &= \det(A) (1+a^\tra A^{-1} a) & \text{if}& & \det(A) &\neq 0
\end{align*}
for all appropriately sized matrices $A$ and column vectors $a$.

With $\det\left(\frac{\dd\bpsi}{\dd\btheta}\right) = \prod_{j=0}^k \frac{1}{\theta_j}$, we get
\begin{align*}
p(\btheta\mid n,\mbf{y})
 &\propto \exp\Bigg\{n\, \Bigg[ \sum_{j=1}^k y_j \ln\bigg(\frac{\theta_j}{\theta_0}\bigg) - \big(-\ln(\theta_0)\big) \Bigg] \Bigg\}
  \cdot \bigg| \prod_{j=0}^k \frac{1}{\theta_j} \bigg| \\
 &=       \exp\Bigg\{n\, \Bigg[ \sum_{j=1}^k y_j \Big(\ln(\theta_j)-\ln(\theta_0)\Big) + \ln(\theta_0) \Bigg]
                         - \sum_{j=0}^k \ln(\theta_j) \Bigg\} \\
 &=       \exp\Bigg\{n\, \Bigg[ \sum_{j=1}^k y_j \ln(\theta_j) + \ln(\theta_0) \bigg(\underbrace{1 - \sum_{j=1}^k  y_j}_{=: y_0}\bigg) \Bigg]
                         - \sum_{j=0}^k \ln(\theta_j) \Bigg\} \\
 &=       \exp\Bigg\{n\, \Bigg[ \sum_{j=0}^k y_j \ln(\theta_j) \Bigg] - \sum_{j=0}^k \ln(\theta_j) \Bigg\} \\
 &=       \exp\Bigg\{\sum_{j=0}^k (n\,y_j -1 ) \ln(\theta_j) \Bigg\}
  =       \exp\Bigg\{\sum_{j=0}^k \ln\left(\theta_j^{n\,y_j -1}\right) \Bigg\}\\
 &= \prod_{j=0}^k \theta_j^{n\,y_j - 1}\,,
\end{align*}
which is the core of a Dirichlet density over $\btheta$.
Therefore, the Dirichlet distribution is the canonically constructed conjugate prior to the multivariate Bernoulli.
Due to the considerations from the beginning of this section,
we see that the Dirichlet distribution is the canonically constructed conjugate prior
also to the Multinomial sample model with arbitrary sample sizes.

The Dirichlet distribution can be seen as a direct generalization of the Beta distribution,
and we will speak of the \emph{Dirichlet-Multinomial model}
as the analogue to the Beta-Binomial model from Section~\ref{sec:beta-binom}.

Returning to the notation from Section~\ref{sec:multinomdist},
where $k$ categories $j=1,\ldots,k$ are considered, 
we have thus
\begin{align*}
p(\btheta\mid\nz,\byz) \propto \prod_{j=1}^k \theta_j^{\nz\yz_j-1}
\end{align*}
as the prior.
The vectorial $\byz$ is an element of the interior of the unit simplex $\Delta$,
thus $\forall j \ \yz_j \in (0,1)$, $\sum_{j=1}^k \yz_j = 1$, in short $\byz \in \text{int}(\Delta)$.
Here, the main posterior parameter is calculated as
\begin{align*}
\yn_j &= \frac{\nz}{\nz+n} \yz_j + \frac{n}{\nz+n} \cdot \frac{n_j}{n}\,, j = 1,\ldots, k,
\end{align*}
and is thus again a weighted average of the main prior parameter
(which can be interpreted as prior guess for $\btheta$, as $\E[\btheta\mid\nz,\byz] = \byz$)
and the fractions of observations in each category,
with weights $\nz$ and $n$, respectively.
$\nz$ again governs the concentration of probability mass around $\yz$,
with larger values of $\nz$ leading to higher concentrations.

With these tangible intuitions for $\nz$ and $\byz$,
we will denote the Dirichlet prior directly in terms of the canonical parameters, i.e.\ by
%\begin{align*}
$\btheta \sim \dir(\nz,\byz)$.
%\end{align*}
The canonical parameters relate to the commonly used parameter $\bs{\alpha}$ with components $\alpha_1, \ldots, \alpha_k$ by
\begin{align*}
\yz_j &= \frac{\alpha_j}{\sum_{l=1}^k \alpha_l} &
\nz   &= \sum_{l=1}^k \alpha_l \,.
\end{align*}

We will now present a problem in reliability analysis for which the most common model
relies on the Dirichlet-Multinomial model.
***It will also serve as a motivation for the use of sets of priors.***


\section{Dirichlet-Multinomial Model for Common-Cause Failure}
\label{sec:commoncause}

In reliabilty analysis of systems with redundant components,
\emph{common-cause failure} refers to simultaneous failure of several redunant components due to a common or shared root cause,
like extreme environmental conditions (e.g., fire, flood, or earthquake) \cite[p.325]{1994:hoyland}.
It has been recognized as the dominant factor to the unreliability of redundant systems,
and its modeling has become an important part of reliability analysis
following the Reactor Safety Study \cite{1975:reactor:safety:study},
which was prepared in the wake of the Three Mile Island accident,
where a partial core meltdown in a nuclear power plant took place \cite{2005:walker}.

\subsection{An Example for Common-Cause Failure Modelling}

For a nuclear power plant, a common-cause failure analysis can relate to, e.g.,
the diesel generators that, if in an emergency the off-site power supply is cut off,
provide the electricity to power the emergency core cooling systems.
Emergency core cooling systems are needed to transfer away residual heat emitted from the core after shutdown,
in case the normal heat removal process%
\footnote{Heat from the core is transferred to steam generators,
producing steam that drives the turbines to produce electricity.
Usually, the depressurized steam exiting the turbines
is then condensed to water and fed back into the steam generators.}
is not available.
When residual heat is not removed, it can overheat the core,
which could lead to a (partial) core meltdown,
and subsequently to a possibly catastrophic release of radioactive material.%
\footnote{The residual heat that is present in the core of a nuclear power plant
after shutdown of the nuclear chain reaction is called \emph{decay heat}.
It results from decay processes of fission products produced during normal operation of the power plant.
Although comprising only a small fraction of the energy output during normal operation
(where the energy stems from the primary fission process \cite[Module~4, p.~33]{united1993doe}),
depending on the design of the reactor,
the decay heat may be enough to damage the core of a reactor significantly
\cite[p.~VIII-9 and VIII-25f]{1975:reactor:safety:study}.}

Due to their critical role for the functioning of emergency cooling systems,
there are usually several diesel generators installed in a nuclear power plant,
each of which can supply enough energy to power the cooling systems on its own.
The number of diesel generators installed is typically in the range of two to four per reactor***??***
\cite[p.~31]{2011:iaea::report} ***.
%i.e., there are four redundant components, of which one is needed for service.
%with a redundancy in the order of four or 
The Fukushima Daiichi nuclear disaster is a recent example of an accident
involving common cause failure of diesel generators.
In this case, all 12 available diesel generators at reactors 1 to 6 
ceased to function due to a tsunami wave flooding the rooms where they were situated \cite[p.~31]{2011:iaea::report}.
The tsunami wave had been caused by the Tohoku*** earthquake,
which had promted the reactors to shutdown automatically, and in doing so,
switching to the diesel generators for power supply.%
\footnote{All six off-site power lines were cut off, also due to the earthquake \cite[p.~31]{2011:iaea::report}.}

The arguably most widely used model for common-cause failure is the Basic Parameter Model.
The alpha-factor parametrisation of this model uses a multinomial distribution
as its aleatory model for observed failures \cite{1988:mosleh::common:cause}. 
As seen in Section~\ref{sec:diri-multi}, the conjugate prior to the multinomial model is the Dirichlet distribution.
In the standard Bayesian approach, the analyst specifies the parameters $(\nz, \byz)$ of a precise Dirichlet distribution
to model epistemic uncertainty in the alpha-factors, which are the parameters of the multinomial sample model.
%This Dirichlet prior is then updated with observed data to obtain a precise posterior distribution, also Dirichlet.

We will first describe the Basic Parameter Model in its basic form,
and subsequently present its reparametrization in terms of so-called alpha-factors.


\subsection{The Basic Parameter Model}

Consider a system that consists of $k$ components.
Throughout, we make the following standard assumptions:
(i) repair is immediate, and
(ii) failures follow a Poisson process.

For simplicity, we assume that all $k$ components are exchangeable,
in the sense that they have identical failure rates.
More precisely,
we assume that all events involving \emph{exactly} $j$ components failing
have the same failure rate, which we denote by $q_j$.
This model is called the \emph{basic parameter model},
and we write $\vec{q}$ for $(q_1,\dots,q_k)$.

For example, if we have three components, A, B, and C,
then the rate at which we see only A failing
is equal to the rate at which we see only B failing,
and is also equal to the rate at which we see only C failing;
this failure rate is $q_1$.
Moreover, the rate at which we observe only A and B jointly failing
is equal to the rate at which we observe only B and C jointly failing,
and also equal to the rate at which we observe only A and C jointly failing;
this failure rate is $q_2$.
The rate at which we see all three components jointly failing is $q_3$.

In case of $k$ identical components without common-cause failure modes,
thus each failing independently at rate $\lambda$, we would have%
\footnote{This is due to our Poisson assumption, and the assumption of immediate repair:
independent Poisson processes never generate events simultaneously when we observe failure times precisely.}
\begin{equation*}
  q_1=\lambda\qquad \text{ and }\qquad q_j=0\text{ for }j\ge 2.
\end{equation*}
The fact that we allow arbitrary values for the $q_j$ reflects
the lack of independence, and whence, our modelling of common-cause failures.
At this point, it is worth noting that we do not actually write down a statistical model
for all possible common-cause failure modes---we could do so if this information was available,
and in fact, this could render the basic parameter model obsolete,
and allow for more detailed inferences.
In essence, the basic parameter model allows us to statistically model
lack of independence between component failures,
without further detail as to where dependencies arise from:
all failure modes are lumped together, so to speak.

It is useful to note that it is possible, and sometimes necessary,
to relax the exchangeability assumption
to accommodate specific asymmetric cases.
For example, when components are in different state of health,
single failures would clearly not have identical failure rates.
Because the formulas become a lot more complicated,
we stick to the exchangeable case here.

Clearly, to answer typical reliability questions, such as for instance
``what is the probability that two or more components fail in the next month?'',
we need $\vec{q}$.
In practice, the following three issues commonly arise.
First, $\vec{q}$ is rarely measured directly,
as failure data is often collected only per component.
Secondly, when direct data about joint failures is available,
typically, this data is sparse,
because events involving more than two components failing simultaneously are usually quite rare.
Thirdly, there are usually two distinct sources of failure data,
one usually very large data set related to failure per component,
and one usually much smaller data set related to joint failures.
For these reasons,
it is sensible to reparametrise the model in terms
of parameters that can be more easily estimated, as follows.


\subsection{The Alpha-Factor Model}

The alpha-factor parametrisation of the basic parameter model
\cite{1988:mosleh::common:cause} starts out with
considering the total failure rate of a component $q_t$,
which could involve failure of any number of components,
that is, this is the rate obtained by looking at just one component,
ignoring everything else.
Clearly,
\begin{equation}\label{eq:qt:in:terms:of:qj}
  q_t=\sum_{j=1}^k\binom{k-1}{j-1}q_j.
\end{equation}
For example, again consider a three component system, A, B, and C.
The rate at which A fails is then
the rate at which only A fails ($q_1$),
plus the rate at which A and B, or A and C fail ($2q_2$),
plus the rate at which all three components fail ($q_3$).

Next, the alpha-factor model introduces
$\theta_j$---the so-called alpha-factor---which
denotes the probability of \emph{exactly} $j$ of the $k$ components
failing given that failure occurs;
in terms of relative frequency,
$\theta_j$ is the fraction of failures
that involve \emph{exactly} $j$ failed components.
We write $\vec{\theta}$ for $(\theta_1,\dots,\theta_k)$.
Clearly,
\begin{equation}\label{eq:alphaj:in:terms:of:qj}
  \alpha_j=\frac{\binom{k}{j}q_j}{\sum_{\ell=1}^k\binom{k}{\ell}q_\ell}.
\end{equation}
For example, again consider A, B, and C.
Then the rate at which exactly one component fails is $3q_1$
(as we have three single components, each of which failing with rate $q_1$),
the rate at which exactly two components fail is $3q_2$
(as we have three combinations of two components,
each combination failing with rate $q_2$),
and the rate at which all components fail is $q_3$.
Translating these rates into fractions,
we arrive precisely at Eq.~\eqref{eq:alphaj:in:terms:of:qj}.

It can be shown that \cite[Table~C-1, p.~C-5]{1988:mosleh::common:cause}:\footnote{Hint: consider $\sum_{j=1}^k j\alpha_j$.}
\begin{equation}\label{eq:qj:in:terms:of:qt}
  q_j=\frac{1}{\binom{k-1}{j-1}}\frac{j\theta_j}{\sum_{\ell=1}^k \ell\theta_\ell}\,q_t.
\end{equation}
Eqs.~\eqref{eq:qt:in:terms:of:qj}, \eqref{eq:alphaj:in:terms:of:qj},
and~\eqref{eq:qj:in:terms:of:qt}
establish a one-to-one link between the so-called
basic parameter model ($\vec{q}$)
and the alpha-factor model ($q_t$, $\vec{\theta}$).
The benefit of the alpha-factor model over the basic parameter model
lies in its distinction between the total failure rate of a component $q_t$,
for which we generally have a lot of information,
and common-cause failures modelled by $\vec{\theta}$,
for which we generally have very little information.

