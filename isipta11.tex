\section{On Prior-Data Conflict in Predictive Bernoulli Inferences}
\label{sec:isipta11}

This section reproduces the work
``On Prior-Data Conflict in Predictive Bernoulli Inferences'',
published as a peer-reviewed contribution to
\emph{ISIPTA'11: Proceedings of the Seventh International Symposium on
Imprecise Probabilities: Theories and Applications} \parencite{Walter2011a}.
As such, it is reproduced here almost verbatim,
except for some minor shortenings, especially in the Introduction (Section~\ref{sec:isipta11-intro}),
and the addition of some comments and footnotes linking this work to other parts of this thesis.
Furthermore, the notation was changed slightly to assure consistency with the rest of the material presented in this thesis.
Specifically, the success probability in Bernoulli trials is denoted by $\theta$ instead of $p$.

%***with Frank's weighting model!***
\subsubsection*{Abstract}
%***abstract***

By its capability to deal with the multidimensional nature of
uncertainty, imprecise probability provides a powerful methodology
to sensibly handle \pdc\ in Bayesian inference. When there is
strong conflict between sample observations and prior knowledge, the posterior model should be more imprecise
than in the situation of mutual agreement or compatibility. Focusing
presentation on the prototypical example of Bernoulli
trials, we discuss the ability of different approaches to deal with \pdc.

We study a generalised Bayesian setting, including Walley's Imprecise Beta-Binomial model
and his extension to handle prior data conflict (called pdc-IBBM here).
We investigate alternative shapes of prior parameter sets, chosen in a way that shows improved
behaviour in the case of \pdc\ and their influence on the posterior predictive distribution.
Thereafter we present a new approach, consisting of an
imprecise weighting of two originally separate inferences, one of which is based on an informative
imprecise prior, whereas the other one is based on an uninformative imprecise prior. This approach
deals with \pdc\ in a fascinating way.
%***end of abstract***

\subsection{Introduction}
\label{sec:isipta11-intro}

Imprecise probability has shown to be a
powerful methodology to cope with the multidimensional nature of
uncertainty (see the discussion in Sections~\ref{sec:ip-intro} and \ref{sec:motivation}).
Imprecision allows the quality of information, on which probability statements are
based, to be modeled. Well supported knowledge is expressed by comparatively
precise models, while highly imprecise (or even vacuous) models reflect scarce (or no) knowledge
on probabilities. This flexible, multidimensional perspective on uncertainty
modeling has intensively been utilized in generalised Bayesian
inference to overcome the criticism of the arbitrariness of
the choice of single prior distributions in traditional Bayesian inference.%
\footnote{%See the basic discussion of the Bayesian approach to statistical inference in Section~\ref{sec:bayes-inference},
This criticism, subsumed by \textcite[\S 5]{1991:walley} as the ``dogma of precision'',
is discussed more detailed in Section~\ref{sec:motivation:bayesian}.}
In addition, only imprecise probability models react reliably to the presence of
\pdc, i.e.\ situations  where ``the prior [places] its mass primarily on
distributions in the sampling model for which the observed data is
surprising'' \parencite[p.~894]{2006:evans}. Lower and upper probabilities%
\footnote{See Section~\ref{sec:ip-main} for a short exposition of lower and upper previsions,
and related mathematical tools for handling uncertainty in statistical inference.}
allow a specific reaction to \pdc, and offer reasonable inferences if the analyst wishes to
stick to his prior assumptions: starting with the same level of ambiguity in the prior
specification, wide posterior intervals can
reflect conflict between prior and data, while
no \pdc\ will lead to narrow intervals.%
\footnote{See the discussions in Sections~\ref{sec:motivation:pdc} and \ref{sec:pdc-sensitivity},
and the ***work*** on \pdc\ in Section~\ref{sec:jstp}.}
Ideally, the model could provide an extra `bonus' of precision if prior %specifications had been `spot on'.
assumptions are very strongly supported by the data.
Such a model would have the advantage
of (relatively) precise answers when the data confirm prior assumptions,
while still rendering more cautionary answers in the case of \pdc,
thus leading to cautious inferences if, and only if, caution is needed.

Although \textcite[p.~6]{1991:walley} explicitly emphasizes this possibility to
express prior-data conflict as one of the main motivations for imprecise probability,
it has received surprisingly little attention. Rare exceptions include two short sections in \textcite[p.~6 and \S 5.4]{1991:walley}
and the papers by \textcite{1991:pericchi}, \textcite{1994:coolen} and
\textcite{2005:whitcomb}. The popular IDM \parencite{1996:walley::idm, 2009:bernard}
and its generalisation to exponential families \parencite{2005:quaeghebeurcooman} do not reflect prior-data conflict.
\textcite{Walter2009a} used the basic ideas of \textcite[\S 5.4]{1991:walley} to extend the approach of
\textcite{2005:quaeghebeurcooman} to models that show sensitivity to \pdc.

In this ***paper***, a deeper investigation of the issue of \pdc\ is undertaken,
focusing on the prototypic special case of predictive inference in Bernoulli trials:%
\footnote{See also the discussion of the Beta-Binomial model in Section~\ref{sec:beta-binom},
and the imprecise Dirichlet-Multinomial model discussed in several examples in Section~\ref{sec:jstp}.}
We are interested in the posterior predictive
probability for the event that a future Bernoulli random quantity
will have the value $1$, also called a `success'. This event is not
explicitly included in the notation, i.e.\ we simply denote its lower
and upper probabilities by $\Pl$ and $\Pu$, respectively. This future Bernoulli random
quantity is assumed to be exchangeable with the Bernoulli random
quantities whose observations are summarised in the data, consisting
of the number $n$ of observations and the number $s$ of these that are
successes. In our analysis of this model, we
will often consider $s$ as a a real-valued observation in $[0,n]$,
keeping in mind that in reality it can only take on
integer values, but the continuous representation is convenient for
our discussions, in particular in our predictive probability plots (PPP),
where for given $n$, $\Pl$ and $\Pu$ are discussed as functions of $s$.

\medskip

Section~\ref{sec:ibbm-framework} describes a general framework for
generalised Bayesian inference in this setting. The method presented in \textcite[\S 5.4.3]{1991:walley},
called `pdc-IBBM' in this paper, is considered in detail in Section~\ref{sec:ibbm-walley}
and we show that its reaction to \pdc\ can be improved
by suitable modifications of the underlying imprecise
priors. A basic proposal along these lines is discussed in
Section~\ref{sec:othershapes}, with further alternatives
sketched in Section~\ref{sec:ibbm-resume}.
Section~\ref{sec:weightedinf} addresses the problem of \pdc\ from a
completely different angle. There, we combine two originally separate
inferences, one based on an informative imprecise prior and one
on an uninformative imprecise prior, by an imprecise weighting
scheme. The ***paper*** concludes with a brief comparison of the different
approaches in Section~\ref{sec:insights}.


\subsection{Imprecise Beta-Binomial Models}
\label{sec:ibbm}

\subsubsection{The Framework}
\label{sec:ibbm-framework}

%***change notation for success probability from $p$ to $\theta$!!!***

The traditional Bayesian approach for our basic problem is the Beta-Binominal model, which expresses prior
beliefs about the probability $\theta$ of observing a `success' by a Beta
distribution. With\footnote{Our notation relates to
Walley's \parencite*{1991:walley} as $\nz \leftrightarrow s_0$, $\yz \leftrightarrow t_0$.}
$f(\theta\mid\nz, \yz) \propto \theta^{\nz\yz -1} (1-\theta)^{\nz(1-\yz)-1}$,
$\yz = \E[\theta\mid\nz,\yz]$ can be interpreted as prior guess of
$\theta$, while $\nz$ governs the concentration of
probability mass around $\yz$, also known as `pseudo counts' or
`prior strength'.\footnote{As in previous parts of the thesis, ${}\uz$ denotes prior parameters; ${}\un$ posterior parameters.}
These denominations are due to the
role of $\nz$ in the update step: With $s$ successes in $n$ draws observed, the
posterior parameters are\footnote{Compare to Section~\ref{sec:regularconjugates},
Equation~\eqref{eq:canonicalupdate}:
the model is prototypic for conjugate Bayesian
analysis in canonical exponential families, for which updating
of the parameters $\nz$ and $\yz$ can be
written as \eqref{equ:update-nn-yn-precise}.} %\vspace*{-1.3ex}
\begin{align}\label{equ:update-nn-yn-precise}
%\nn &= \nz + n, & \yn &= \frac{\nz}{\nz + n}\,\yz + \frac{n}{\nz + n}\, \frac{s}{n}.
\nn &= \nz + n, & \yn &= \frac{\nz\yz + s}{\nz + n}\,.
\end{align}
Thus $\yn$ is a weighted average of the prior parameter $\yz$ and
the sample proportion $s/n$, and potential prior data conflict is simply averaged out.

Overcoming the dogma of precision, formulating generalised Bayes
updating in this setting is straightforward. By Walley's Generalised
Bayes Rule \parencite[\S 6]{1991:walley}%
\footnote{For more details on the Generalised Bayes' Rule and the generalised Bayesian inference procedure,
see Sections~\ref{sec:gbr} and \ref{sec:imprecisebayes}, respectively.}
the imprecise prior $\MZ$, described by convex sets of precise prior distributions,
is updated to the imprecise posterior $\MN$ obtained by updating $\MZ$ element-wise.
In particular, the convenient conjugate analysis used above can be extended:%
\footnote{See the discussion of the general framework for generalised Bayesian inference
based on sets of conjugate priors in Section~\ref{sec:basicsetting}.}
One specifies a prior parameter set
$\PZ$ of $(\nz,\yz)$ values and takes as imprecise
prior the set $\MZ$ consisting of  all convex mixtures of Beta
priors with $(\nz, \yz) \in \PZ$. In this sense, the set of Beta
priors corresponding to $\PZ$ gives the set of extreme points for
the actual convex set of priors $\MZ$. Updating $\MZ$ with the
Generalised Bayes' Rule results in the convex set $\MN$ of posterior
distributions, that conveniently can be obtained by taking the
convex hull of the set of Beta posteriors, which in turn are defined
by the set of updated parameters
$\PN = \{(\nn,\yn) \mid (\nz, \yz) \in \PZ\}$.
This relationship between the sets $\PZ$ and $\PN$ and the sets $\MZ$
and $\MN$ will allow us to discuss different models $\MZ$ and $\MN$ by depicting
the corresponding parameter sets $\PZ$ and $\PN$. When interpreting our results,
care will be needed with respect to convexity. Although
$\MZ$ and $\MN$ are convex, the parameter sets $\PZ$ and $\PN$
generating them need not necessarily be so. %convex.
Indeed, convexity of the parameter set is not necessarily preserved in the update
step: Convexity of $\PZ$ does not imply convexity of $\PN$.

Throughout, we are interested in the posterior predictive
probability $[\Pl,\Pu]$ for the event that a
future draw is a success. In the Beta-Bernoulli model, this
probability is equal to $\yn$, and we get\footnote{%
\textcite{2005:quaeghebeurcooman}, \textcite{Walter2009a}, and \textcite{Walter2007a} use the prototypical character of
(\ref{equ:update-nn-yn-precise}) underlying (\ref{equ:General-Pl}) and
(\ref{equ:General-Pu}) to generalise this inference to models based on canonical exponential families.
See Section~\ref{sec:generalmodel} and \ref{sec:jstp}. ***leave this out here??***}
%
\begin{align}
\Pl = \ynl &:= \min_{\PN} \yn = \min_{\PZ} \frac{\nz\yz + s}{\nz +n}\,, \label{equ:General-Pl}\\
%           & = \min_{\nz \in [\nzl, \nzu]} \frac{\nz\yzl + s}{\nz +n} & &\text{and} \nonumber\\
\Pu = \ynu &:= \max_{\PN} \yn = \max_{\PZ} \frac{\nz\yz + s}{\nz +n}\,. \label{equ:General-Pu}
%           & = \max_{\nz \in [\nzl, \nzu]} \frac{\nz\yzu + s}{\nz +n}. \nonumber
\end{align}
%\begin{align*}
%\Pl(S^* = 1\mid s) &= \min_{(\nn, \yn) \in \PN} \yn \quad \text{and}\\
%\Pu(S^* = 1\mid s) &= \max_{(\nn, \yn) \in \PN} \yn.
%\end{align*}
%$\Pl(S^* = 1\mid s) = \min_{(\nn, \yn) \in \PN} \yn$ and
%$\Pu(S^* = 1\mid s) = \max_{(\nn, \yn) \in \PN} \yn$.


\subsubsection{Walley's pdc-IBBM}
\label{sec:ibbm-walley}

Special imprecise probability models are now obtained by specific
choices of $\PZ$. If one fixes $\nz$ and varies $\yz$ in an interval $[\yzl,\yzu]$,
Walley's \parencite*[\S 5.3]{1991:walley} model with learning parameter $\nz$ is obtained, which typically
is used in its near-ignorance form $[\yzl, \yzu] \to (0,1)$,
denoted as the imprecise Beta (Binomal/Bernoulli) model (IBBM)%
\footnote{We use `IBBM' also for the model with prior information.},
which is a special case of the popular Imprecise Dirichlet (Multinomial) Model
\parencite{1996:walley::idm,1999:walleybernard}. Unfortunately, in this basic form with fixed $\nz$, the model is
insensitive to prior-data conflict \parencite[p.~263]{Walter2009a} ****.
\textcite[\S 5.4]{1991:walley} therefore generalised this
model by additionally varying $\nz$. In his extended model,
called \emph{pdc-IBBM} in this ***paper***, the set of priors is defined via the
set of prior parameters $\PZ = [\nzl, \nzu] \times [\yzl, \yzu]$,
being a two-dimensional interval, or a rectangle set.
Studying inference in this model, it is important to note that the set of posterior parameters
%$\PN = \{(\nn,\yn) \mid [\nzl, \nzu] \times [\yzl, \yzu]\}$
%$\PN = \{(\nn,\yn) \mid (\nz, \yz) \in \PZero\}$ defining the set
%of ***vertex*** posterior distributions is not rectangular anymore.
$\PN$ is not rectangular anymore. The resulting shapes are illustrated in Figure~\ref{fig:spot-banana}: For the
prior set $\PZ = [1, 5 ] \times  [0.4, 0.7]$---thus assuming a priori the
fraction of successes to be between 40\% and 70\% and rating these assumptions
with at least $1$ and at most $5$ pseudo observations---the resulting posterior parameter sets $\PN$
are shown for data consisting of 3 successes in 6 draws (left) and with all 6 draws successes (right). %***TPDA***. On the right, ***PDC***.
We call the left shape \emph{spotlight}, and the right shape
\emph{banana}. In both graphs, the elements of $\PN$ yielding
%the minimal and maximal $\yn$, $\ynl = \LE[p\mid s]$ and $\ynu = \UE[p\mid s]$,
$\ynl$ and $\ynu$, and thus $\Pl$ and $\Pu$,
are marked with a circle.

%\begin{verbatim}
\begin{figure}%[t]
\begin{tikzpicture}
%\pgftransformscale{0.475}
\pgftransformscale{0.04}
%\include{simpleEx.tex}
\input{fig/spotbanana.tex}
%
\end{tikzpicture}
\caption[Posterior parameter sets $\PN$ for rectangular $\PZ$.]%
{Posterior parameter sets $\PN$ for rectangular $\PZ$. Left: \emph{spotlight} shape; right: \emph{banana} shape.}
\label{fig:spot-banana}
\end{figure}
%\end{verbatim}

The transition point between the \emph{spotlight} and the \emph{banana} shape in Figure~\ref{fig:spot-banana}
is the case when $\frac{s}{n} = \yzu$. Then $\ynu$, being a weighted average
of $\yzu$ and $\frac{s}{n}$, is attained for all $\nz \in [\nzl, \nzu]$,
and the top border of $\PN$ in the graphical representation of Figure~\ref{fig:spot-banana} is constant.
Likewise, $\ynl$ is constant if $\frac{s}{n} = \yzl$.
Therefore, \eqref{equ:General-Pl} and \eqref{equ:General-Pu} can be subsumed as
\begin{align*}
\Pl &= \begin{cases} \frac{\nzu\yzl + s}{\nzu + n} & \text{if } s \geq n \cdot \yzl =: S_1 \\[2ex]
                     \frac{\nzl\yzl + s}{\nzl + n} & \text{if } s \leq n \cdot \yzl =: S_1\end{cases}\,, \\[2ex]
\Pu &= \begin{cases} \frac{\nzu\yzu + s}{\nzu + n} & \text{if } s \leq n \cdot \yzu =: S_2 \\[2ex]
                     \frac{\nzl\yzl + s}{\nzl + n} & \text{if } s \geq n \cdot \yzu =: S_2 \end{cases}\,.
\end{align*}
The interval $[S_1, S_2]$ gives the range of expected successes $[n \cdot \yzl, n \cdot \yzu]$ and
will be called `Total Prior-Data Agreement' interval, or TPDA. For $s$ in the TPDA,
%For the observed number of successes $s$ within the range of expected successes $[n \cdot \yzl, n \cdot \yzu]$,
we are `spot on': $\ynl$ and $\ynu$ are attained for $\nzu$ and $\PN$ has the \emph{spotlight} shape.
But if the observed number of successes is outside TPDA, %the range of expected successes,
$\PN$ goes \emph{bananas} and either $\Pl$ or $\Pu$
%(resulting from updating the one of $\yzl$ and $\yzu$ being nearer to $\frac{s}{n}$***)
is calculated with $\nzl$. %attained for $\nzl$.

\begin{figure}%[h]
\centering %
\begin{tikzpicture}
%\pgftransformscale{0.475}
\pgftransformscale{0.8}
\draw[thick] (15,10) -- (0,10) node[left] {\small 1} -- (0,0) node[left] {\small 0} node[below] (Zero) {\small 0} -- node[below,pos=0.3] {\tikz\draw[-stealth,thick] (0,0) -- (1,0) node[right] {\small $s$};} (15,0) -- cycle; %node[below] {n}
\node[node distance=11.6, base right=of Zero] {\small n};
\coordinate (A) at (0,1);
\coordinate (B) at (0,4.5);
\foreach \point in {A, B}
 \draw[fill] (\point) node[left] {\small $\point$} circle (2pt) ;
%\coordinate (sini) at (0,6);
\coordinate (S1) at (8,0);
\coordinate (S2) at (10,0);
\foreach \point/\name in {S1/S_1, S2/S_2}
 \draw[dashed] (\point) node[below] {\small $\name$} -- ++(0,10);
%
\coordinate (El) at ($(S1) + (0,5)$);
\coordinate (Fu) at ($(S2) + (0,6.5)$);
\coordinate (C) at ($(El) + (7,7/5)$);
\coordinate (D) at ($(Fu) + (5,2.5)$);
\foreach \point in {C, D}
 \draw[fill] (\point) node[right] {\small $\point$} circle (2pt) ;
%
\coordinate (Eu) at ($(B)!{8/10}!(Fu)$);
\foreach \point/\name in {El/E_1, Eu/E_2}
 \draw[fill] (\point) node[above  left=-3pt and -2pt] {\small $\name$} circle (2pt) ;
\coordinate (Fl) at ($(El)!{2/7}!(C)$);
\foreach \point/\name in {Fl/F_1, Fu/F_2}
 \draw[fill] (\point) node[below right=-3pt and -2pt] {\small $\name$} circle (2pt) ;
%
%\draw (B) -- (Fu) -- (D);
%\draw (A) -- (El) -- (C);
\draw (B) -- node[sloped,below] {\small sl.~1} (Eu) -- node[sloped,above,pos=0.55] {\small sl.~1} (Fu) -- node[sloped,above] {\small sl.~2} (D);
\draw (A) -- node[sloped,below] {\small sl.~2} (El) -- node[sloped,below,pos=0.45] {\small sl.~1} (Fl) -- node[sloped,above] {\small sl.~1} (C);
\end{tikzpicture}
%\caption{***error***}
%\caption{$\Pl$ and $\Pu$ for models in Sections~*** }% \ref{sec:ibbm-walley} and \ref{sec:othershapes}.}
\caption{\underline{P}\ and $\Pu$
%Lower and upper predictive probability of success
for models in Sections~\ref{sec:ibbm-walley} and \ref{sec:othershapes}.}
\label{fig:priorset-generic}
\end{figure}

To summarise, the predictive probability plot (PPP),
displaying $\Pl$ and $\Pu$ for $s \in [0, n]$,
is given in Figure~\ref{fig:priorset-generic}.
For the pdc-IBBM, the specific values are
\begin{align*}
S_1 &= n\yzl &
A   &= \frac{\nzl \yzl}{\nzl + n} & 
C   &= \frac{\nzu \yzl + n}{\nzu + n} \\
S_2 &= n\yzu &
B   &= \frac{\nzu \yzu}{\nzu + n} & 
D   &= \frac{\nzl \yzu + n}{\nzl + n} \\
E_1 &= \yzl &
E_2 &= \frac{\nzu \yzu + n \yzl}{\nzu + n} &
\text{sl.~1} &= \frac{1}{\nzu + n} \\
F_2 &= \yzu &
F_1 &= \frac{\nzu \yzl + n \yzu}{\nzu + n} &
\text{sl.~2} &= \frac{1}{\nzl + n} \,.
\end{align*}
As noted by \textcite[p.~224]{1991:walley}, the posterior predictive imprecision $\Delta = \Pu - \Pl$ can be calculated as
\begin{align*}
\Delta &= \frac{\nzu (\yzu - \yzl)}{\nzu + n} + \frac{\nzu - \nzl}{(\nzl + n)(\nzu + n)} \Delta(s, \PZ)\,,
\end{align*}
where $\Delta(s, \PZ) = \inf\{ |s - n \yz| : \yz \in [\yzl, \yzu] \}$ is the distance of
$s$ to the TPDA.
If $\Delta(s, \PZ) \neq 0$, we have an effect of additional imprecision as desired,
increasing linearly in $s$, because $\PN$ is going \emph{bananas}.
However, when considering the fraction of observed successes instead
of $s$, the onset of this additional imprecision immediately if
$\frac{s}{n} \not\in [\yzl, \yzu]$ seems very abrupt. Moreover, and even more severe,
it happens irrespective of the number of trials $n$. When updating successively, this means that all single
Bernoulli observations, being either $0$ or $1$, have to be treated as if being in conflict
(except if $\yzu = 1$ and $s=n$ or if $\yzl = 0$ and $s=0$). Furthermore, regarding $s/n = 7/10$ %the observation of 7 out of 10 trials
as an instance of \pdc\ when $\yzu = 0.6$ had been assumed seems somewhat picky.
To explore possibilities to amend this behaviour, alternative
approaches are explored next.


\subsubsection{\emph{Anteater} Shape Prior Sets}
\label{sec:othershapes}

Choosing a two-dimensional interval $\PZ$ seems logical,
but the resulting inference is not fully satisfactory in case of prior data conflict.
Recall that $\PZ$ is used to produce $\MZ$, %****the set of prior distributions***,
which then is processed by the Generalised Bayes rule. Any shape can be chosen for $\PZ$,
including the composure of single pairs $(\nz, \yz)$. %In this Section, 
Here, we investigate
an alternative shape, with $\yz$ a function of $\nz$, aiming at a more advanced behaviour in the case of \pdc.
To elicit $\PZ$, one could consider a thought experiment:%
\footnote{This strategy is also known as `pre-posterior' analysis in the Bayesian literature.}
Given the hypothetical observation of $s^h$ successes in $n^h$ trials,
which values should $\Pl$ and $\Pu$ take? In other words, what
would one like to learn from data $s^h/n^h$ in accordance with
prior beliefs? As a simple approach, we can define $\PZ$ such
that $\Pl = \cl$ and $\Pu = \cu$ are constants in $\nn = \nz + n^h$.
Then, the lower and upper bounds for $\yz$ must be %\vspace*{-0.5ex}
\begin{equation} \label{equ:anteater-y0}
\begin{aligned}
&
\yzl(\nz) &= \frac{(n^h + \nz)\cl - s^h}{\nz}\,, \\
%\yzl(\nz) &= \big((n^h + \nz)\cl - s^h\big)/\nz\,, \\
&
\yzu(\nz) &= \frac{(n^h + \nz)\cu - s^h}{\nz}\,,
%\yzu(\nz) &= \big((n^h + \nz)\cu - s^h\big)/\nz\,,
\end{aligned}
\end{equation}
%for $\nz$ in an interval $[\nzl, \nzu]$ that must be elicited as well.
for $\nz$ in an interval $[\nzl, \nzu]$ derived by the range $[\nnl, \nnu]$ one wishes to attain for $\Pl$ and $\Pu$
given the $n^h$ hypothetical observations.%
\footnote{For the rest of the ***paper***, we tacitly assume that $n^h$, $s^h$, $\nz$ and $\cl$/$\cu$
are chosen such that $\yzl \geq 0$ resp.\ $\yzu \leq 1$ to generate Beta distributions as priors.}
% or by its own right as prior range of confidence.
The resulting shape of $\PZ$ is as in Figure~\ref{fig:anteater-ex1} (left) and called \emph{anteater} shape.
%
\begin{figure}%[h]
\begin{tikzpicture}
%\pgftransformscale{0.475}
\pgftransformscale{0.04}
%\include{simpleEx.tex}
\input{fig/anteater1a.tex}
%
\end{tikzpicture}
\caption{$\PZ$ and $\PN$ for the \emph{anteater} shape.}
\label{fig:anteater-ex1}
\end{figure}
%
Rewriting \eqref{equ:anteater-y0}, $\PZ$ is now defined by %\vspace*{-1ex}
%\begin{multline*}
\begin{align*}
%\{ (\nz, \yz) \mid \nz &\in [\nzl, \nzu],\,\\
%                   \yz &\in [ \cl - \frac{n^h}{\nz}\Big(\frac{s^h}{n^h} - \cl\Big),
%                              \cu + \frac{n^h}{\nz}(\cu -\frac{s^h}{n^h})  ] \}\,,
%\hspace*{-2ex}%
\PZ &= 
\Big\{ \Big. (\nz, \yz) \,\Big|\, \nz \in [\nzl, \nzu],\, %\\
%\hspace*{3ex}     
                  \yz(\nz) \in \Big[ \cl - \frac{n^h}{\nz}\Big(\frac{s^h}{n^h} - \cl\Big),\,
                                     \cu + \frac{n^h}{\nz}\Big(\cu - \frac{s^h}{n^h}\Big)  \Big] \Big\}\,.
\end{align*}
%\end{multline*}
With the reasonable choice of $\cl$ and $\cu$ such that $\cl \leq s^h/n^h \leq \cu$,
$\PZ$ can be interpreted as follows: The range of $\yz$
protrudes over $[\cl, \cu]$
on either side far enough to ensure $\Pl = \cl$ and $\Pu = \cu$ if
updated with $s = s^h$ for $n = n^h$, the amount of protrusion
decreasing in $\nz$ as the movement of $\yz(\nz)$ towards $s^h/n^h$ is slower
for larger values of $\nz$. As there is a considerable difference in behaviour if
$n > n^h$ or $n < n^h$, these two cases are discussed separately.

If $n > n^h$, the PPP graph in Figure~\ref{fig:priorset-generic}
holds again, now with the values
\begin{align*}
A   &= \frac{\cl (\nzl + n^h) - s^h}    {\nzl + n} & % \frac{\nzl \yzl}{\nzl + n} &
S_1 &= s^h + \cl (n - n^h) \\ % n\yzl &
B   &= \frac{\cu (\nzu + n^h) - s^h}    {\nzu + n} & % \frac{\nzu \yzu}{\nzu + n} \\
S_2 &= s^h + \cu (n - n^h) \\ % n\yzu \\
%\end{align*}%\vspace*{-4ex}
%\begin{align*}
C   &= \frac{\cl (\nzu + n^h) - s^h + n}{\nzu + n} & % \frac{\nzu \yzl + n}{\nzu + n} &
\text{sl.~1} &= 1/(\nzu + n) \\
D   &= \frac{\cu (\nzl + n^h) - s^h + n}{\nzl + n} &  % \frac{\nzl \yzu + n}{\nzl + n} \\
\text{sl.~2} &= 1/(\nzl + n)
%\text{sl.~1} &= \frac{1}{\nzu + n} &
%\text{sl.~2} &= \frac{1}{\nzl + n}
\end{align*}%\vspace*{-4ex}
\begin{align*}
E_1 &= \cl &
E_2 &= \cl + \frac{\nzu + n^h }{\nzu + n} (\cu - \cl) %&
     = \cu - \frac{ n   - n^h }{\nzu + n} (\cu - \cl) \\ % \frac{\nzu \yzu + n \yzl}{\nzu + n} \\
F_2 &= \cu &
F_1 &= \cu - \frac{\nzu + n^h }{\nzu + n} (\cu - \cl) %&
     = \cl + \frac{ n   - n^h }{\nzu + n} (\cu - \cl) \,.% \frac{\nzu \yzl + n \yzu}{\nzu + n} &
\end{align*}
As for the pdc-IBBM, the TPDA boundaries $S_1$ and $S_2$ mark the transition points
where either $\ynl$ or $\ynu$ are constant in $\nz$. We now have
\begin{align*}
\frac{S_1}{n} &= \cl + \frac{n^h}{n}\Big( \frac{s^h}{n^h} - \cl \Big), &
\frac{S_2}{n} &= \cu - \frac{n^h}{n}\Big( \cu - \frac{s^h}{n^h} \Big),
\end{align*}
so this TPDA is a subset of $[\cl, \cu]$. The \emph{anteater}
shape is, for $n>n^h$, even more strict than the pdc-IBBM, as, e.g.,
$\yzl(\nzu) = \cl - \frac{n^h}{\nzu}\big( \frac{s^h}{n^h} - \cl \big) < \frac{S_1}{n}$.


The situation for $n < n^h$ is illustrated in Figure~\ref{fig:anteater-nsmall},
where $A$, $B$, $C$, $D$, $E_1$, $F_2$ and slopes 1 and 2 are the same as for $n > n^h$, but
\begin{align*}
E_2 &= \cl + \frac{\nzl + n^h }{\nzl + n} (\cu - \cl) %&
     = \cu + \frac{ n^h - n   }{\nzl + n} (\cu - \cl) \,,\\ % \frac{\nzu \yzu + n \yzl}{\nzu + n} \\
F_1 &= \cu - \frac{\nzl + n^h }{\nzl + n} (\cu - \cl) %&
     = \cl - \frac{ n^h - n   }{\nzl + n} (\cu - \cl) \,.%    \frac{\nzu \yzl + n \yzu}{\nzu + n} &
\end{align*}
Note that now $S_2<S_1$, so the TPDA is $[S_2, S_1]$. In this interval, $\Pl$ and $\Pu$ are now
calculated with $\nzl$; for $s \not\in [S_2, S_1]$ the same situation
as for $n > n^h$ applies, with the bound nearer to $s/n$
calculated with $\nzl$ and the other with $\nzu$.

\begin{figure}%[h]
\centering %
\begin{tikzpicture}
\pgftransformscale{0.8}
%\pgftransformscale{0.475}
\draw[thick] (15,10) -- (0,10) node[left] {\small 1} -- (0,0) node[left] {\small 0} node[below] (Zero) {\small 0} -- node[below,pos=0.3] {\tikz\draw[-stealth,thick] (0,0) -- (1,0) node[right] {\small $s$};} (15,0) -- cycle; %node[below] {n}
\node[node distance=11.6, base right=of Zero] {n};
\coordinate (S2) at (8,0);
\coordinate (S1) at (10,0);
\foreach \point/\name in {S1/S_1, S2/S_2}
 \draw[dashed] (\point) node[below] {\small $\name$} -- ++(0,10);
\coordinate (A) at (0,0.5);
\coordinate (El) at ($(S1) + (0,5.5)$);
\coordinate (C) at ($(El) + (5,1)$);
\coordinate (Fl) at ($(A)!{8/10}!(El)$);
%
\coordinate (D) at (15,9.5);
\coordinate (Fu) at ($(S2) + (0,6)$);
\coordinate (B) at ($(Fu) + (-8,-8/5)$);
\coordinate (Eu) at ($(Fu)!{2/7}!(D)$);
%
\draw[fill] (A) node[above left=-3pt and 0pt] {\small $A$} circle (2pt) ;
\draw[fill] (B) node[left] {\small $B$} circle (2pt) ;
\foreach \point in {C, D}
 \draw[fill] (\point) node[right] {\small $\point$} circle (2pt) ;
\foreach \point/\name in {El/E_1, Eu/E_2}
 \draw[fill] (\point) node[below right=-3pt and -2pt] {\small $\name$} circle (2pt) ;
\foreach \point/\name in {Fl/F_1, Fu/F_2}
 \draw[fill] (\point) node[above  left=-3pt and -2pt] {\small $\name$} circle (2pt) ;
\draw (B) -- node[sloped,below] {\small sl.~1} (Fu) -- node[sloped,above,pos=0.55] {\small sl.~2} (Eu) -- node[sloped,above] {\small sl.~2} (D);
\draw (A) -- node[sloped,below] {\small sl.~2} (Fl) -- node[sloped,below,pos=0.45] {\small sl.~2} (El) -- node[sloped,above] {\small sl.~1} (C);
\end{tikzpicture}
\caption{%$\Pl$ and $\Pu$
\underline{P}\ and $\Pu$
%Lower and upper predictive probability of success
for the \emph{anteater} shape if $n < n^h$.}
\label{fig:anteater-nsmall}
\end{figure}
%
%

\begin{figure}%[h]
\begin{tikzpicture}
%\pgftransformscale{0.475}
\pgftransformscale{0.04}
%\include{simpleEx.tex}
\input{fig/anteater2a.tex}
\draw[-stealth, thick] (70,115) -- (180,115);
%\draw (0,0) rectangle (400, 220);
%
\end{tikzpicture}
\caption[Posterior parameter sets $\PN$ for \emph{anteater} prior sets $\PZ$.]%
{Posterior parameter sets $\PN$ for \emph{anteater} prior sets $\PZ$.
Left: the transition point where %$\ynl$
the lower contour of the posterior parameter set 
is attained for all $\nn$, right: the \emph{banana} shape.}
\label{fig:anteater-ex2}
\end{figure}



%The case of $n < n^h$ seems the most useful,
%Considering the upper switch point $S_1$ (analogue findings hold for $S_2$),
The upper transition point $S_1$ can now be between $\yzu(\nzl)$ and $\yzu(\nzu)$, and having
$S_1$ decreasing in $n$ now makes sense: the smaller $n$, the larger $S_1$, i.e.\
the more tolerant is the \emph{anteater} set.
The switch over $S_1$ (with $s/n$ increasing)
%The switch over the upper transition point $S_1$ (with $s/n$ increasing) \com{in the case of $n < n^h$}
is illustrated in the three graphs
in Figures~\ref{fig:anteater-ex1} (right) and \ref{fig:anteater-ex2} (left, right):
First, $\PZ$ from Figure~\ref{fig:anteater-ex1} (left) is updated with
$s/n = 3/6 < S_1/n$, leading again to an \emph{anteater} shape, %\com{ (albeit less pronounced)},
%($\PN$ would retain the \emph{anteater} shape only if $n < n^h$, and be rectangle with $n = n^h$ and $s = s^h$),
and so we get $\Pl$ and $\Pu$ from the elements of $\PN$ at $\nnl$, as marked with circles.
Second, the transition point is reached for $s = S_1 = 4.27$,
and now $\Pl$ is attained for any $\nn \in [\nnl, \nnu]$, as emphasized by the arrow.
Third, as soon as $s$ exceeds $S_1$ (in the graph: $s/n = 6/6$),
it holds that $\ynl(\nnl) > \ynl(\nnu)$, and $\Pl$ is now attained at $\nnu$.
As for the pdc-IBBM, for $s$ outside the TPDA $\PN$ goes \emph{bananas},
leading to additional imprecision.
The imprecision $\Delta = \Pu - \Pl$ if $n < n^h$ is
\begin{align*}
\Delta &= \frac{\nzl\! + n^h}{\nzl\! + n} (\cu - \cl) + \frac{\nzu - \nzl}{(\nzl\! + n)(\nzu\! + n)} \Delta(s, n, \vec{c}),
\end{align*}
where $\Delta(s, n, \vec{c}) = n \big|c^* - \frac{s}{n} \big| - n^h \big| c^* - \frac{s^h}{n^h} \big|$,
and $c^* =  \arg\max_{c \in [\cl, \cu]} |\frac{s}{n} - c|$ is the boundary of $\vec{c} := [\cl, \cu]$ with the largest distance to $s/n$.
For $s \in [S_2, S_1]$, $\Delta(s, n, \vec{c}) = 0$,
giving a similar structure as for the pdc-IBBM, except that
$\Delta(s, n,\vec{c})$ does not directly give the distance of $s/n$ to $\PZ$ but is based on $[\cl, \cu]$.
The imprecision increases again linearly with $s$,
but now also with $n$. The distance of $s/n$ to the opposite bound of $[\cl, \cu]$
(weighted with $n$) is discounted by the distance of $s^h/n^h$ to the same bound
(weighted with $n^h$). In essence, $\Delta(s, n, \vec{c})$ is thus a reweighted distance of $s/n$ to $s^h/n^h$.
The more dissimilar these fractions are, the larger the posterior predictive imprecision is.

% now again $n \geq n^h$:

For $n = n^h$, $S_1 = S_2 = s^h$, so the TPDA is reduced to a single point. In this case, the \emph{anteater}
shape can be considered as an equilibrium point,
with any $s \neq s^h$ leading to increased posterior imprecision.
In this case, the weights in $\Delta(s, n, \vec{c})$ coincide, and so
the posterior imprecision depends directly on $|s-s^h|$.

For $n > n^h$ the transition behaviour is as for the pdc-IBBM:
As long as $s \in [S_1, S_2]$, $\PN$ has the \emph{spotlight} shape,
where both $\Pl$ and $\Pu$ are calculated with $\nnu$; $\Delta$ for $s \in [S_1, S_2]$ is
thus calculated with $\nnu$ as well.
If, e.g.,\ $s > S_2$, $\Pu$ is attained with $\nnl$, and $\Delta(s, n, \vec{c})$
gives directly the distance of $s/n$ to $s^h/n^h$, the part of which is
inside $[\cl, \cu]$ is weighted with $n$, and the remainder with $n^h$.
Table~\ref{tab:anteater-shapes} provides an overview of the possible shapes of $\PN$.

%When instead $s \neq [S_2, S_1]$ (or $s \neq [S_1, S_2]$ if $n > n^h$),
%$\PN$ is `going bananas' just as $\PN$ in the pdc-IBBM.
%
%***The posterior parameter set $\PN$ may have the following shapes:
\begin{table}
\centering
\begin{tabular}{c|ccc}
%          & $s \in$ \parbox{6ex}{left\\
%                                 zone} & $s \in$ \parbox{6ex}{center\\
%                                                              zone} & $s \in$ \parbox{6ex}{right\\
%                                                                                           zone} \\
%\hline
%$n > n^h$ & \parbox{07ex}{\centering $s < S_1$\\
%                          banana}      & \parbox{12ex}{\centering $s \in [S_1, S_2]$\\
%                                                       spotlight}   & \parbox{07ex}{\centering $s > S_2$\\
%                                                                                    banana} \\
%\hline
%$n = n^h$ & \parbox{07ex}{\centering $s < s^h$\\
%                          banana}      & \parbox{12ex}{\centering $s = s^h$\\
%                                                       rectangular} & \parbox{07ex}{\centering $s > s^h$\\
%                                                                                    banana}\\
%\hline
%$n < n^h$ & \parbox{07ex}{\centering $s < S_2$\\
%                          banana}      & \parbox{12ex}{\centering $s \in [S_2, S_1]$\\
%                                                       anteater}    & \parbox{07ex}{\centering $s > S_1$\\
%                                                                                    banana}
\multirow{2}{*}{$n > n^h$} & $s < S_1$ & $s \in [S_1, S_2]$ & $s > S_2$ \\
                           & banana    & spotlight          & banana \\
\hline
\multirow{2}{*}{$n = n^h$} & $s < s^h$ & $s = s^h$          & $s > s^h$ \\
                           & banana    &  rectangular       &  banana \\
\hline
\multirow{2}{*}{$n < n^h$} & $s < S_2$ & $s \in [S_2, S_1]$ & $s > S_1$ \\
                           & banana    & anteater           &  banana
\end{tabular}
\caption{Shapes of $\PN$ if $\PZ$ has the \emph{anteater} shape.}
\label{tab:anteater-shapes}
\end{table}


\subsubsection{Intermediate R\'{e}sum\'{e}}
\label{sec:ibbm-resume}

%***also here two slopes, dead flamingo, other shapes possible.
%conjecture: arbitrary number of slopes are possible with careful selection of shape:
Despite the (partly) different behaviour inside the TPDA, both %the
pdc-IBBM and the \emph{anteater} shape display only two different slopes in their PPPs
(Figures~\ref{fig:priorset-generic} and \ref{fig:anteater-nsmall}),
with either $\nnl$ or $\nnu$ used to calculate $\Pl$ and $\Pu$.
It is possible to have shapes such that for some $s$ other
values from $[\nnl, \nnu]$ are used. As a toy example, consider
$\PZ = \{ (1, 0.4), (3, 0.6), (5, 0.4) \}$, so consisting only of
three parameter combinations $(\nz, \yz)$. $\Pu$ is then derived
as $\ynu = \max \{ \frac{0.4 + s}{1 + n}, \frac{1.8 + s}{3 + n}, \frac{2 + s}{5 + n} \}$,
leading to
\begin{align*}
\ynu &= \begin{cases} \frac{0.4 + s}{1 + n} & \text{if }               s > 0.7 n + 0.3 \\
                      \frac{1.8 + s}{3 + n} & \text{if } 0.1 n - 1.5 < s < 0.7 n + 0.3 \\
                      \frac{2 + s}{5 + n}   & \text{if } s < 0.1 n - 1.5 \end{cases}\,.
\end{align*}
So, in a PPP %graph like Figure~\ref{fig:priorset-generic} or \ref{fig:anteater-nsmall},
we would observe the three different slopes $1/(1+n)$, $1/(3+n)$ and $1/(5+n)$
depending on the value of s. Our conjecture is therefore that with carefully tailored
sets $\PZ$, an arbitrary number of slopes is possible, and so even smooth curvatures.
Using a thought experiment as for the \emph{anteater} shape, $\PZ$ shapes can be derived to
fit any required behaviour. Another approach for constructing a $\PZ$ that is more tolerant
with respect to \pdc\ could be as follows: As the onset of additional
imprecision in the pdc-IBBM is caused by the fact that $\ynu(\nnl) > \ynu(\nnu)$
as soon as $s/n > \yzu$, we could define the $\yz$ interval at $\nzl$ %$[\yzl(\nzl), \yzu(\nzl)]$
to be narrower than the $\yz$ interval at $\nzu$, %$[\yzl(\nzu), \yzu(\nzu)]$,
so that the \emph{banana} shape results only when $s/n$ exceeds $\yzu(\nzu)$ far enough.
Having a narrower $\yz$ interval at $\nzl$ than at $\nzu$ could also make
sense from an elicitation point of view: We might be able
to give quite a precise $\yz$ interval for a low prior strength $\nzl$,
whereas for a high prior strength $\nzu$ we must be more cautious
with our elicitation of $\yz$, i.e.\ giving a wider interval.
The rectangular shape for $\PZ$ as discussed in
Section~\ref{sec:ibbm-walley} seems thus somewhat peculiar. One
could also argue that if one has substantial prior information, but
acknowledges that this information may be wrong, one should not
reduce the weight of the prior $\nz$ on the posterior while keeping
the same informative interval of values of $\yz$.
%the proportion of successes reflecting the prior information.

Generally, the actual shape of a set $\PZ$ influences the inferences,
but for a specific inference, only a few aspects of the set are relevant. So, while
a detailed shape of a prior set may be very difficult to elicit, it may not
even be that relevant for a specific inference. A further general issue seems unavoidable in the
generalised Bayesian setting as developed here, namely the dual role of $\nz$. On the one
hand, $\nz$ governs the weighting of prior information $\yz$ with
respect to the data $s/n$, as mentioned in
Section~\ref{sec:ibbm-framework}: The larger $\nz$, the more
$\Pl$ and $\Pu$ are dominated by $\yzl$ and $\yzu$. On
the other hand, $\nz$ governs also the degree of posterior
imprecision: the larger $\nz$, the larger c.p.\ $\Delta$. A larger
$\nz$ thus leads to more imprecise posterior inferences, although a
high weight on the supplied prior information should boost the trust
in posterior inferences if $s$ in the TPDA, i.e.\ the prior
information turned out to be appropriate. In the next section,
we thus develop a different approach separating these two
roles: Now, two separate models for predictive inference, each
%weighting prior and data information with $\nz$,
resulting in different precision as governed by $\nz$,
are combined with an imprecise weight $\alpha$ %, with $\alpha$
taking the role of regulating prior-data agreement.


\subsection{Weighted Inference}
\label{sec:weightedinf}

We propose a variation of the Beta-Binomial model
that is attractive for prior-data conflict
and has small yet fascinating differences with the models in
Sections~\ref{sec:ibbm-walley} and \ref{sec:othershapes}. We present a basic
version of the model in Section~\ref{wi-basic}, followed by an
extended version in Section~\ref{wi-ext}. Opportunities to
generalize the model are mentioned in
Section~\ref{wi-prop}.
%In line with the other sections of this paper,
%we focus on the event that, following $s$ successes in $n$
%observations, the next observation is a success. The model proposed
%here is motivated by two main observations related to the models discussed in Sections ********************** 2 and 3.


\subsubsection{The Basic Model}
\label{wi-basic}

The idea for the proposed model is to combine the inferences
based on two models, each part of an imprecise
Bayesian inferential framework using sets of prior distributions,
although the inferences can also result from alternative inferential
methods. The combination is not achieved by
combining the two sets of prior distributions into a single set, but
by combining the posterior predictive \emph{inferences}
by imprecise weighted averaging. When the weights assigned to the
two models can vary over the whole range $[0,1]$ we actually return
to imprecise Bayesian inference with a prior set, as considered in this subsection.
In Section~\ref{wi-ext} we
restrict the values of the model weights. The basic model
turns out to be relevant from many perspectives, in
particular to highlight similarities and differences with the
methods presented in Sections~\ref{sec:ibbm-walley} and
\ref{sec:othershapes}, and it is a suitable starting point for more
general models. These aspects will be discussed in
Subsection~\ref{wi-prop}.

We consider the combination of the imprecise posterior
predictive probabilities $[\Pl^i, \Pu^i]$ and $[\Pl^u, \Pu^u]$ for
the event that the next observation is a success with
\begin{align}
\Pl^i &= \frac{s^i+s}{n^i+n+1} & \text{and }& & \Pu^i &= \frac{s^i+s+1}{n^i+n+1}\,,  \label{ilowupp} \\
\Pl^u &= \frac{s}{n+1}         & \text{and }& & \Pu^u &= \frac{s+1}{n+1}        \,.  \label{ulowupp}
\end{align}
The superscript $i$ indicates `informative', in the sense
that these lower and upper probabilities relate to an `informative'
prior distribution reflecting prior
beliefs of similar value as $s^i$ successes in $n^i$ observations.
The superscript $u$ indicates `uninformative', which can be interpreted
as absence of prior beliefs. These
lower and upper probabilities can for example result from
Walley's IBBM, with $\Pl^i$ and $\Pu^i$ based on the prior set with
$\nz = n^i+1$ and $\yz \in \left[\frac{s^i}{n^i+1},\frac{s^i+1}{n^i+1}\right]$,
and $\Pl^u$ and $\Pu^u$ on the prior set with $\nz = 1$ and $\yz \in [0,1]$.
There are other methods for imprecise statistical inference that
lead to these same lower and upper probabilities, including
Nonparametric Predictive Inference for Bernoulli quantities
\parencite{1998:coolen}\footnote{See also \url{www.npi-statistics.com}.},
where the $s^i$ and $n^i$ would only be included if they were actual
observations, for example resulting from a second data set that one
may wish to include in the `informative' model but not in the `uninformative' model.

The proposed method combines these
lower and upper predictive probabilities by imprecise weighted averaging.
Let $\alpha \in [0,1]$, we define%\vspace*{-1.0ex}
%\begin{align} %\label{alpha-lowupp}
%\Pl_{\alpha} &= \alpha \Pl^i + (1-\alpha) \Pl^u \label{alpha-low} \,,\\
%\Pu_{\alpha} &= \alpha \Pu^i + (1-\alpha) \Pu^u \label{alpha-upp} \,,
%\end{align}
\begin{align} \label{alpha-lowupp}
\Pl_{\alpha}\! &= \alpha \Pl^i + (1-\alpha) \Pl^u, &
\Pu_{\alpha}\! &= \alpha \Pu^i + (1-\alpha) \Pu^u,
\end{align}
and as lower and upper predictive probabilities for the event that
the next Bernoulli random quantity is a success,\footnote{%
While in \eqref{equ:General-Pl} and \eqref{equ:General-Pu}, prior
and sample information are imprecisely weighted, here informative
and uninformative models are combined.}%\vspace*{-1.0ex}
%
\begin{align*}
\Pl &= \min_{\alpha \in [0,1]} \Pl_{\alpha} & \text{and}& & % \label{P-low} \\
\Pu &= \max_{\alpha \in [0,1]} \Pu_{\alpha} \,.             % \label{P-upp}
\end{align*}
Allowing $\alpha$ to take on any value in $[0,1]$ reduces
this method to the IBBM %imprecise Beta-Binomial model
with a single prior set, as discussed in Section~\ref{sec:ibbm}, with the prior
set simply generated by the union of the two prior sets for the
`informative' and the `uninformative' models as described above.
For all $s$ these minimum and maximum values
are obtained at either $\alpha=0$
or $\alpha=1$. With switch points $S_1=(n+1)\frac{s^i}{n^i}-1$ and
$S_2=(n+1)\frac{s^i}{n^i}$, they are equal to%\vspace*{-0.5ex}
%\begin{eqnarray*}
%\Pl &=& \left\{
%  \begin{array}{ll}
%    \Pl^u = \frac{s}{n+1}          \;\; &\mbox{ if } s \leq S_2  \\
%    \Pl^i = \frac{s^i+s}{n^i+n+1}  \;\; &\mbox{ if } s \geq S_2
%  \end{array} \right. \\
%\Pu &=& \left\{
%  \begin{array}{ll}
%    \Pu^i = \frac{s^i+s+1}{n^i+n+1} \;\; &\mbox{ if } s \leq S_1  \\
%    \Pu^u = \frac{s+1}{n+1}         \;\; &\mbox{ if } s \geq S_1
%  \end{array} \right.
%\end{eqnarray*}
\begin{align*}
\Pl &=
  \begin{cases}
    \Pl^u = \frac{s}{n+1}           & \text{if } s \leq S_2  \\
    \Pl^i = \frac{s^i+s}{n^i+n+1}   & \text{if } s \geq S_2 
  \end{cases}\,, & 
\Pu &=
  \begin{cases}
    \Pu^i = \frac{s^i+s+1}{n^i+n+1} & \text{if } s \leq S_1  \\
    \Pu^u = \frac{s+1}{n+1}         & \text{if } s \geq S_1 
  \end{cases} \,.
\end{align*}
%Figure~\ref{fig:weighted-generic} presents such lower and upper
%probabilities as functions of $s$ (represented as a continuous
%variable on $[0,n]$) in a generic way.
The PPP graph for this model is displayed %presented
in Figure~\ref{fig:weighted-generic}.
%***the following par can be shortened*** Note that the lower probability
%$\Pl$ is made up of two line segments, one from $s=0$ to $s=S_2$ and
%a second line segment, with smaller slope, for the larger values
%until $s=n$. The upper probability $\Pu$ is also made up of two line
%segments, one from $s=0$ to $s=S_1$ and a second line segment, with
%larger slope, for the larger values until $s=n$. These switch points
%are the same for the special cases of the weighted inference method
%as discussed in detail in this section.
The upper probability for $s=S_1$ and the lower probability for
$s=S_2$ are both equal to $\frac{s^i}{n^i}$. The TPDA
contains only a single possible value of $s$ (except if $S_1$ and $S_2$ are
integer), namely the one that is nearest to $\frac{s^i}{n^i}$. The specific
values for this basic case are %\vspace*{-0.5ex}
%$A=0$, $B=\frac{s^i+1}{n^i+n+1}$,
%$C=\frac{s^i+n}{n^i+n+1}$, $D=1$, $E=\frac{s^i}{n^i}-\frac{1}{n+1}$,
%$F=\frac{s^i}{n^i}+\frac{1}{n+1}$, $\text{sl.~1}=\frac{1}{n^i+n+1}$,
%$\text{sl.~2}=\frac{1}{n+1}$.
\begin{align*}
A &= 0 &
B &= \frac{s^i+1}{n^i+n+1} &
C &= \frac{s^i+n}{n^i+n+1} \\
D &= 1 &
E &= \frac{s^i}{n^i}-\frac{1}{n+1} &
F &= \frac{s^i}{n^i}+\frac{1}{n+1} \\
& &
\text{sl.~1} &= \frac{1}{n^i+n+1} &
\text{sl.~2} &= \frac{1}{n+1}\,.
\end{align*}
If $s$ is in the TPDA, it reflects optimal
agreement of the `prior data' $(n^i,s^i)$ and the (really
observed) data $(n,s)$, so it may be a surprise that both the
lower and upper probabilities in this case correspond to $\alpha=0$,
so they are fully determined by the `uninformative' part of the
model. This is an important aspect, it will be discussed in more
detail and compared to the methods of Section~\ref{sec:ibbm} in
Subsection \ref{wi-prop}. For $s$ in the TPDA, both
$\Pl$ and $\Pu$ increase with slope $\frac{1}{n+1}$, and imprecision $\Delta = \frac{1}{n+1}$.

Figure~\ref{fig:weighted-generic}, with the specific values for this
basic case given above, illustrates what happens for values of $s$
outside this TPDA. Moving away from the TPDA in
either direction, the imprecision increases, as was also the case in
the models in Section~\ref{sec:ibbm}. For $s$ decreasing towards
$0$, this is effectively due to the smaller slope of the upper
probability, while for $s$ increasing towards 1 it is due to the
smaller slope of the lower probability. For $s\in [0,S_1]$, the
imprecision is $\Delta = \frac{s^i+1}{n^i+n+1} - \frac{sn^i}{(n^i+n+1)(n+1)}$.
For $s\in [S_2,n]$, the imprecision is $\Delta = \frac{1}{n+1} - \frac{s^i}{n^i+n+1} + \frac{sn^i}{(n^i+n+1)(n+1)}$.
For the two extreme possible cases of prior data conflict, with
either $s^i=n^i$ and $s=0$ or $s^i=0$ and $s=n$, the imprecision is
$\Delta = \frac{n^i+1}{n^i+n+1}$. For this combined model with
$\alpha \in [0,1]$, we have $\Pl \leq \frac{s}{n} \leq \Pu$ for all
$s$, which is attractive from the perspective of objective inference.

\begin{figure}%[h]
\centering %
\begin{tikzpicture}
%\pgftransformscale{0.475}
\pgftransformscale{0.8}
\draw[thick] (15,10) -- (0,10) node[left] {1} -- (0,0) node[left] {\small 0} node[below] (Zero) {\small 0} -- node[below,pos=0.3] {\tikz\draw[-stealth,thick] (0,0) -- (1,0) node[right] {\small $s$};} (15,0) -- cycle; %node[below] {n}
\node[node distance=11.6, base right=of Zero] {\small n};
\coordinate (A) at (0,1);
\coordinate (B) at (0,5);
\foreach \point in {A, B}
 \draw[fill] (\point) node[left] {\small $\point$} circle (2pt) ;
\coordinate (sini) at (0,6);
\coordinate (S1) at (8,0);
\coordinate (S2) at (10,0);
\foreach \point/\name in {S1/S_1, S2/S_2}
 \draw[dashed] (\point) node[below] {\small $\name$} -- ++(0,10);
%
\coordinate (Eu) at ($(S1) + (sini)$);
%\draw[fill] (Eu) node[above] {Eu} circle (3pt) ;
\coordinate (Fl) at ($(S2) + (sini)$);
%\draw[fill] (Fl) node[above] {Fl} circle (3pt) ;
\coordinate (El) at ($(A)!{8/10}!(Fl)$);
\draw[fill] (El) node[above  left=-3pt and -2pt] {\small $E$} circle (2pt) ;
\coordinate (Fu) at ($(Fl) + (0,1)$);
\draw[fill] (Fu) node[below right=-3pt and -2pt] {\small $F$} circle (2pt) ;
%
%\begin{scope}[label distance=-2.5mm]
%\draw[fill] (El) node[label=160:$E$] {} circle (2pt) ;
%\draw[fill] (Fu) node[label=-20:$F$] {} circle (2pt) ;
%\end{scope}
%
\coordinate (D) at ($(Fu) + (5,2.5)$);
\coordinate (C) at ($(Fl) + (5,5/8)$);
\foreach \point in {C, D}
 \draw[fill] (\point) node[right] {\small $\point$} circle (2pt) ;
%
\draw[dashed] (Fl) -- (sini) node[left] {\small $\frac{s^i}{n^i}$};
%
\draw (B) -- node[sloped,below] {\small sl.~1} (Eu) -- node[sloped,above,pos=0.6] {\small sl.~2} (Fu) -- node[sloped,above] {\small sl.~2} (D);
\draw (A) -- node[sloped,below] {\small sl.~2} (El) -- node[sloped,below,pos=0.4] {\small sl.~2} (Fl) -- node[sloped,above] {\small sl.~1} (C);
\end{tikzpicture}
\caption{%$\Pl$ and $\Pu$
\underline{P}\ and $\Pu$
%Lower and upper predictive probability of success
for the weighted inference model.}
\label{fig:weighted-generic}
\end{figure}

There are important further aspects of this model to be discussed,
in particular how to meaningfully choose $s^i$ and $n^i$, but this
is postponed until Section~\ref{wi-prop}, where we will start
discussing these aspects by again focussing first on this basic
model followed by discussion of an extended version of this model,
which is introduced next.


\subsubsection{The Extended Model}
\label{wi-ext}

We extend the basic model from Subsection~\ref{wi-basic}, perhaps
remarkably by reducing the interval for the weighting variable
$\alpha$. We assume that $\alpha \in [\alpha_l,\alpha_r]$ with $0
\leq \alpha_l \leq \alpha_r \leq 1$. We consider this an extended
version of the basic model as there are two more parameters that
provide increased modelling flexibility. It is important to remark
that, with such a restricted interval for the values of $\alpha$,
this weighted model is no longer identical to an IBBM
with a single set of prior distributions. One
motivation for this extended model is that the basic
model seemed very cautious by not using the
informative prior part if $s$ is in the TPDA. For $\alpha_l>0$,
the informative part of the model influences the
inferences for all values of $s$, including the one in the TPDA. As
a consequence of taking $\alpha_l>0$, however, the line segment
$(s,\frac{s}{n})$ with $s\in [0,n]$ will not always be in between the lower and upper
probabilities anymore, specifically not at, and close to, $s=0$ and
$s=n$, as follows from the results presented below.

The lower and upper probabilities resulting from the two models that
are combined by taking an imprecise weighted average are again as
given by formulae \eqref{ilowupp}--\eqref{ulowupp}, with the weighted
averages $\Pl_{\alpha}$ and $\Pu_{\alpha}$, for any
$\alpha \in [\alpha_l,\alpha_r]$, again given by %(\ref{alpha-low}) and (\ref{alpha-upp}).
\eqref{alpha-lowupp}. This leads to the lower and
upper probabilities for the combined inference
\begin{align*}
\Pl &= \min_{\alpha \in [\alpha_l,\alpha_r]} \Pl_{\alpha} & \text{and}& & % \label{ext-P-low} \\
\Pu &= \max_{\alpha \in [\alpha_l,\alpha_r]} \Pu_{\alpha} \,.             % \label{ext-P-upp}
\end{align*}
The lower and upper probabilities have, as function of $s$,
the generic forms presented in Figure~\ref{fig:weighted-generic},
with
$[S_1,S_2]=\left[(n+1)\frac{s^i}{n^i}-1, (n+1)\frac{s^i}{n^i}\right]$ as in Section~\ref{wi-basic}.
The specific values for Figure~\ref{fig:weighted-generic} are
%$A=\frac{\alpha_l s^i}{n^i+n+1}$,
%$B=\frac{1}{n+1} + \frac{\alpha_r [s^i(n+1)-n^i]}{(n^i+n+1)(n+1)}$,
%$C=\frac{n}{n+1}-\frac{\alpha_r[(n^i-s^i)(n+1)-n^i]}{(n^i+n+1)(n+1)}$,
%$D=1-\frac{\alpha_l(n^i-s^i)}{n^i+n+1}$,
%$E=\frac{s^i}{n^i}-\frac{1}{n+1}\left[1-\frac{\alpha_l n^i}{n^i+n+1}\right]$,
%$F=\frac{s^i}{n^i}+\frac{1}{n+1}\left[1-\frac{\alpha_l n^i}{n^i+n+1}\right]$,
%$\text{sl.~1}=\frac{n^i+n+1-\alpha_r n_i}{(n^i+n+1)(n+1)}$,
%$\text{sl.~2}=\frac{n^i+n+1-\alpha_l n_i}{(n^i+n+1)(n+1)}$.
\begin{align*}
A &= \textstyle \frac{\alpha_l s^i}{n^i+n+1} &
B &= \textstyle \frac{1}{n+1} + \frac{\alpha_r [s^i(n+1)-n^i]}{(n^i+n+1)(n+1)} \\
D &= \textstyle 1-\frac{\alpha_l(n^i-s^i)}{n^i+n+1} &
C &= \textstyle \frac{n}{n+1}-\frac{\alpha_r[(n^i-s^i)(n+1)-n^i]}{(n^i+n+1)(n+1)} \\
\text{sl.~1} &= \textstyle \frac{n^i+n+1-\alpha_r n_i}{(n^i+n+1)(n+1)} &
E &= \textstyle \frac{s^i}{n^i}-\frac{1}{n+1}\left[1-\frac{\alpha_l n^i}{n^i+n+1}\right] \\
\text{sl.~2} &= \textstyle \frac{n^i+n+1-\alpha_l n_i}{(n^i+n+1)(n+1)} &
F &= \textstyle \frac{s^i}{n^i}+\frac{1}{n+1}\left[1-\frac{\alpha_l n^i}{n^i+n+1}\right] \,.
\end{align*}
The increase in imprecision when $s$ moves away from the TPDA can again be
considered as caused by the informative part of the model, which is logical as the uninformative part of the model cannot
exhibit prior-data conflict.
Maximum prior-data conflict occurs again if $s^i=0$ and $s=n$, in which case
$\Pl = \frac{n}{n+1} - \frac{\alpha_r n^i n}{(n^i+n+1)(n+1)}$ and
$\Pu = 1 - \frac{\alpha_l n^i}{n^i+n+1}$, or if $s^i=n^i$ and $s=0$,
when $\Pl = \frac{\alpha_l n^i}{n^i+n+1}$ and
$\Pu = \frac{1}{n+1} + \frac{\alpha_r n^in}{(n^i+n+1)(n+1)}$.

The possibility to choose values for $\alpha_l$ and $\alpha_r$
provides substantially more modelling flexibility compared to the
basic model presented in Section~\ref{wi-basic}. One may, for
example, wish to enable inferences solely based on the informative
part of the model, hence choose $\alpha_r=1$, but ensure that this
part has influence on the inferences in all situations, with equal
influence to the uninformative part in case of TPDA. This latter
aspect can be realized by choosing $\alpha_l=0.5$. When compared to
the situation in Section~\ref{wi-basic}, this choice moves, in
Figure~\ref{fig:weighted-generic}, $A$ and $D$ away
from 0 and 1, respectively, but does not affect $B$ and $C$. It also
brings $E$ and $F$ a bit closer to the corresponding upper and lower
probabilities, respectively, hence reducing imprecision in the TPDA.


\subsubsection{Weighted Inference Model Properties}
\label{wi-prop}

The basic model presented in Section~\ref{wi-basic} fits in the Bayesian framework, but
its use of prior information is different to the usual way in Bayesian
statistics. The lower and upper probabilities are mainly driven by the uninformative part, which, e.g.,
implies that $\Pl \leq \frac{s}{n} \leq \Pu$ for all values
of $s$. While in (imprecise, generalised) Bayesian statistics any
part of the model that uses an informative prior can be
regarded as adding information to the data, the informative part of the basic model
leads to more careful inferences when there is prior-data conflict.
Figure~\ref{fig:weighted-generic} shows that, for the basic case of
Section~\ref{wi-basic}, the points $A$ and $D$ are based
only on the uninformative part of the model, but the points $B$ and $C$ are based on the
informative part of the model.

Prior-data conflict can be of different strength, one would expect
to only talk about `conflict' if consideration is required, hence the information in the prior
and in the data should be sufficiently strong. The proposed method
in Section~\ref{wi-basic} takes as starting point inference that is
fully based on the data, it uses the informative prior part of
the model to widen the interval of lower and upper
probabilities in the direction of the value $\frac{s^i}{n^i}$. For
example, if one observed $s=0$, the upper probability of a success
at the next observation is equal to $\frac{s^i+1}{n^i+n+1}$, which
reflects inclusion of the information in the prior set for the
informative part of the model that is most supportive for this
event, equivalent to $s^i+1$ successes in $n^i+1$
observations. As such, the effect of the prior information is to weaken
the inferences by increasing imprecision in case of prior-data conflict.

One possible way in which to view this weighted inference model is
as resulting from a multiple expert or information source problem,
where one wishes to combine the inferences resulting individually
from each source. The basic model of Section~\ref{wi-basic} leads to
the most conservative inference such that no individual model or
expert disagrees, while the restriction on weights provides a
guaranteed minimum level for the individual contributions to the combined
inference.

It should be emphasized that the weighted inference model has wide applicability. The key idea is to combine,
by imprecise weighting, the actual inferences resulting from
multiple models, and as such there is much scope for the use and
further development of this approach. The individual models could
even be models such as those described in
Sections~\ref{sec:ibbm-walley} and \ref{sec:othershapes}, although
that would lead to more complications. If the individual models
are coherent lower and upper probabilities, i.e.\ provide separately
coherent inferences, then the combined inference via weighted
averaging and taking the lower and upper envelopes is also
separately coherent.\footnote{This follows, e.g., from\textcite[\S 2.6.3f]{1991:walley}.}

In applications, it is often important to determine a sample size
(or more general design issues) before data are collected. If one
uses a model that can react to prior-data conflict, this is likely
to lead to a larger data requirement. One very cautious approach is to choose $n$ such that the maximum possible resulting
imprecision does not exceed a chosen threshold. In the models
presented in this paper, this maximum imprecision will always occur
for either $s=0$ or $s=n$, whichever is further away from the TPDA.
In such cases, a preliminary study has shown an attractive feature if one can actually
sample sequentially. If some data are obtained with success proportion close to $s^i/n^i$, %then
the total data requirement (including these first observations) %in order
to ensure that the resulting maximum
imprecision cannot exceed the same threshold level is substantially less
than had been the case before any data were available.
This would be in line with intuition, and further
research into this and related aspects is ongoing, including of
course the further data need in case first sampled data is in
conflict with $(n^i,s^i)$, and the behaviour of the models of Section~\ref{sec:ibbm} in such cases.

The weighted inference method combines the inferences based on two
models, and can be generalized to allow more than two models and
different inferential methods. It is also possible to allow more
imprecision in each of the models that are combined, leading to more
parameters in the overall model that can be used to control the
behaviour of the inferences. Similar post-inference combination via
weighted averaging, but with precise weights, has been presented in the
frequentist statistics literature \parencite{2003:hjort,2003:longford}, where the
weights are actually determined based on the data and a chosen
optimality criterion for the combined inference. In Bayesian
statistics, estimation or prediction inferences based on different
models can be similarly combined using Bayes factors \parencite{1995:kass-raftery},
which are based on both the data (via the likelihood function) and
prior weightings for the different models.%
\footnote{See also the brief characterisation of Bayes factors
in the context of hypotheses testing and model selection in Section~\ref{sec:beta-binom}.}
In our approach, we do not use the data or prior beliefs about the models to derive precise
weights for the models, instead we cautiously base our combined
lower and upper predictive probabilities on those of the individual
models with a range of possible weights. This range is set by the
analyst and does not explicitly take the data or prior beliefs into
account, but it provides flexibility with regard to the relative
importance given to the individual models.


\subsection{Insights and Challenges}
\label{sec:insights}

We have discussed two different classes of inferential
methods to handle \pdc\ in the Bernoulli case. These can be
generalised to the multinomial case corresponding to the IDM.
It also seems possible to extend the approaches to continuous sampling models like the normal
or the gamma distribution, by utilizing the fact that the basic form
of the updating of $\nz$ and $\yz$ in
\eqref{equ:update-nn-yn-precise} underlying \eqref{equ:General-Pl}
and \eqref{equ:General-Pu} is valid for arbitrary canonical
exponential families \parencite[see][and Section~\ref{sec:generalmodel}]{2005:quaeghebeurcooman,Walter2009a}.
Further insight into the weighting method may also be provided by
comparing it to generalised Bayesian analysis based on sets of
conjugate priors consisting of nontrivial mixtures of two Beta
distributions. There, however, the posterior mixture parameter depends
on the other parameters.
%It is also interesting to investigate the application of
%our methods with coarse data, in an analogous way to \cite{UtkinAugustin1:2007}
%and \cite{TroffaesCoolen2009}, which may provide further insights into
%prior-data conflict. ***Text von Thomas zu contamination***
For a deeper understanding of prior-data conflict, it may also be helpful
to extend our methods to coarse data, in an analogous way to \textcite{2007:utkinaugustin} and
\textcite{2009:Troffaes:Coolen}, and to look at other model classes of prior distributions, most
notably at contamination neighbourhoods. Of particular interest here may
be to combine both types of prior models, considering contamination
neighbourhoods of our exponential family based-models with sets of
parameters, as developed in the Neyman-Pearson setting by
\textcite[\S~5]{2002:augustin}.

The models presented here address \pdc\
in different ways, either by fully utilizing the prior information in a way that is close to the
traditional Bayesian method, where this information is added to data
information, or by not including them initially as in
Section~\ref{sec:weightedinf}. All these models show the desired increase of
imprecision in case of prior-data conflict. It may be of
interest to derive methods that explicitly respond to (perhaps
surprisingly) strong prior-data agreement. ***link to boatshape material in outlook***
One possibility to achieve this with the methods presented here is to
consider the TPDA as this situation of strong agreement in which one wants
imprecision reduced further than compared to an `expected'
situation, and to choose the prior set (Section~\ref{sec:ibbm}) or the two
inferential models (Section~\ref{sec:weightedinf}) in such a way to create this effect.
This raises interesting questions for elicitation, but both approaches
provide opportunities for this and we consider it as an important topic for further
study. %, which we also expect will suggest useful generalizations of these methods.

Far beyond further extensions %along these lines,
one has, from the foundational point of view, to be aware that there are
many ways in which people might react to prior-data conflict, and we
may perhaps at best hope to catch some of these in a specific
model and inferential method.
This is especially important when the conflict is very strong, and
indeed has to be considered as full contradiction of modeling
assumptions and data, which may lead to a revision of the whole
system of background knowledge in the light of surprising
observations, as Hampel argues.\footnote{See in particular the discussion of the
structure and role of background knowledge in \textcite{2009:hampel:knowledge}.}
In this context, applying the weighting approach to the NPI-based model
for categorical data \parencite{2009:Coolen:Augustin} may provide some
interesting opportunities, as it explicitly allows to consider not yet observed and
even undefined categories \parencite{2005:Coolen:Augustin}.

There is another intriguing way in which one may react to prior-data
conflict, namely by considering the combined information to be of
less value than either the real data themselves or than both
information sources. Strong prior beliefs about
a high success rate could be strongly contradicted by data, as such
leading to severe doubt about what is actually going on. The increase of
imprecision in case of \pdc\ in the methods presented in this paper
might be interpreted as reflecting this, but there may be other
opportunities to model such an effect. It may be possible to link these
methods to some popular approaches in frequentist statistics, where some robustness
can be achieved or where variability of inferences can be studied by
%effectively deleting some of the real observations.%
%deleting some of the real observations in turns. %
round robin deletion of some of the real observations. %
This idea may open up interesting research challenges for imprecise probability
models, where the extent of data reduction could perhaps be
related to the level of prior-data conflict. Of course, such
approaches would only be of use in situations with substantial
amounts of real data, but as mentioned before, these are typically
the situations where prior-data conflict is most likely to be of
sufficient relevance to take its modelling seriously. As (imprecise,
generalised) Bayesian methods all work essentially by \emph{adding}
information to the real data, it is unlikely that such new methods
can be developed within the Bayesian framework, although there may
be opportunities if one restricts the inferences to situations where
one has at least a pre-determined number of observations to ensure
that posterior inferences are proper. For example, one could
consider allowing the prior strength parameter
$\nz$ in the IBBM to take on negative values, opening up a rich
field for research and discussions.


\subsubsection*{Acknowledgements}%\vspace*{-1.0ex}%

The authors thank the referees for very helpful comments.




