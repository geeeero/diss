\chapter{Generalised Bayesian Inference with Sets of Conjugate Priors in Exponential Families}
\label{cha:imprecisebayes-conjugate}

Imprecise probability models based on conjugate priors are an important model class,
and have been central to the development and application of imprecise probability methods
in statistical inference.
They have lead to the so-called \emph{Imprecise Dirichlet model} (IDM, see Section~\ref{sec:idm-and-near-ignorance} below)
by \textcite{1996:walley::idm}, see also \textcite{2009:bernard},
and more generally to powerful imprecise probability models for inference
based on i.i.d.\ exponential family sampling models by \textcite{2005:quaeghebeurcooman} and \textcite{2009:quaeghebeur::phd}.
These models were extended by \textcite[see Section~\ref{sec:jstp}]{Walter2009a} and \textcite[see Section~\ref{sec:isipta11}]{Walter2011a}
to allow in addition an elegant handling of prior-data conflict.

\medskip

Section~\ref{sec:generalmodel} attempts to give a systematic overview on these models, %the different models discussed in the literature,
and illustrates some characteristic modelling opportunities of generalised Bayesian inference.

Section~\ref{sec:alternatives} briefly discusses some alternative models based on sets of priors.***

Section~\ref{sec:jstp} then presents one model in detail, namely the so-called
\nymodel\ developed in \textcite{Walter2009a},
and illustrates its application to the Normal-Normal and Dirichlet-Multinomial models. % in more detail. %***JSTP***

Section~\ref{sec:luck} gives a short overview on a software implementation of \nymodel s,
the add-on package \texttt{luck} ***citation*** for the statistical programming environment \textbf{R} \parencite{2013:r}.

%***isipta07 summary in appendix?***

Section~\ref{sec:isipta11} presents attempts to further refine inference behaviour in the presence of \pdc,
one by considering more sophisticated shapes for prior parameter sets,
the other by a fundamentally different approach of combining inferences from two arbitrary distinct models.
Interestingly, the inferences from the two approaches in Section~\ref{sec:isipta11} show nevertheless fascinating similarities.
%***ISIPTA'11***

%***football example?***



\section{Model Overview and Discussion}
%\section{The General Model}
\label{sec:generalmodel}

%***structure of section: here comes the important part!****

This section gives a systematic overview on imprecise probability models
for inference in canonical exponential families
based on sets of canonical conjugate priors.
It integrates the approaches discussed in Sections~\ref{sec:jstp} and \ref{sec:isipta11},
and further approaches discussed in the literature, into a general framework,
and reviews their inference properties.

\medskip

In Section~\ref{sec:basicsetting}, a general framework that characterises these models is elaborated,
and the models discussed in the literature that can be subsumed*** under this framework are named/given/enumerated***.
%
Section~\ref{sec:gbicp-properties-criteria} presents a number of inference properties that this framework provides,
along with two further criteria characterising the unique modelling opportunities that
generalised Bayesian inference can offer: sensitivity to \pdc, and the case of weakly informative priors.
%
The models from Section~\ref{sec:basicsetting} are then discussed in the light of the two criteria in
Sections~\ref{sec:idm-and-near-ignorance} and \ref{sec:pdc-sensitivity},
providing a summary of model strengths and weaknesses.


\subsection{The General Framework}
%\subsection{The Basic Setting}
\label{sec:basicsetting}

Consider inference based on samples from a regular canonical exponential family \eqref{eq:expofam-sampledens}
using the conjugate prior \eqref{eq:canonicalprior} as discussed in Section~\ref{sec:regularconjugates}.
One specifies a prior parameter set $\PZ$ of $(\nz,\yz)$ values
and takes as imprecise prior---described via the credal set $\MZ$---the set of traditional priors with $(\nz, \yz) \in \PZ$.
The credal set $\MN$ of posterior distributions,%
\footnote{To emphasize the dependence of the sample size $n$,
the posterior credal set is denoted by $\MN$ instead of $\mathcal{M}_{\vert \x}$ as in \eqref{eq:posteriorcredalset}.}
obtained by updating each element of $\MZ$ via Bayes' Rule,
then can be described as the set of parametric distributions
with parameters varying in the set of updated parameters $\PN = \{(\nn,\yn) \vert (\nz, \yz) \in \PZ\}$.

Alternatively, $\MZ$ can be defined as the set of all convex mixtures of parametric priors with $(\nz, \yz) \in \PZ$.
In this case, the set of priors corresponding to $\PZ$ considered above gives the set of extreme points for the actual convex set $\MZ$.
Updating this convex prior credal set with the Generalized Bayes' Rule results in a set $\MN$ of posterior distributions that is again convex,
and $\MN$ conveniently can be obtained by taking the convex hull of the set of posteriors
defined by the set of updated parameters $\PN$.

When the credal set $\MZ$ is taken to contain convex combinations (i.e., finite mixtures) of parametric priors,
$\MZ$ is very flexible and contains, through the mixture distributions, a wealth of distributional shapes.%
\footnote{Indeed, if the parametric distributions are normal distributions
and $\PZ$ is large enough,
it can be assumed that $\MZ$ contains a very wide range of priors,
as mixtures of normal distributions are dense in the space of well-behaved probability distributions
\parencite[see, e.g.,][p.~44]{2000:priebe}***other citation****.}
Nevertheless, maximisation and minimisation over $\MN$
is quite feasible for quantities that are \emph{linear} in the parametric posteriors contained in $\MN$,
as then we can be sure that suprema and infima are attained at the extreme points of $\MN$,
which are the parametric distributions generated by $\PN$, enabling us to search over $\PN$ only.
Extremes for prior and posterior quantities of interest that are not linear in the parametric distributions
may be very difficult to obtain,
or the model could even be useless for them (because the model may give too imprecise, or even vacuous, bounds).
Expectations are linear in the parametric distributions, while variances are not.%
\footnote{In the context of decision making (as mentioned in Sections~\ref{sec:inferencetasks} and \ref{sec:beta-binom}),
the Bayes criterion selects as optimal acts those acts which minimise posterior risk,
where posterior risk is the expected loss under the posterior distribution.
With credal sets, the optimal acts are usually determined by minimising the upper posterior risk
\parencite[see, e.g.,][\S 3.2]{itip-decision}.
It thus depends on the loss function if the supremum of the expected loss over $\MN$ can be easily determined.****check***}
%****footnote: loss minimising / utility maximising acts, decision making tasks often rely on expectations only****

%When the credal set $\MZ$ is taken to contain convex combinations (i.e., finite mixtures) of parametric priors,
%$\MZ$ is very flexible and contains a wide variety of priors.
%makes the inference procedure very general,
%
%Still, minimisation and maximisation over $\MZ$ or $\MN$ are feasible
%for quantities that are \emph{linear} in the prior or the posterior, respectively.
%For these quantities, extremes are attained at the extreme points of the credal set,
%which are the parametric distributions.

For both cases, the relationship between the parameter sets $\PZ$ and $\PN$ and the credal sets $\MZ$ and $\MN$
will allow us to discuss different models $\MZ$ and $\MN$
by considering the corresponding parameter sets $\PZ$ and $\PN$.%
\footnote{Note that, although, by the general framework, the credal sets $\MZ$ and $\MN$ may be defined as convex hulls,
the parameter sets $\PZ$ and $\PN$ generating them need not necessarily be so, and typically are not convex, indeed.
See, e.g., Figure~\ref{fig:spot-banana} on page~\pageref{fig:spot-banana}.}


As an example, in the precise Beta-Bernoulli model as discussed in Section~\ref{sec:beta-binom},
the posterior predictive probability %$[\lpr,\upr]$
for the event that a future single draw is a success is equal to $\yn$, and so we get,
for an imprecise model with $\MZ = \{p(\theta\mid\nz, \yz) \vert (\nz, \yz) \in \PZ\}$,
the lower and upper probability
\begin{align*}
\ynl &:= \inf_{\PN} \yn = \inf_{\PZ} \frac{\nz\yz + s}{\nz +n}\,, \\
%  & = \min_{\nz \in [\nzl, \nzu]} \frac{\nz\yzl + s}{\nz +n} & &\text{and} \nonumber\\
\ynu &:= \sup_{\PN} \yn = \sup_{\PZ} \frac{\nz\yz + s}{\nz +n}\,.
%  & = \max_{\nz \in [\nzl, \nzu]} \frac{\nz\yzu + s}{\nz +n}. \nonumber
\end{align*}

Special imprecise probability models are now obtained by specific choices of $\PZ$.
We distinguish the following types of models:
\begin{enumerate}[(a)]
\item $\nz$ is fixed, while $\yz$ varies in a set $\YZ$.\\
\label{enum:modeltypes-a}%
The IDM \parencite{1996:walley::idm},
as well as its generalisation to all sample distributions of the canonical exponential form \eqref{eq:expofam-sampledens}
by \textcite{2005:quaeghebeurcooman} are of this type.
The approach by \textcite{1997:boratynska} also belongs to this category,
as she specifies bounds for $\nz \yz$ while holding $\nz$ constant \parencite[see][p.~1973]{2012:benavolizaffalon}.
\item $\nz$ varies in a set $\NZ$, while $\yz$ is fixed.\\
\label{enum:varyn}%
This type of model is rarely discussed in the literature,
but is mentioned by \textcite{1991:walley} in {\S 7.8.3} and in {\S 1.1.4}, footnote no.~10.
Both instances assume the Normal-Normal model as described in Section~\ref{sec:norm-norm},
where the set of priors is spanned by normal distributions with a fixed mean $\yz$ and a range of variances $\sigma_0^2 / \nz$.
\item Both $\nz$ and $\yz$ vary in a set $\{ (\nz, \yz) \mid \nz \in \NZ, \yz \in \YZ \}$.\\
\label{enum:rectangular}%
%$\nz \in \NZ \times \yz \in \YZ$.\\
This type of model is first discussed in \textcite[\S 5.4.3]{1991:walley} for the Beta-Bernoulli model,
and was later generalised by \textcite{Walter2009a}
to sample distributions of the canonical exponential form \eqref{eq:expofam-sampledens}.%
\footnote{%
%***from \textcite[\S 4.4]{itip-statinf}:\\
More precisely, \textcite[see Section~\ref{sec:jstp}]{Walter2009a} proposed
a direct extension of the model framework discussed in Section~\ref{sec:generalmodel}
by the definition of so-called \emph{LUCK-models},
which were already used in \textcite{Walter2007a}
to generalise Bayesian linear regression (see a short summary in Section~\ref{sec:isipta07}).
These exploit*** the fact that the central properties of the model framework
(see Section~\ref{sec:gbicp-properties-criteria} below)
rely on the specific form of the update step \eqref{eq:canonicalupdate} only.
Thus, they can be generalised to settings that are not based on i.i.d.\ observations from canonical exponential family distributions,
but nevertheless follow the same update step.} %This is elaborated in Section~\ref{sec:jstp} \parencite{Walter2009a} 
We will discuss and illustrate this model in Section~\ref{sec:jstp}.
It should be noted here that while the prior parameter set is a Cartesian product of $\NZ = [\nzl, \nzu]$ and $\YZ$,
the posterior parameter set is not.
This is due to Eq.~\eqref{eq:canonicalupdate},
which results in different ranges for $\yn$ depending on the value of $\nz$ used in the update step.%
\footnote{This is illustrated, e.g., in Figure~\ref{fig:spot-banana}, page~\pageref{fig:spot-banana}.}
\item Both $\nz$ and $\yz$ vary in other sets $\PZ \subset (\reals_{>0} \times \Y)$.\\
In this type, also in the prior parameter set the range of $\yz$ may depend on $\nz$,
as in \textcite[\S 2.3, see Section~\ref{sec:othershapes}]{Walter2011a},
or, vice versa, the range of $\nz$ may depend on the value of $\yz$, as in \textcite{2012:benavolizaffalon}.
\label{enum:modeltypes-d}
\end{enumerate}


\subsection{Properties and Criteria}
\label{sec:gbicp-properties-criteria}

Before discussing the approaches mentioned above in some detail,
we will describe some properties that all of them have in common.
These properties are due to the update mechanism \eqref{eq:canonicalupdate}
for $\nz$ and $\yz$ and the resulting size and position of $\PN$,
being a direct consequence of the (Generalised) Bayes' Rule in the setting of canonical exponential families.
Remember that $\nz$ is incremented with $n$, while $\yn$ is a weighted average of $\yz$ and the sample statistic $\ttau(\x)$,
with weights $\nz$ and $n$, respectively.
Thus, while the (absolute) stretch of $\PN$ in the $\nz$ resp.\ $\nn$ dimension will not change during updating,
the stretch in the $\yz$ resp.\ $\yn$ dimension will do so.
When speaking of the size of $\PN$, we will thus refer to the stretch in the main parameter dimension, also denoted by $\Delta_y(\PN)$.
\begin{enumerate}[I.]
\item \label{enum:n0vsn}The larger $n$ relative to (values in the range of) $\nz$,
ceteris paribus (c.p.) the smaller $\PN$, i.e.\ the more precise the inferences.
Vice versa, the larger the $\nz$ value(s) as compared to $n$, c.p.\ the larger $\PN$, and the more imprecise the inferences.
Thus, a high weight on the imprecise prior $\MZ$ will lead to a more imprecise posterior $\MN$.%
\footnote{For model types with fixed $\nz$, if $\nz = n$, then $\Delta_y(\PN) = \Delta_y(\PZ) / 2$,
i.e.\ the width of the posterior expectation interval is half the width of the prior interval.
This fact may also guide the choice of $\nz$.}
\item \label{enum:ntoinfty}In particular, for $n \to \infty$, the stretch of $\yn$ in $\PN$ will converge towards zero,
i.e.\ $\Delta_y(\PN) \to 0$, with the limit located at $\ttau(\x)$.
For inferences based mainly on $\yn$, this leads to a convergence towards the `correct' conclusions.
This applies, e.g., to point estimators like the posterior mean or median, which converge to the `true' parameter,
to interval estimates (HPD intervals) contracting around this point estimate (length of interval $\to 0$),
and to the probability that a test decides for the `right' hypothesis, which converges to 1.%
\footnote{In the binomial and normal example,
the posteriors in $\MN$ will concentrate all their probability mass at $\yn \to \ttau(\x)$,
and as $\ttau(\x) \to \theta$ in probability, all these inference properties follow.}
This property, that holds also for traditional (precise) Bayesian inferences,
is similar to the consistency property often employed in frequentist statistics.
\item \label{enum:deltay}The larger $\Delta_y(\PZ)$, the larger c.p.\ $\Delta_y(\PN)$;
%The larger the stretch in the $\yz$ dimension a priori, the larger c.p.\ the stretch of the $\yn$ values obtained a posteriori.
a more imprecise prior $\MZ$ will naturally lead to a more imprecise posterior $\MN$, which carries over to the inferences.
\end{enumerate}

Apart from the above properties that are guaranteed in all of the four model types %(a) -- (d),
\eqref{enum:modeltypes-a} -- \eqref{enum:modeltypes-d},
one might want the models to adhere to (either of) the following additional criteria:
\begin{enumerate}[I.]
\setcounter{enumi}{3}
\item \textbf{Prior-data conflict sensitivity.}\label{enum:pdc}
In order to mirror the state of posterior information,
posterior inferences should, all other things equal, be more imprecise in the case of prior-data conflict.
Reciprocally, if prior information and data coincide especially well, an additional gain in posterior precision may be warranted.
Such models could deliver (relatively) precise answers when the data confirm prior assumptions,
while rendering much more cautionary answers in the case of prior-data conflict,
thus leading to cautious inferences if, and only if, caution is needed.

Most statisticians using precise priors would probably expect a more diffuse posterior in case of prior-data conflict.
However, in the canonical conjugate setting of Eq.~\eqref{eq:canonicalprior},
which is often used when data are scarce and only strong prior beliefs allow for a reasonably precise inference answer,
this is usually not the case.
E.g., for the Normal-Normal model, the posterior variance \eqref{eq:normalpostvar} is not sensitive to the location of $\bar{\x}$,
and decreases by the factor $\nz/(\nz+n)$ for any $\bar{\x}$,
thus giving a false certainty in posterior inferences in case of prior-data conflict.
In the Beta-Binomial model, the posterior variance $\yn (1-\yn)/(\nz + n)$ depends on the location of $s/n$,
but in a similar way as the prior variance $\yz (1-\yz)/\nz$ depends on $\yz$,
mirroring only the fact that Beta distributions centered at the margins of the unit interval are constrained in their spread.
Thus, there is no systematic reaction to prior-data conflict also in this case.

In the imprecise Bayesian framework as discussed here,
prior-data conflict sensitivity translates into having a larger $\PN$ (leading to a larger $\MN$) in case of prior-data conflict,
and, mutatis mutandis, into a smaller $\PN$ if prior and data coincide especially well.

\item \textbf{Possibility of weakly or non-informative priors.}\label{enum:noninformative}
When only very weak or (almost) no prior information is available on the parameter(s) one wishes to learn about,
it should be possible to model this situation adequately.
The traditional Bayesian approach to this problem, so-called \emph{non-informative priors},
are, due to their nature as single, i.e.\ precise, probability distributions, not expressive enough;
a precise probability distribution necessarily produces a precise value for $\p(\vartheta \in A)$ for any $A \subseteq \Theta$,
which seems incompatible with the notion of prior ignorance about $\vartheta$.
Furthermore, in the literature there are several, often mutually incompatible, approaches to construct non-informative priors,
such as Laplace's prior, Jeffreys' prior, or reference priors \parencite[see, e.g.,][\S 5.6.2]{2000:bernardosmith}.
Most of these precise priors seem to convey mainly a state of indifference instead of ignorance \parencite[p.~271]{1999:rueger}.%
\footnote{\label{footnote:symm}E.g., in the designation of the uniform prior
as a non-informative prior by the principle of insufficient reason (i.e., taking Laplace's prior),
it is argued that there is no reason to favor one parameter value over another,
and thus, all of them should get the same probability resp.\ density value.
For analysts restricting themselves to precise priors, this argument leads necessarily to the uniform prior.
When considering imprecise priors, however, the principle of insufficient reason does not uniquely determine a certain prior.
It only states that the probability resp.\ density \emph{interval} should be equal for all parameter values,
but that interval may be any interval $\subseteq [0,1]$.
We may thus realise that the principle of insufficient reason actually implies \emph{indifference} between parameter values only,
and that other considerations are needed to distinguish a certain imprecise prior as (nearly) non-informative;
usually, it is postulated that (a certain class of) inferences based on the prior should be (nearly) vacuous
\parencite[see, e.g.,][]{2012:benavolizaffalon},
and specific information (e.g., symmetry of parameter values) would be needed to reduce the size of the prior credal set.
See, in particular, \textcite[\S 4.3]{2001:weichselberger} formulating two symmetry principles
extending the principle of insufficient reason,
and the closely related discussion of weak and strong invariance in \textcite[\S 3]{itip-structural}.
For a critique on non-informative priors from an imprecise probability viewpoint see, e.g., \textcite[\S 5.5]{1991:walley};
their partition dependence is also discussed in the context of elicitation \parencite[see][\S 3]{itip-elicitation}.}
%\item ***further desirable properties??***
\end{enumerate}

The approaches mentioned in \eqref{enum:modeltypes-a} -- \eqref{enum:modeltypes-d} are now discussed in detail,
sorted according to two basic scenarios regarding the intended use:
When no prior information on $\vartheta$ is available (or nearly so),
so-called \emph{near-ignorance priors} are used to model the state of prior ignorance.
When, in contrast, there is substantial prior information available,
the challenge is to model this information adequately in the prior,
while ensuring easy handling and prior-data conflict sensitivity.%
\footnote{\label{foot:sequential}As another possible modelling aim,
in situations when data is revealed to the analyst sequentially in distinct batches,
it might also be useful if the model is able to resonate unusual patterns or extreme differences between the batches.
This actually effects to doubting the i.i.d.\ assumptions on which these models are founded.
This could be useful in the area of \emph{statistical surveillance},
where, e.g., the number of cases of a certain infectious disease is continuously monitored,
with the aim to detect epidemic outbreaks in their early stages.}


\subsection{The IDM and other Prior Near-Ignorance Models}
\label{sec:idm-and-near-ignorance}

The \emph{Imprecise Dirichlet model} (IDM) was developed by \textcite{1996:walley::idm}
as a model for inferences from multinomial data when no prior information is available.%
\footnote{The imprecise Beta-Binomial model from \textcite[\S 5.3.2]{1991:walley}
can be seen as a precursor to the IDM, covering the special case of two categories.}
As indicated by its name, it uses as imprecise prior a (near-) noninformative set of Dirichlet priors,
which is obtained by choosing $\YZ$ as the whole interior of the unit simplex $\Delta$, with $\nz$ fixed,%
\footnote{Our notation relates to Walley's \parencite*{1996:walley::idm} as
$t_j \leftrightarrow \yz_j$, $s \leftrightarrow \nz$, $t^*_j \leftrightarrow \yn_j$.}
\begin{align*}
\MZ &= \left\{ p(\theta \mid \nz, \yz) \left| 0 < \yz_j < 1\ \forall j, \sum_{j=1}^k \yz_j = 1 \right. \right\}\,.
\end{align*}
The prior credal set is thus determined by the choice of $\nz$.
\Textcite{1996:walley::idm} argues for choosing $\nz = 1$ or $\nz = 2$, where, in the latter case,
inferences from the IDM encompass both frequentist and Bayesian results based on standard choices for noninformative priors.
For any choice of $\nz$, this imprecise prior expresses a state of ignorance about $\theta$,
as, for all $j = 1,\ldots, k$,
\begin{align*}
\El[\theta_j] &= \yzl_j = \inf_{\yz_j \in \YZ} \yz_j = 0\,,\\
\Eu[\theta_j] &= \yzu_j = \sup_{\yz_j \in \YZ} \yz_j = 1\,,
\end{align*}
and probabilities for events regarding $\theta_j$ are vacuous, i.e.\
$[\Pl,\Pu](\theta_j \in A) = (0,1)$, for any $A \subset [0, 1]$.%
%****also true for $\theta \in A \subset \Delta$?***%
\footnote{However, the IDM may give non-vacuous prior probabilities for some more elaborate events.
An example \parencite[p.~14]{1996:walley::idm} is the event $(A_J, A_K)$
that the next observation belongs to a category subset $J \subseteq\{1, \ldots, k\}$,
and the observation following that belongs to a category subset $K$,
where $J \cap K = \emptyset$ and $|n(A_J) - n(A_K)| < \nz$.}

The posterior credal set $\MN$ is then the set of all Dirichlet distributions
with parameters $\nn$ and $\yn$ obtained by \eqref{eq:canonicalupdate},
\begin{align*}
\MN &= \left\{ p(\theta \mid \nn, \yn) \ \left| \  0 < \yz_j < 1\ \forall j, \sum_{j=1}^k \yz_j = 1 \right. \right\} \,.
\end{align*}
For any event $A_J$ that the next observation belongs to a subset $J$ of the categories, $J \subseteq\{1, \ldots, k\}$,
the posterior lower and upper probabilities calculate as
\begin{align*}
\Pl(A_J) &= \frac{n(A_J)}{\nz + n} &
\Pu(A_J) &= \frac{\nz + n(A_J)}{\nz + n}\,,
\end{align*}
where $n(A_J) = \sum_J n_j$ is the number of observations from the category subset $J$.

The IDM is motivated by a number of inference principles put forward by \textcite[\S 1]{1996:walley::idm},
most notably the \emph{representation invariance principle} \parencite[RIP, see][\S 2.9]{1996:walley::idm}:
Inferences based on the IDM are invariant under different numbers of categories considered in the sample space.%
\footnote{In the example discussed in \textcite{1996:walley::idm},
where colored marbles are drawn from a bag,
it does not matter, e.g., for prior and posterior probabilities for ``red'' as the next draw,
whether one considers the categorization $\{$red, other$\}$ or $\{$red, blue, other$\}$.}
The usefulness of the RIP has been controversially discussed \parencite[see, e.g., the discussion to][]{1996:walley::idm},
and alternative imprecise probability models that do not rely on it have been developed.%
\footnote{See, e.g., Coolen and Augustin \parencite*{2005:Coolen:Augustin, 2009:Coolen:Augustin}
for an alternative based on the NPI approach (see footnote~\ref{foot:npi}, page~\pageref{foot:npi}).
Important differences between this model and the IDM are also briefly discussed at the end of \textcite[\S 6.1]{itip-statinf}.
****note on \textcite{2009:bickis} (see Section~\ref{sec:alternatives:conjugate}) and \textcite{2013:mangilibenavoli}?****}

%The IDM can be shown to lead to $\infty$-monotone lower probabilities \parencite[see, e.g.,][\S 5]{itip-special},
%and its credal set is already completely determined by the lower and upper probabilities of the singletons,
%i.e., the corresponding lower and upper probabilities constitute a probability interval \parencite[see, e.g.,][\S 4]{itip-special}.

%***other properties of IDM: embeddig principle, symmetry principle?\\

Due to its tractability, the IDM has been employed in a number of applications.
\textcite{1996:walley::idm} offers an application to data from medical studies;
\textcite{2005:bernard} details an extension of the IDM to contingency table data that was briefly covered in \textcite{1996:walley::idm}.
In 2009, a special issue of the \emph{International Journal of Approximate Reasoning} \parencite{2009:bernard} was devoted to the IDM.
Since its introduction, the IDM has found applications in, e.g.,
reliability analysis (e.g., \cite{1997:Coolen, 2010:utkin:kozine, 2010:utkin:zatenko:coolen, 2011:li:chen:yi:tao}),
or operations research (e.g., \cite{2006:utkin::expertjudgements});
% 152 citations according to "web of knowledge",
however, the IDM has had an especially strong impact in the area of artificial intelligence,
namely in the construction of classification methods (including, e.g., pattern recognition) and in inference based on graphical models,
see, e.g., \textcite{itip-classification} and \textcite{itip-ipgms} and their references.
These IDM-based methods, in turn, are used in a vast variety of tasks from all kinds of subjects,
such as medicine \parencite[e.g.,][]{2003:zaffalon::dementia},
agriculture \parencite[e.g.,][]{2005:zaffalon::environment},
or geology \parencite[e.g.,][]{2007:antonucci}.
The IDM can be regarded as the most influential imprecise probability model so far.

In the IDM, satisfying near-ignorance for the prior and still having non-vacuous posterior probabilities is possible
because the domain of the prior main parameter $\yz$ is bounded ($\Y = \text{int}(\Delta)$).
For most conjugate priors to exponential family distributions, $\Y$ is not bounded,
and thus, trying to reach prior ignorance in the same way as in the IDM,
by taking $\YZ = \Y$ for a fixed $\nz$, would lead to vacuous posterior probabilities.%
\footnote{From \eqref{eq:canonicalupdate}, it follows that for $\yz \rightarrow \infty$ we get $\yn \rightarrow \infty$ if $\nz$ is fixed.}
Instead, as was shown by \textcite{2012:benavolizaffalon}, for conjugate priors to one-parameter exponential family distributions,
one needs to vary $\nz$ in conjunction with $\yz$ to get both prior near-ignorance and non-vacuous posterior probabilities.
In essence, the term $\nz \yz$ appearing in \eqref{eq:canonicalupdate} must be bounded while letting $\YZ = \Y$,
which effects to a prior parameter set $\PZ$ where the range of $\nz$ depends on $\yz$.

To summarize, imprecise probability methods allow for a much more adequate modeling of prior ignorance
than non-informative priors, the traditional Bayesian approach to this problem, can deliver.
Instead of the somehow awkward choice of a certain non-informativeness approach,
to define an imprecise non-informative prior, the analyst just needs to specify one parameter
(or two, as partly in \cite{2012:benavolizaffalon})
determining the learning speed of the model, namely $\nz$ for the IDM.


\subsection{Substantial Prior Information and Sensitivity to Prior-Data Conflict}
\label{sec:pdc-sensitivity}

Models intended specifically for use in situations with substantial prior information are presented in
\textcite[footnote no.~10 in \S 1.1.4, and \S 7.8.3]{1991:walley}, \textcite{2005:quaeghebeurcooman}, and \textcite{Walter2009a};%
\footnote{See Section~\ref{sec:jstp} for a more detailed coverage and examples.}
the IDM can be modified by not taking $\YZ = \text{int}(\Delta)$, but a smaller set $\YZ$ fitting the prior information.

Generalising the IDM approach to conjugate priors for sample distributions of the canonical form \eqref{eq:expofam-sampledens},
\textcite{2005:quaeghebeurcooman} proposed an imprecise prior $\MZ$ based on $\PZ = \YZ \times \nz$.
E.g., in the Normal-Normal model as described in Section~\ref{sec:norm-norm},
one can take as imprecise prior all convex mixtures of normals with mean in $\YZ = [\yzl, \yzu]$ and a fixed variance $\sigma_0^2/\nz$.
$\YN$, the posterior set of expectations (or modes, or medians) of $\mu$, is then bounded by
\begin{align}
\label{eq:qdc-ynl}
\ynl &= \inf_{\PZ} \yn = \inf_{\YZ} \frac{\nz\yz + n\bar{x}}{\nz + n} = \frac{\nz\yzl + n\bar{x}}{\nz + n}\\
\label{eq:qdc-ynu}
\ynu &= \sup_{\PZ} \yn = \sup_{\YZ} \frac{\nz\yz + n\bar{x}}{\nz + n} = \frac{\nz\yzu + n\bar{x}}{\nz + n}\,.
\end{align}
The lower (upper) posterior expectation of $\mu$ is thus a weighted average of the lower (upper) prior expectation and the sample mean,
with weights $\nz$ and $n$, respectively.
As mentioned above, Quaeghebeur and de Cooman's \parencite*{2005:quaeghebeurcooman} model for Bernoulli or multinomial data leads to the IDM;
because $\Y$, the domain of $\yz$, is not bounded in the general case,
the model is normally used to express substantial prior information.%
\footnote{However, it could be used for near-ignorance prior situations
in case of other sampling models where $\YZ$ can encompass the whole domain without causing posterior vacuousness.
This applies, e.g., to circular distributions like the von Mises distribution,
where the mean direction angle $\mu$ has the domain $(-\pi, \pi]$,
see \textcite[\S B.1.4]{2009:quaeghebeur::phd} and \textcite{1976:mardiaelatoum}.}

More generally in case of a one-parameter exponential family,
$\PZ$ is fully described by the three real parameters $\yzl$, $\yzu$, and $\nz$,
which are straightforward to elicit; furthermore, also $\PN$ is fully described by $\ynl$, $\ynu$, and $\nn$,
and many inferences will be expressible in terms of these three parameters only.
Models of this kind allow for a simple yet powerful imprecise inference calculus,
where the amount of ambiguity in the prior information can be represented by the magnitude of the set $\YZ$,
with $\nz$ determining the learning speed.

The downside of this easily manageable model is that it is insensitive to prior-data conflict,
as the imprecision for the main posterior parameter,
\begin{align}
\label{eq:deltay}
\Delta_y(\PN) &=  \ynu - \ynl = \frac{\nz (\yzu - \yzl)}{\nz +n}\,,
\end{align}
does not depend on the sample $\x$.
Imprecision is thus the same for any sample $\x$ of size $n$,
whenever prior information about $\mu$ as encoded in $\YZ$ is in accordance with data information $\ttau(\x)$ or not.
The relation of $\Delta_y(\PN)$ (which determines the precision of posterior inferences) to the inferential situation at hand is loosened,
as possible conflict between prior and data is not reflected by increased imprecision.
In that sense, the IDM with prior information and the model by \textcite{2005:quaeghebeurcooman}
do not utilize the full expressive power of imprecise probability models,
behaving similar to precise conjugate models by basically ignoring prior-data conflict.

To counter this unwanted behaviour,
\textcite{Walter2009a} suggested that imprecise priors to canonical sample distributions \eqref{eq:expofam-sampledens}
should be based on parameter sets of the form
\begin{align}
\label{equ:rectparamset}
\PZ &= [\nzl, \nzu] \times \YZ\,,
\end{align}
as \textcite[\S 5.4.3]{1991:walley} had already implemented for the Beta-Binomial model.
Then, \eqref{eq:qdc-ynl} and \eqref{eq:qdc-ynu} become
\begin{align*}
\ynl &= \begin{cases} \dfrac{\nzu\yzl + n\bar{x}}{\nzu + n} & \bar{x} \geq \yzl \\[2ex]
                      \dfrac{\nzl\yzl + n\bar{x}}{\nzl + n} & \bar{x} <    \yzl \end{cases}\,, &
\ynu &= \begin{cases} \dfrac{\nzu\yzu + n\bar{x}}{\nzu + n} & \bar{x} \leq \yzu \\[2ex]
                      \dfrac{\nzl\yzu + n\bar{x}}{\nzl + n} & \bar{x} >    \yzu \end{cases}\,.
\end{align*}
If $\yzl < \bar{x} < \yzu$, both $\ynl$ and $\ynu$ are calculated using $\nzu$;
when $\bar{x}$ falls into $[\yzl, \yzu]$, the range of prior expectations for the mean,
prior information gets maximal weight $\nzu$ in the update step \eqref{eq:canonicalupdate},
leading to the same results as for a model with fixed $\nz = \nzu$.
If, however, $\bar{x} < \yzl$, then $\ynl$ is calculated using $\nzl$,
giving less weight to the prior information that turned out to be in conflict with the data.
Thus, as $\ynl$ is a weighted average of $\yzl$ and $\bar{x}$, with weights $\nz$ and $n$, respectively,
$\ynl$ will be lower (nearer towards $\bar{x}$) as compared to an update using $\nzu$,
resulting in increased imprecision $\Delta_y(\PN)$ compared to the situation with $\yzl < \bar{x} < \yzu$.
In the same way, there is additional imprecision $\Delta_y(\PN)$ if $\bar{x} > \yzu$.%
\footnote{In the above, $\bar{x}$ can be replaced by $\ttau(\x)$
to hold for canonical priors \eqref{eq:canonicalprior} in general.
See Section~\ref{sec:4-gw-071216} for an definition in general terms.}

Indeed, \eqref{eq:deltay} can then be written as
\begin{align*}
\Delta_y(\PN) &= \frac{\nzu (\yzu - \yzl)}{\nzu + n} + \inf_{\yz \in \YZ} |\ttau(\x) - \yz| \frac{n (\nzu - \nzl)}{(\nzl + n)(\nzu + n)}\,,
\end{align*}
such that we have the same $\Delta_y(\PN)$ as for a model with $\PZ = \YZ \times \nzu$ when $\ttau(\x) \in \YZ$,
whereas $\Delta_y(\PN)$ increases if $\ttau(\x) \notin \YZ$, the increase depending on the distance of $\ttau(\x)$ to $\YZ$,
as well as on $\nzl$, $\nzu$, and $n$.
This model is described in more detail in Section~\ref{sec:jstp}, along with illustrative examples
(see Figure~\ref{fig:idm-nvar-nopdc} for the Dirichlet-Multinomial model,
and Figure~\ref{fig:nv-nvar-vertfu} for the Normal-Normal model).

%\begin{figure}
%\centering
%\includegraphics[width=\textwidth]{inference-scalednormal.pdf}
%\caption{\label{inference:fig:normalnormal}
%Illustration of prior-data conflict sensitivity in the normal-normal model\myindex{Normal-Normal model}.
%Displayed are sets of normal cdfs, where the graphs in the upper row show the update step if $\bar{\x} \in [\yzl, \yzu]$,
%and the graphs in the lower row show a situation with prior-data conflict.
%The black bars below the cdfs denote the unions of prior resp.\ posterior highest density intervals,
%while the position of $\bar{\x}$ used in the update step is indicated by the lower point of the triangle.
%Note how prior-data conflict in the example depicted in the lower row leads to increased imprecision,
%as reflected by the larger set of posteriors and the longer union of HPD intervals.
%Graph taken from \textcite[p.~267]{2009:walteraugustin}.} %\textbf{***comment by Frank: depict also prior/posterior parameter sets?***}}
%\end{figure}

Models with $\PZ$ as in \eqref{equ:rectparamset}, %= \{ (\nz, \yz) \mid [\nzl, \nzu], \yz \in \YZ \}$
i.e., belonging to model type~\eqref{enum:rectangular},
are sensitive to prior-data conflict,
where prior-data conflict is operationalised as $\ttau(\x) \notin \YZ$.
There is no such direct mechanism for a gain in precision when prior and data information coincide especially well.
However, $\YZ$ could be chosen relatively small such that it mirrors this situation,
considering as the neutral situation $\ttau(\x)$ being not too far away from $\YZ$,
and taking as prior-data conflict situations when $\ttau(\x)$ is in a greater distance to $\YZ$.%
\footnote{See also Section~\ref{sec:insights} for this idea.}
***link to boatshape material in outlook?***

As mentioned in Section~\ref{sec:basicsetting}, page~\pageref{enum:rectangular},
the parameter set $\PN$ resulting from updating $\PZ$ as in \eqref{equ:rectparamset}
by \eqref{eq:canonicalupdate} is not a Cartesian product of $\YN$ and $\NN$,
i.e., a \emph{rectangle} set in case of one-dimensional $\yz$, as was the case for $\PZ$.
It might therefore be necessary to minimize and maximize over $\PN$ itself if inferences depend on $\yn$ and $\nn$ simultaneously.
If, e.g., $\nn\yn$ must be minimised to determine the posterior lower bound of a characteristic of interest,
$\min_{\PN} \nn\yn$ may not be found at $\yzl$, i.e., $\min_{\PN} \nn\yn \not = \min_{\NZ} \nn\ynl$.

The model of type~\eqref{enum:varyn}, where $\PZ = \NZ \times \yz$,
briefly mentioned only in \textcite[footnote no.~10 in \S 1.1.4, and \S 7.8.3]{1991:walley},
also leads to a more complex description of $\PN$ as compared to the models with $\PZ = \nz \times \YZ$ (type~\eqref{enum:modeltypes-a}).

In principle, $\PZ$ could have any form fitting the prior information at hand (type~\eqref{enum:modeltypes-d}).
On close inspection, a rectangular shape for $\PZ$ may not be appropriate in many situations.
One could, e.g., argue that the $\yz$ interval should be narrower at $\nzl$ than at $\nzu$,
because we might be able to give quite a precise $\yz$ interval for a low prior strength $\nzl$,
whereas for a high prior strength $\nzu$, we should be more cautious with our elicitation of $\yz$ and thus give a wider interval;
interestingly, one could also argue the other way round based on similarly convincing arguments.%
\footnote{See, e.g., the rationale discussed at the beginning of Section~\ref{sec:othershapes}.}
To fully specify $\PZ$ along these lines,
lower and upper bounds for $\yz$ must be given for all intermediate values of $\nz$ between $\nzl$ and $\nzu$,
e.g., by some functional form $\yzl(\nz)$ and $\yzu(\nz)$.
The choice of such general forms is not straightforward,
as it may heavily influence the posterior inferences,
and it may be very difficult to elicit as a whole.
One such choice is discussed in Section~\ref{sec:isipta11} \parencite{Walter2011a} for the Binomial case,
developed with the intention to create a smoother reaction to prior-data conflict than in the model with rectangle $\PZ$.

In summary, there is a trade-off between easy description and handling of $\PZ$ on one side,
and modeling accuracy and fulfillment of desired properties on the other:
\begin{itemize}%[(i)]
\item The model in \textcite{2005:quaeghebeurcooman}, which takes $\PZ = \nz \times \YZ$, is very easy to handle,
as the posterior parameter set $\PN = \nn \times \YN$ has the same form as $\PZ$,
and it often suffices to consider the two elements $(\nz, \yzl)$ and $(\nz, \yzu)$ to find posterior bounds for inferences.
It is, however, insensitive to prior-data conflict.
\item The model by \textcite{Walter2009a} is sensitive to prior-data conflict,
but this advantage is payed for by a more complex description of $\PN$.
\item More general set shapes $\PZ \subset \reals_{>0} \times \Y$ are possible,
but may be difficult to elicit and complex to handle.%
\footnote{For an example see, as mentioned above, the work in Section~\ref{sec:isipta11}, specifically in Section~\ref{sec:othershapes},
or, alternatively, ***boatshape material***.}
\end{itemize}

To conclude, when substantial prior information is available, that, however,
is not permitting the identification of a single prior distribution,
imprecise probability models allow for adequate modeling of partial information and prior-data conflict sensitivity,
and will ultimately result in more reliable inferences.

****more on parameter set shapes? refer to boatshape material****

\medskip

Before we will ***go into the details of models*** in Sections~\ref{sec:jstp} -- \ref{sec:isipta11},
we will have a short look*** at some other models based on sets of priors in Section~\ref{sec:alternatives}.







